#+TITLE:open soft restricted data 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* INIT
** COMMENT init
#+name:init
#+begin_src R :session *shell* :tangle no :exports none :eval yes
  #### name:init ####
  projdir  <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  
  
#+end_src

#+RESULTS: init
| A1BDRY_RainSD07.jpg    |
| A1BWET_RainSD07.jpg    |
| code                   |
| components             |
| data                   |
| Figure1.png            |
| index.org              |
| manuscript_files       |
| manuscript.pdf         |
| manuscript.Rmd         |
| manuscript.Rmd~        |
| manuscript.tex         |
| meemodified.csl        |
| opensoft.pdf           |
| opensoft_workflow.xlsx |
| references.bib         |

* exposure
*** TODO just get the preprocessed drought_futures from garnaut
#+begin_src R :session *shell* :tangle no :exports none :eval no
  qc <- read.csv("~/projects/GARNAUT_CLIMATE_CHANGE_REVIEW/drought_futures/data/drought_future_estimated_dry.csv")
  sd_i <- "Central West"
  str(qc)
  qc$date <- as.Date(qc$date)
  dir()
  qc2 <- qc[qc$sd_group == sd_i,]
  png("graphs/qc_drought_count_central_west.png", width = 1200, height = 600)
  with(qc2,
       plot(date, count, type = "l")
       )
  dev.off()
  # perhaps let's exclude years > 2090 as too the uncertain?
  drt <- qc[qc$year > 2008,]
  qc[qc$year == 1900 & qc$month == 1,]
  
  
  qc <- read.csv("~/projects/GARNAUT_CLIMATE_CHANGE_REVIEW/drought_futures/data/drought_future_estimated_wet.csv")
  sd_i <- "Central West"
  str(qc)
  qc$date <- as.Date(qc$date)
  dir()
  qc2 <- qc[qc$sd_group == sd_i,]
  png("graphs/qc_drought_count_central_west_wet.png", width = 1200, height = 600)
  with(qc2,
       plot(date, count, type = "l")
       )
  dev.off()
  
  drt_wet <- qc[qc$year > 2008,]
  
#+end_src

*** TODO DEPRECATED BoM awap is improved historical, but took too long to do zonal stats per SD
#+name:or use awap grids and csiro access
#+begin_src R :session *shell* :tangle code/awap_grids_on_nswsd.R :exports none :eval no
  #### name:or use awap grids and csiro access ####
  require(swishdbtools)
  if(!require(raster)) install.packages("raster", dependencies = T); require(raster)
  if(!require(rgdal)) install.packages("rgdal", dependencies = T); require(rgdal)
  library(sqldf)  
  
  
  projdir <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  outfile <- "awap_rain_nswsd07.csv"
  
  # load the spatial data for nsw sds
  #args(readOGR2)
  pwd <- getPassword(remote = T)
  shp <- readOGR2(hostip = "gislibrary.anu.edu.au", user = "gislibrary", db = "gislibrary", layer = "abs_sd.aussd07", p = pwd)
  
  #shp <- readOGR2(hostip = "localhost", user = "ivan_hanigan", db = "postgis_hanigan", layer = "abs_sd.aussd07", p = pwd)
  head(shp@data)
  shp <- shp[shp@data$state07==1,]
  writeOGR(shp, "data", "nswsd07", driver = "ESRI Shapefile")
  
  png("graphs/qc_sdmap.png")
  plot(shp)
  dev.off()
  shp@data
  
  # now climate data
  
  indir <- "~/ResearchData/AWAP_GRIDS/AWAP_GRIDS_RAIN_MONTHLY"
  #dir(indir)
  
  setwd(indir)
  cfiles <-  dir(pattern="tif$")
  cfiles[1:10]
  tail(cfiles)
  for(i in seq_len(length(cfiles))){
    #i <- 1 ## for stepping thru
    gridname <- cfiles[[i]]
    r <- raster(gridname)
    e <- extract(r, shp, df=T, fun = mean)
    e1 <- shp
    e1@data$values <- e[,2]
    e1@data$gridname <- gridname
    # e1@data
    # write to to target file
    write.table(e1@data, file.path(projdir,"data", outfile),
      col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE)
  }
  setwd(projdir)
  dat <- read.csv(file.path("data",outfile))
  names(dat)
  names(table(dat$gridname))
  qc_foo <- qc[qc$year == 1900 & qc$month == 1,]
  qc_foo2 <- dat[grep("190001", dat$gridname),]
  with(merge(qc_foo, qc_foo2, by.x = "sd_group", by.y = "sdname07")[,c("sd_group", "avrain", "values")],
       plot(avrain, values)
       )
#+end_src

*** TODO CSIRO ACCESS is improved future
#+name:CSIRO ACCESS is improved future
#+begin_src R :session *shell* :tangle code/csiro_access_nswsd_future.R :exports none :eval no
  #### name:CSIRO ACCESS is improved future ####
  # see 'climate change csiro access'
  library(swishdbtools)
  library(raster)
  library(rgdal)
  library(sqldf)  
  library(ncdf4)
  
  projdir <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  outfile <- "csiro_rain_nswsd07.csv"
  
  # load the spatial data for nsw sds
  #args(readOGR2)
  #get_passwordTable()
  ## pwd <- getPassword(remote = T)
  ## shp <- readOGR2(hostip = "gislibrary.anu.edu.au", user = "gislibrary", db = "gislibrary", layer = "abs_sd.aussd07", p = pwd)
  
  ## #shp <- readOGR2(hostip = "localhost", user = "ivan_hanigan", db = "postgis_hanigan", layer = "abs_sd.aussd07", p = pwd)
  ## head(shp@data)
  ## shp <- shp[shp@data$state07==1,]
  ## writeOGR(shp, "data", "nswsd07", driver = "ESRI Shapefile")
  
  shp <- readOGR("data", "nswsd07")
  # TODO subset so island is gone
  png("graphs/qc_sdmap.png")
  plot(shp)
  dev.off()
  shp@data
  
  # now climate data
  
  indir <- "~/ResearchData/CSIRO-ACCESS-NSW-past-and-future-2100/data_provided"
  dir(indir)
  ## [1] "NSW_pr_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc"
  ## [2] "NSW_pr_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc"     
  
  setwd(indir)
  
  dir(indir)
  infile <- "NSW_pr_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc"
  in_nc <- file.path(indir, infile)
  
  nc <- nc_open(in_nc)
  nc
  nc_close(nc)
      ##  3 dimensions:
      ##     longitude  Size:9
      ##         units: degrees_east
      ##         long_name: longitude
      ##     latitude  Size:8
      ##         units: degrees_north
      ##         long_name: latitude
      ##     value  Size:1872   *** is unlimited ***
      ##         units: unknown
      ##         long_name: value
  
      ## 3 global attributes:
      ##     Conventions: CF-1.4
      ##     created_by: R, packages ncdf and raster (version 2.3-12)
      ##     date: 2015-11-10 11:04:37
  
  
  in_nc
  yy  <- data.frame(yy = 1850:2005)
  mm  <-  data.frame(mm= 1:12)
  mnths <- sqldf("select * from yy join mm order by yy, mm")
  nrow(mnths)
  # 1872
  head(mnths, 24)
  
  for(i in 1:1872){
  #  i = 1
    r <- raster(in_nc, band = i)  
    gridname <- paste(mnths[i,1],mnths[i,2], 1, sep = "-")
  #print(gridname)
  #}
    e <- extract(r, shp, df=T, fun = mean)
  ## Warning message:
  ## In .local(x, y, ...) :
  ##   Transforming SpatialPolygons to the CRS of the Raster
  ## shp is   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
  ## .. .. ..@ projargs: chr "+proj=longlat +ellps=GRS80 +no_defs"
  ## r is   ..@ crs     :Formal class 'CRS' [package "sp"] with 1 slot
  ##.. .. ..@ projargs: chr "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
    
    e1 <- shp@data
    e1$values <- e[,2]
    e1$gridname <- gridname
    # e1
    # NOTE THAT MID NORTH COAST IS NA
    # write to to target file
    write.table(e1,
                file.path(projdir,"data", outfile),
                col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE
                )
  }
  getwd()
  setwd(projdir)
  dat <- read.csv(file.path("data",outfile))
  names(dat)
  dat$gridname <- as.Date(dat$gridname)
  dat$year <- as.numeric(substr(dat$gridname ,1, 4))
  dat$month <- as.numeric(substr(dat$gridname ,6, 7))
  #names(table(dat$gridname))
  str(dat)
  
  str(qc)
  qc_foo <- qc[qc$year >= 1900,]
  qc_foo2 <- dat[dat$year >= 1900,]
  qc_foo3  <- merge(qc_foo, qc_foo2, by.x = c("sd_group", "year", "month"), by.y = c("sdname07", "year" , "month"))[,c("sd_group", "year", "month", "avrain", "values")]
  head(qc_foo3)
  png("graphs/qc_csiro_vs_bom_grids.png")
  with(qc_foo3,
        plot(avrain, values)
        )
  title("qc csiro vs bomgrids 1900-2005")
  dev.off()
  
#+end_src

* baseline outcome
*** COMMENT baseline_outcome
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:baseline_outcome ####
  require(swishdbtools) # get from http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html
  ch <- connect2postgres2("delphe")
  data <- dbGetQuery(ch,
  "
  select cast(dthyy || '-' || dthmm || '-' || 1 as date) as time, *
  from ivan_hanigan.suicidedroughtnsw19702007_rates_drought
  ")
  str(data)
  data.frame(table(data$sd_group))
  ##                     Var1 Freq
  ## 1           Central West 6356
  ## 2                 Hunter 6356
  ## 3              Illawarra 6356
  ## 4        Mid-North Coast 6356
  ## 5                 Murray 6356
  ## 6           Murrumbidgee 6356
  ## 7  North and Far Western 6356
  ## 8               Northern 6356
  ## 9         Richmond-Tweed 6356
  ## 10         South Eastern 6356
  ## 11                Sydney 6356
  
#+end_src

* predicted attributable in future
*** clean_set_datatypes.R
#+begin_src R :session *shell* :tangle code/clean_set_datatypes.R :exports none :eval no
  #### name:fit_baseline_model ####
  # create a drought variable for each category
  # ie pre-calculated Drought by Age, Sex and Rural/Urban Region terms, constructed to have the value of the drought index in the specified groups (with Ages grouped by 20 year age brackets) and zero otherwise.
  # NOTE that we initially fitted this model with a drought effect in each 10 year age bracket, however the 20 year age brackets give essentially the same results, and is simpler to calculate.
    
  require(mgcv)
  require(splines)
  
  # Log transform drought variable, see data preparation for that diagnostic
  data$logDroughtCount = log1p(data$avcount)
  
  # set up the formats of these variables
  data$time=as.Date(paste(data$dthyy,data$dthmm,1,sep='-'))
  data$dthmm=as.factor(data$dthmm)
  data$mm=as.numeric(data$dthmm)
  
  # set up timevar for sinusoidal want
  timevar <- as.data.frame(names(table(data$time)))
  index <- 1:length(names(table(data$time)))
  timevar$time2 <- index/ (length(index) / (length(index)/12))
  names(timevar) <- c('time','timevar')
  timevar$time <- as.Date(timevar$time)
  data <- merge(data,timevar)
  data$time <- as.numeric(data$time)
  data$agegp <- as.factor(data$agegp)
  data$sd_group <- as.factor(data$sd_group)
  str(data)
  
  
  data$rural <-ifelse(data$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
    
  data$agegp2 <-ifelse(data$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(data$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(data$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  data$agegp2 <- as.factor(data$agegp2)
    
  ages <- c('10_19','20_29','30_39','40_49','50_59','60_69','70plus')
  ages2 <- c('10_29','30_49','50plus')
    
  # step thru each
  ## for(sexs in 1:2){
  ## # sexs <- c(2)#,2)
  ## if(sexs == 1) {sexid <- 'Males'} else {sexid <- 'Females'}
  ## #sexid <- c('Females')#,'Females')
  ## for(rural in 0:1){
  ## # rural <- c(1)#,0)
  ## if(rural == 0) {ruralid <- c('urban')} else {ruralid<-'rural'} #,'urban')
    
  ## cat(
  ## paste(
  ## 'data$Drt',sexid,ages2,ruralid,' <- ifelse(data$agegp2 == ',ages2,' & data$sex == ',sexs,' & data$rural == ',rural,', data$logDroughtCount, 0)',
  ## collapse = '
  ## ',sep='')
  ## )
  ## cat('
    
  ## ')
  ## }
    
  ## }
    
  # need to add ' to each agegp
  data$DrtMales10_29urban <- ifelse(data$agegp2 == '10_29' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtMales30_49urban <- ifelse(data$agegp2 == '30_49' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtMales50plusurban <- ifelse(data$agegp2 == '50plus' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
    
  data$DrtMales10_29rural <- ifelse(data$agegp2 == '10_29' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtMales30_49rural <- ifelse(data$agegp2 == '30_49' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtMales50plusrural <- ifelse(data$agegp2 == '50plus' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
    
  data$DrtFemales10_29urban <- ifelse(data$agegp2 == '10_29' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtFemales30_49urban <- ifelse(data$agegp2 == '30_49' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtFemales50plusurban <- ifelse(data$agegp2 == '50plus' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
    
  data$DrtFemales10_29rural <- ifelse(data$agegp2 == '10_29' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtFemales30_49rural <- ifelse(data$agegp2 == '30_49' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtFemales50plusrural <- ifelse(data$agegp2 == '50plus' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
#+end_src
*** fit_baseline_model
#+begin_src R :session *shell* :tangle code/do_fit_model.R :exports none :eval no
    
  ######################
  #do,  The final drought model estimates by age, sex and region
  ######################
  # fit the GLM with recommended df
  strt=Sys.time()
  interactionDrtAgeSexRuralModel3 <- glm(deaths ~ sin(timevar*2*pi) + cos(timevar*2*pi)
  + tmax_anomaly
  + DrtMales10_29rural
  + DrtMales30_49rural
  + DrtMales50plusrural
  + DrtFemales10_29rural
  + DrtFemales30_49rural
  + ns(DrtFemales50plusrural, df = 5)
  + ns(DrtMales10_29urban, df = 6)
  + DrtMales30_49urban
  + ns(DrtMales50plusurban, df = 4)
  + DrtFemales10_29urban
  + ns(DrtFemales30_49urban, df = 3)
  + DrtFemales50plusurban
  + agegp2
  + rural
  + sd_group
  + sex
  + agegp
  + agegp*sex*ns(time,3)
  + offset(log(pop)), data=data,family=poisson)
  #save.image()
  endd=Sys.time()
  print(endd-strt)
  
  summary(interactionDrtAgeSexRuralModel3)
  #Rsquared.glm.gsm(interactionDrtAgeSexRuralModel3)
  
  
  
#+end_src

*** predict_attributable_future
#+name:predict_attributable_future
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:predict_attributable_future ####
  # DEPRECATED drt <- read.csv("data/drought_future_estimated_dry.csv", stringsAsFactors = F)
  # newnode get estimate as attributable deaths
  # need to calculate
  # y(attributableToX) = sum((y0 x (exp(beta * X) - 1) x Pop))
  # where y0 is the baseline incidence rate for the health endpoint being quantified;
  # Pop is the population affected and
  # beta is the effect coefficient drawn from the model.
    
    
  # get a test dataset
  
  paste(names(data)[c(2:9,17)],sep='', collapse="','")
  data2 <- data[,c('sd_group','rural','sex','agegp','agegp2','dthyy', 'dthmm','deaths','pop','logDroughtCount')]
  head(data2)
  # use the average rates deaths/person/month
  # newnode get descriptive deaths by age/sex/month/zone groups
  # calculate baseline incidence
    
  names(data)
  desc <- sqldf('
  select sd_group, sex, agegp,avg(deaths) as avgMonthlyDeaths, avg(pop) as avgPop,
  avg(deaths)/avg(pop) as avgRate
  from data
  group by sd_group, sex, agegp
  order by sd_group, sex, agegp
  ', drv = "SQLite")
  head(desc)
  desc[1:40,]
  sqldf(
  'select sd_group, sum(avgMonthlyDeaths), sum(avgPop)
  from desc
  group by sd_group
  order by sd_group
  ', drv = "SQLite")
  subset(desc, sd_group == 'Sydney')
  ## with(subset(data, sd_group == 'Sydney' & sex == 1), plot(agegp,deaths/pop))
  ## with(subset(data, sd_group == 'Sydney' & sex == 1 & agegp == '70plus'),
  ## plot(as.Date(paste(dthyy, dthmm, 1, sep='-')), deaths, type = 'l', col = 'grey')
  ## )
  ## abline(2.3392070,0)
  ## dev.off()
  # ok merge with the test dataset
  str(desc)
  data2 <- merge(data2, desc, by =  c('sd_group', 'sex', 'agegp'))
  subset(desc, sd_group == 'Central West')
  head(data2)
#+end_src
*** COMMENT dry
#+name:dry
#+begin_src R :session *shell* :tangle no :exports none :eval no
   #### name:dry ####



    
  #### Add the future drought estimates (log)
  str(data2)
  str(drt)
  drt$logDroughtCount_future <- log1p(drt$count)
  
  
  # now use the coefficient in
  # y(attributable) = baselineIncidence x (exp(beta * X) - 1) x Pop
  # recall I used
  glmest<-summary(interactionDrtAgeSexRuralModel3)$coefficients
  betai <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),1]
  sei <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),2]
  # estimate only for  DrtMales30_49rural
  attributable <- subset(data2, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable$sd_group)
  str(attributable)
  
  # previous work used the monthly observed incidence
  # for this work I will use the avg incidnce (and pop) over the 38
  # years
  # subset to rural, add age2
  desc$rural <-ifelse(desc$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
  desc$agegp2 <-ifelse(desc$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(desc$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(desc$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  desc$agegp2 <- as.factor(desc$agegp2)
  
  attributable2 <- subset(desc, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable2$sd_group)
  str(attributable2)
  attributable2
  str(drt)
  drt$sd_group <- as.factor(drt$sd_group)
  attributable2 <- merge(drt, attributable2, by = "sd_group")
  str(attributable2)
  attach(attributable2)
    
  attributable2$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCount_future) - 1) * avgPop
  # SE
  #LCI
  attributable2$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCount_future) - 1) * avgPop
  #UCI
  attributable2$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCount_future) - 1) * avgPop
    
  detach(attributable2)
  head(attributable2)
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select year, sum(deathsAttributable) as deathsAttributable
  from attributable2
  group by year
  order by year
  ', drv = "SQLite")
  summaryAttributable
  # plot the estimated deaths
  ## with(summaryAttributable,
  ## plot(dthyy, deathsAttributable/deaths, type = 'l')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, logDroughtCount, type = 'l',col = 'blue')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  ## )
  # calcualte estimate
    
  estOut <- sqldf(
  'select 
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable2
  ', drv = "SQLite")
    
  # The predicted number of rural male suicides aged 30-49 per annum associated with droughts over our study period was 4.01 (95%CI 2.14 to 6.05)
  estOut$deathsAttributable
  length(names(table(attributable2$year)))
  estOut$deathsAttributable / 92
  estOut$deathsAttributableLower / 92
  estOut$deathsAttributableUpper / 92
    
  # DRY scenario given all years 2009-2100 droughts
  ## > estOut$deathsAttributable
  ## [1] 819.4857
  ## > length(names(table(attributable2$year)))
  ## [1] 92
  ## > estOut$deathsAttributable / 92
  ## [1] 8.907453
  ## >  estOut$deathsAttributableLower / 92
  ## [1] 4.563259
  ## >  estOut$deathsAttributableUpper / 92
  ## [1] 14.00149
   
  
#+end_src
*** COMMENT wet
#+name:dry
#+begin_src R :session *shell* :tangle no :exports none :eval no
  
    
  #### Add the future drought estimates (log)
  str(data2)
  str(drt_wet)
  drt <- drt_wet
  drt$logDroughtCount_future <- log1p(drt$count)
  
  
  # now use the coefficient in
  # y(attributable) = baselineIncidence x (exp(beta * X) - 1) x Pop
  # recall I used
  glmest<-summary(interactionDrtAgeSexRuralModel3)$coefficients
  betai <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),1]
  sei <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),2]
  # estimate only for  DrtMales30_49rural
  attributable <- subset(data2, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable$sd_group)
  str(attributable)
  
  # previous work used the monthly observed incidence
  # for this work I will use the avg incidnce (and pop) over the 38
  # years
  # subset to rural, add age2
  desc$rural <-ifelse(desc$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
  desc$agegp2 <-ifelse(desc$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(desc$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(desc$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  desc$agegp2 <- as.factor(desc$agegp2)
  
  attributable2 <- subset(desc, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable2$sd_group)
  str(attributable2)
  attributable2
  str(drt)
  drt$sd_group <- as.factor(drt$sd_group)
  attributable2 <- merge(drt, attributable2, by = "sd_group")
  str(attributable2)
  attach(attributable2)
    
  attributable2$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCount_future) - 1) * avgPop
  # SE
  #LCI
  attributable2$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCount_future) - 1) * avgPop
  #UCI
  attributable2$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCount_future) - 1) * avgPop
    
  detach(attributable2)
  head(attributable2)
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select year, sum(deathsAttributable) as deathsAttributable
  from attributable2
  group by year
  order by year
  ', drv = "SQLite")
  summaryAttributable
  # plot the estimated deaths
  ## with(summaryAttributable,
  ## plot(dthyy, deathsAttributable/deaths, type = 'l')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, logDroughtCount, type = 'l',col = 'blue')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  ## )
  # calcualte estimate
    
  estOut <- sqldf(
  'select 
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable2
  ', drv = "SQLite")
    
  # The predicted number of rural male suicides aged 30-49 per annum associated with droughts over our study period was 4.01 (95%CI 2.14 to 6.05)
  estOut$deathsAttributable
  length(names(table(attributable2$year)))
  estOut$deathsAttributable / 92
  estOut$deathsAttributableLower / 92
  estOut$deathsAttributableUpper / 92
    
  # DRY scenario given all years 2009-2100 droughts
  ## > estOut$deathsAttributable
  ## [1] 269.3154
  ## > length(names(table(attributable2$year)))
  ## [1] 92
  ## > estOut$deathsAttributable / 92
  ## [1] 2.927341
  ## > estOut$deathsAttributableLower / 92
  ## [1] 1.542382
  ## > estOut$deathsAttributableUpper / 92
  ## [1] 4.465849
   
  
#+end_src

*** COMMENT estimates_per_drougth_year
#+name:estimates_per_drougth_year
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:estimates_per_drougth_year ####



    
  # This is not as good a representation as by drought year.
  # to calculate number of drought years get average of the number of drought years by Rural Regions
  # DROUGHT MONTHS DEFINED AS ANY MONTH WHERE THE DROUGHT INDEX IS
  # GREATER THAN OR EQUAL TO 5.
  droughtyears <- sqldf("select sd_group, sum(droughtmonth)/12 as droughtyears
  from
  (
  select sd_group, agegp, sex, time, avcount,
  case when avcount >= 5 then 1 else 0 end as droughtmonth
  from data
  where agegp = '10_19' and sex = 1
  order by sd_group
  ) t1
  group by sd_group
  ")
    
  # sanity check
  qc <- sqldf("select sd_group, agegp, sex, time, avcount,
        case when avcount >= 5 then 1 else 0 end as droughtmonth
  from data
  where agegp = '10_19' and sex = 1 and sd_group = 'Central West'
  order by sd_group
  ")
    
  png(file.path(rootdir,'CentralWestDrought19702007.png'),res=200,width = 2100, height = 1000)
  with(qc, plot(time, avcount, type = 'l', axes=F))
  with(qc, points(time, avcount, pch = 16, cex=.5))
  box();axis(2);
  axis(1,at=as.Date(paste(1970:2007,'-01-01',sep='')),labels=NA)
  axis(1,at=as.Date(paste(seq(1970, 2007,5),'-01-01',sep='')),labels=seq(1970, 2007,5))
       segments(as.Date(paste(1970:2007,'-01-01',sep='')),0,as.Date(paste(1970:2007,'-01-01',sep='')),12,lty=3)
  segments(min(qc$time),5,max(qc$time),5)
    
  # calculate beginning and end of drougths
  indicator <- cbind(qc$avcount,c(NA,qc[1:(nrow(qc)-1),'avcount']))
  drtstrt <- which(indicator[,1] >=5 & indicator[,2] <5)
  #points(qc$time[drtstrt],qc$avcount[drtstrt], col = 'red')
  drtend <- which(indicator[,1] <5 & indicator[,2] >=5)
  #points(qc$time[drtend-1],rep(5,length(drtend)))
    
     cbind(rep(c(min(qc$time)-(5*365),max(qc$time)+(5*365),max(qc$time)+(5*365),min(qc$time)-(5*365)),3),
  c(drtstrt,drtstrt,drtend-1,drtend-1))
    #polygon(c(min(qc$time)-(5*365),max(qc$time)+(5*365),max(qc$time)+(5*365),min(qc$time)-(5*365)),c(4,4,14,14),col='grey')
  for(i in 1:9){
  polygon(c(qc$time[drtstrt[i]],qc$time[drtend[i]-1],qc$time[drtend[i]-1],qc$time[drtstrt[i]]),
  c(5,5,14,14), col='grey')
  }
  with(qc, lines(time, avcount))
  with(qc, points(time, avcount, pch = 16, cex=.5))
  #points(qc$time[drtstrt],qc$avcount[drtstrt], col = 'red')
    legend('topleft',legend=c('droughtIndex','droughtDeclared'),fill=c(NA,'grey'),border=c(NA,'black'),lty=c(1,NA))
  dev.off()
    
  # check against http://www.dpi.nsw.gov.au/agriculture/emergency/drought/planning/climate/advance-retreat
    
    
  # THIS NEXT ONE CALCULATES THE NUMBER PER DROUGHT YEAR AND COMES UP WITH 17
  # INTERESTING ATTEMPT THAT I MIGHT COME BACK TO
  # BUT FOR NOW WE ARE NOT HAPPY TO INCORPORATE THE ARBITRARY DROUGHT THRESHOLDS IN OUR PREDICTION
    
    
  droughtyearsRural <- droughtyears[!droughtyears$sd_group %in% c('Sydney','Hunter','Illawarra'),]
  #                 sd_group droughtyears
  # 1           Central West            3
  # 4        Mid-North Coast            3
  # 5                 Murray            2
  # 6           Murrumbidgee            3
  # 7  North and Far Western            2
  # 8               Northern            2
  # 9         Richmond-Tweed            5
  # 10         South Eastern            4
  mean(droughtyearsRural$droughtyears)
  # 3
  # so 3 out of 38
  (3/38)*100 # 7.9%
    
  table(attributable$sd_group)
  # set drought index to 0 if <5
  attributable$logDroughtCountDeclared <- ifelse(attributable$logDroughtCount >= log1p(5), attributable$logDroughtCount, 0)
  attach(attributable)
  # TODO this is clobbering the previous calculation, it would be best to keep that and make new names?
  attributable$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCountDeclared) - 1) * pop
  # SE
  #LCI
  attributable$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCountDeclared) - 1) * pop
  #UCI
  attributable$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCountDeclared) - 1) * pop
    
  detach(attributable)
  head(subset(attributable, logDroughtCountDeclared != 0))
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select dthyy, sum(deathsAttributable) as deathsAttributable,
  sum(deaths) as deaths,
  sum(pop) as pop,
  round(avg(logDroughtCountDeclared),1) as logDroughtCountDeclared
    
  from attributable
  group by dthyy
  order by dthyy
  ')
  summaryAttributable
  # plot the estimated deaths
  with(summaryAttributable,
  plot(dthyy, deathsAttributable, type = 'b', pch = 16)
  )
  par(new=T)
  with(summaryAttributable,
  plot(dthyy, logDroughtCountDeclared, type = 'l',col = 'blue')
  )
  #   par(new=T)
  #   with(summaryAttributable,
  #    plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  #    )
  # calcualte estimate
    
  estOut <- sqldf(
  'select sum(deaths) as deaths,
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable
  ')
    
  # The predicted number of rural male suicides aged 30-49 per drought year over our study period was 17.73 (95%CI 9.26 to 27.29)
  estOut$deathsAttributable
  # [1] 53.19648
    
  estOut$deathsAttributable / 3
  # 17.73216
  estOut$deathsAttributableLower / 3
  # 9.260883
  estOut$deathsAttributableUpper / 3
  # 27.28826
    
#+end_src
* report
*** COMMENT go
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:go ####
  setwd("~/projects/opensoftware-restricteddata.github.com/report1_high_level/")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  #rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  dir()
  render("manuscript.Rmd", "pdf_document")
  #browseURL("manuscript.pdf")
#+end_src

*** COMMENT manuscript.Rmd
#+name:manuscript.Rmd
#+begin_src R :session *R* :tangle manuscript.Rmd :exports none :eval no :padline no
---
title: 'Open Software - Restricted Data:  A Suicide/Climate Case Study.'
author:  
- name: Ivan C. Hanigan
  affilnum: 1
  email: ivan.hanigan@anu.edu.au  
- name: David Fisher
  affilnum: 2
- name: Steven McEachern
  affilnum: 3
affiliation:
- affilnum: 1
  affil: National Centre for Epidemiology and Population Health (ANU) 
- affilnum: 2
  affil: Information Technology Services (ANU)
- affilnum: 3
  affil: Australian Data Archives (ANU)
header-includes:
  - \usepackage{graphicx}
  - \usepackage{url}   
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    template: components/manuscript.latex
  html_document: null
  word_document: null
fontsize: 11pt
capsize: normalsize
csl: meemodified.csl
documentclass: article
classoption: a4paper
spacing: singlespacing
linenumbers: no
bibliography: references.bib
abstract: no
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Example Manuscript}
-->
```{r, eval = F, echo = F}
setwd("~/projects/opensoftware-restricteddata.github.com/report1_high_level/")
library(rmarkdown)
library(knitr)
library(knitcitations)
library(bibtex)
cleanbib()
#rm("bib")
#options("cite_format"="pandoc")
cite_options(citation_format = "pandoc", check.entries=FALSE)

dir()
render("manuscript.Rmd", "pdf_document")
browseURL("manuscript.pdf")
```
```{r, echo = F, results = 'hide'}
# load
if(!exists("bib")){
bib <- read.bibtex("~/references/library.bib")

for(bibkey in c("SarathiBiswas2012",
  "Mcmichael2002a", "Gelman2013"
)){
bib[[bibkey]]$url <- gsub("\\{\\\\_\\}","_", bib[[bibkey]]$url)
bib[[bibkey]]$url <- gsub("\\{~\\}","~", bib[[bibkey]]$url)
}

}
```
```{r setup, include=FALSE, echo=FALSE}
#Put whatever you normally put in a setup chunk.
#I usually at least include:
#devtools::install_github("manuscriptPackage","jhollist")
#library("manuscriptPackage")
#Didn't do that here to expedite building of the example vignette
library("knitr")

opts_chunk$set(dev = 'pdf', fig.width=6, fig.height=5)

# Table Captions from @DeanK on http://stackoverflow.com/questions/15258233/using-table-caption-on-r-markdown-file-using-knitr-to-use-in-pandoc-to-convert-t
#Figure captions are handled by LaTeX

knit_hooks$set(tab.cap = function(before, options, envir) {
                  if(!before) { 
                    paste('\n\n:', options$tab.cap, sep='') 
                  }
                })
default_output_hook = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  if (is.null(options$tab.cap) == FALSE) {
    x
  } else
    default_output_hook(x,options)
})
```

```{r analysis , include=FALSE, echo=FALSE, cache=FALSE}
#All analysis in here, that way all bits of the paper have access to the final objects
#Place tables and figures and numerical results where they need to go.
```

<!-- Abstract is being wrapped in latex here so that all analysis can be run in the chunk above and the results reproducibly referenced in the abstract. -->

\singlespace

\vspace{2mm}\hrule
\paragraph*{Background:} This essay was written to accompany the material presented as a
speedtalk and poster at the National Climate Change Adaptation
Research Facility Conference 'Climate Adaptation knowledge and
partnership', June 2013, Sydney.  The poster and slideshow are both available to download from this website: [http://opensoftware-restricteddata.github.io/presentations-nccarf-2013/](http://opensoftware-restricteddata.github.io/presentations-nccarf-2013/)

\paragraph*{Methods:} The paper reports on a project to build tools and procedures for enhancing open and transparent analysis of restricted datasets.
Some datasets such as suicide or climate
change scenarios need to be accessed in a restricted way.  On the
other hand scientists need to make their methods, models and
assumptions transparent and available for scientific debate even
though the datasets may require authorisation to access.
    
\paragraph*{Results:} We built a secure Server/Client Computational
Environment for using open software with restricted data.  We
demonstrate the use of this system using drought and suicide as a case study.  
We describe the potential use of this system in modelling climate change scenarios.

\paragraph*{Conclusions:} The project shows that restricted data and
open software can be used in an appropriate way to further the
progress of scientific enquiry.

\vspace{3mm}\hrule
\doublespace

# Background 
## Open software for restricted data


Some datasets such as sensitive personal information about suicide or
climate change scenarios with protected intellectual property need to
be accessed in a restricted way.  In the context of Reproducible
Research (RR) methods, models and assumptions need to be made
transparent and available for scientific debate even though the
datasets may require authorisation to access `r citep(bib[["Peng2011"]])`.

Restrictions around access to data have increased recently in
Australia, especially to the national mortality database after the
discovery of an incident in which researchers at the University of
Queensland School of Population Health, dissatisfied with the time
it was taking the data custodians to provide data, had hacked into
information about deaths in Australia `r citep(bib[["OKeefe2007"]])`.  
The subsequent investigations lead to
a wide ranging modification to the procedures for approval and
provision of these data that make the access much more restricted.
  
The Replicability Crisis `r citep(bib[["Peng2011"]])` has emerged, reducing public confidence in statements by scientists. The true extent of the problem may turn out to be overstated `r citep(bib[["Jager2014"]])` however the concern should be addressed to avoid the problems a lack of confidence in scientific publications would entail. Appropriate access to data and analytic software addresses this issue.   We investigated available workflow tools for data management and analysis and implemented a range of these products on our server.  This server has enhanced our capacity for experimentation, reviews, revisions and extensions of work in this field.  We present the results of this project and report that it has streamlined access to population health and environmental data for analysis.  

## Motivating case study: Climate/suicide

Suicide has been linked to climate in a variety of studies.
The climate change impact on mental health is a gap in knowledge.
The motivating case for this project was to analyse the relationship between Drought and Suicide. 
The historical exposure-response function can be used to estimate future climate change impacts.

There has been substantial public interest within Australia in recent decades of the putative relationship between drought and rural mental health, including suicide. The topic has frequently been raised by the media, by rural politicians and by mental health support groups `r citep(bib[["AustralianB2006"]])`. There have also been media reports in India indicating substantial concerns about drought and rural suicide in that country, including `r citep(bib[["SarathiBiswas2012"]])` from May 2012.

The number of studies that have examined the relationship between suicide and drought is limited. However, many papers explore links between suicide and climate variables other than drought (such as temperature) and there are two major reviews papers available of the literature on climatic influences on suicide `r citep(c(bib[["Dixon2009"]],bib[["Deisenhammer2003b"]]))`. However, very few studies have investigated the "dryer than average conditions" that is drought specifically.

There are several mechanisms through which unusually low rainfall, especially if exacerbated by increased soil dryness due to higher temperatures may increase the suicide rate. First, droughts increase the financial stress on farmers and farming communities (even if partially compensated by drought relief welfare payments). Such difficulty may occur in conjunction with other economic stresses, such as rising interest rates, falling commodity prices, or an unfavourable foreign exchange rate.

As mentioned the number of studies that have examined the relationship
between suicide and drought is limited to only a handful `r citep(c(bib[["Nicholls2006c"]], bib[["Page2002c"]], bib[["Guiney2012a"]], bib[["Hanigan2012e"]]))` A  systematic
literature review of the Health Effects of Drought found little other
evidence for the putative causal effect of drought on suicide `r citep(bib[["Stanke2013"]])`.


# Methodology

The approach we took to meet the challenge of analysing restricted suicide and climate change scenario data in a safe environment was to
build a new hardware and software stack to using Open-Source software.  We based our
planning on the realisation that there is a growing need of these
technologies in the context of Reproducible Research (RR).  This
requires that methods, models and assumptions need to be made
transparent and available for scientific debate even though the
datasets may require authorisation to access.  This is true not just
in Health data, but also including the context 
of data with restrictive Intellectual Property and licence requirements.

### Open software for restricted data 

To develop an over-all view of
the issue and analyse the dimensions of the problem we spent the
initial phase of the project conceptualising an overall "Rich Picture"
of the issue, and focused on identifying risks that the project might
face. Several papers that describe similar systems were reviewed `r citep(c(bib[["Evans2012"]], bib[["Fleming2014"]]))` and 
several recommendations from these papers were adopted in our system. 

Our design responds directly to the primary threat of unintentional release of
sensitive data so we decided to build a safe Server/Client environment
for analysts to develop their software in an open way, while ensuring
the safety of the datasets.  Other risks we identified were in
relation to the provision of the server hardware and we were able to
take advantage of the Nectar Research Cloud for virtualised services.

Then we defined the scope and quality of the project outcomes that we
were aiming to deliver.  The fact that restrictions around access to
data have increased recently, coupled with arguments that appropriate
access to analytic software is needed to address the Replicability
Crisis meant that the scope of this project was very broad.  we also
explored the ambitions of our analysts to support their publishable
outputs with open software.  Given that examples of un-replicable work
has spread even to the results published in top journals such as
Nature and Science, the scope we decided to set for this project was
for very high levels of open-ness for the evidence being presented for
peer-review along with very high levels of restriction on access to
the data. Luckily however we were able to rule out the need for the
extreme level of restriction such as getting Defence Science and
Technology Organisation (DSTO) accreditation for the security of these
servers against malicious hacking attacks.

We also looked at the workflow system Kepler to assess it's utility
for providing access to the data, but found that there were a lot of
limitations at the time `r citep(bib[["Curcin2008"]])` and decided
that the R environment for statistical computing and graphics would be
the platform we would focus on.

During the next phase of the project we dealt with issues of the costs
associated with developing the software and hosting the hardware at
different locations, as well as the time needed to test and get user
acceptance on the services.  Throughout the project we have had to deal
with lack of resources, especially in terms of Software
Engineering skills for the Client-side user interface and Linux
Systems Administration support for the Server-side backend.  

The results of the IT Infrastructure part of this project are described next.

## System requirements

In this case study we utilise Virtual Machines (VMs) in the Cloud. Our
system requires two VMs so that the storage and processing of data can
be compartmentalised, with various benefits.  A high level overview of
the system is shown in Figure \ref{fig:sys}.  Full details including
Linux commands and configuration specifications are available online at
[http://opensoftware-restricteddata.github.com](http://opensoftware-restricteddata.github.io).

\begin{figure}[!h]
\centering
\includegraphics[width=.85\textwidth]{opensoft.pdf}
\caption{High Level Schematic System Design, colours indicate restrictions (red), open (blue)}
\label{fig:sys}
\end{figure}


## Software selection process
We researched a variety of systems and found the following set-up worked best for us.

### Linux cluster

- National Research Cloud [www.nectar.org.au/research-cloud](www.nectar.org.au/research-cloud)
- Centos 6.4 [www.centos.org](www.centos.org)

### PostGIS database

- PostgreSQL 9.2 [www.postgresql.org](www.postgresql.org)
- PostGIS 2.0 [http://postgis.refractions.net](http://postgis.refractions.net)

### Analysis

-  R language for statistical computing [www.r-project.org](www.r-project.org)
-  Rstudio server [www.rstudio.com](www.rstudio.com)
-  OpenGeo Suite [http://opengeo.org](http://opengeo.org)

### Information management

- Projects,UsersDB Oracle XE APEX [www.oracle.com](www.oracle.com)
- Data Catalogue [http://assda.anu.edu.au/ddiindex.html](http://assda.anu.edu.au/ddiindex.html)

### The client side

-  Any standard web-browser
-  The Kepler Project [www.kepler-project.org](www.kepler-project.org)
-  pgAdmin [www.pgadmin.org](www.pgadmin.org)
-  Git Version Control and GitHub [www.github.com](www.github.com)

# Results 

## Case study 1: Historical exposure-response functions

For this case study we replicated the work we had previously conducted
on our personal desktop computers within the University Research
School.  A key result is shown in Figure \ref{fig:Figure1.png}.  That
work was published already (Hanigan \emph{et al.} 2012), however
the improved IT infrastructure offered by this project allows the
analysis to be re-run from a secure web-browser interface.  Such
improved access allowed a much broader discussion of the data,
techniques and results because the researcher was able to discuss the
details of the modelling with other scientists at conferences and
workshops, while actually repeating the computations in real time.
This is a vast improvement over the previous option of leaving the
data analysis on the secure desktop computers at the University, and
merely describing the computations to colleagues at the workshops.

\begin{figure}[!h]
\centering
\includegraphics[width=.45\textwidth]{Figure1.png}
\caption{Drought exposure-response functions Rural Males}
\label{fig:Figure1.png}
\end{figure}

## Case study 2: Future drought scenarios and attributable fraction of suicides

Following the methods of `r citet(bib[["Climate2008"]])` we used the climate change scenarios provided for the Garnaut Review to project estimates of future suicides under various drought conditions.  The statistical method for this calculation is:

$$Y_{ijk}=\sum_{lm}(e^{(\beta_{ijk}} \times { X_{lm})} - 1) \times {BaselineRate_{jkl}} \times { Population_{jklm}}$$

\noindent Where:

$\beta_{ijk}$ = the ExposureVariable coefficient for zone$_i$, age$_j$ and sex$_{k}$ 

${X_{lm}}$ = Projected Future ExposureVariables  

\emph{BaselineRate$_{jkl}$} = \emph{avgDeathsPerTime}/\emph{avgPopPerTime} in age$_j$, sex$_k$ and zone$_l$ 

\emph{Population$_{jklm}$} = projected populations by age$_j$, sex$_k$, zone$_l$ and time$_m$


```{r, eval = F, echo = F}
dir()
dir("~/data")
indir <- "~/data/GARNAUT mental health archive/1 Data/Rain"
dir(indir, pattern = "jpg")
filelist  <- c("A1BDRY_RainSD07.jpg",             "A1BWET_RainSD07.jpg"            )
for(fi in filelist){
  #file.copy(file.path(indir, fi), fi)
  txt <- sprintf("\\begin{figure}[!h]
\\centering
\\includegraphics[width=\\textwidth]{%s}
\\caption{%s}
\\label{fig:%s}
\\end{figure}
", fi, fi, fi)
  cat(txt)
}
```

## The Garnaut review climate change scenarios

We can demonstrate the use of this system by using the climate change scenarios held on the database to project out future droughts, and use the confidential suicide data that is safely stored there to estimate baseline risks and future burden of suicide attributable to the droughts.  Because the server system is easy to access and modify, and these results are reproducible, alternate scenarios and assumptions can be tested.

In the table below, the two rainfall scenarios used by Berry et al 2008 and Bambrick et al 2008 are used to demonstrate this drought impact assessment.  This shows that the estimated impact of climate change can vary a lot given the input datasets.  The codes used to fit models, project scenarios and estimate burden of deaths is all available on the system and can be assessed and modified.

```{r, eval = T, echo = F, results = 'asis'}
library(xtable)
tab <- read.csv(textConnection("Scenario, Deaths per annum, LCI, UCI
Historical (1970-2008), 4.01, 2.14, 6.05
A1FIR1 (Dry), 8.91, 4.56, 14.00
A1FIR2 (Wet), 2.93, 1.5, 4.47
"), strip.white = T)
#tab
print(xtable(tab), comment = F, include.rownames = F)

```

\clearpage

Below are pictorial represenations of the climate change scenarios influence on rainfall across the country.

\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{A1BDRY_RainSD07.jpg}
\caption{A1BDRY RainSD07}
\label{fig:A1BDRYRainSD07}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[width=.5\textwidth]{A1BWET_RainSD07.jpg}
\caption{A1BWET RainSD07}
\label{fig:A1BWET-RainSD07}
\end{figure}



# Discussion 
## Principal findings 

The results of our project are applicable more generally than just
Reproducible Research (RR). For instance in relation to issues of
governance and management at the multidisciplinary institutions and
multi-institutional projects it is vitally important that data sharing
is enabled, while some data are still being restricted (based either
on authorisation requirements or merely for identifying the profile of
users downloading data for re-use).  It is important that data access
can be made restrictive WITHOUT impeding the progress of the local
science agenda (collaborations, workshops, papers etc) and keeping the
relevant data custodian parties informed about what is happening with
the release of their datasets.  The openness of the analytical
software also has a positive effect on the value of the data
infrastructure (through education and outreach activities) without
risking any unethical or negligent use of these datasets.

#  Discussion and conclusion 

- Drought is related to increased suicide risk in Australia
- Future Drought associated deaths can be calculated
- These estimates will be very uncertain, contentious and difficult to justify
- Data management and analysis technology such as that presented is needed to enable rigorous and transparent exploration 

This system:

- Enables data analysis in a safe environment
- Allows comparison of multiple climate scenarios and assumptions
- Demonstrated with a Climate/Health Impact Assessment 
- This is Reproducible



# References

```{r, echo=FALSE, message=FALSE, eval = T}
write.bibtex(file="references.bib")
```


#+end_src
