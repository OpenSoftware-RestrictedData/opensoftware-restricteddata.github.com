#+TITLE:open soft restricted data 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----

* INIT
** COMMENT init
#+name:init
#+begin_src R :session *shell* :tangle no :exports none :eval yes
  #### name:init ####
  projdir  <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  
  
#+end_src

#+RESULTS: init
| A1BDRY_RainSD07.jpg    |
| A1BWET_RainSD07.jpg    |
| code                   |
| components             |
| data                   |
| Figure1.png            |
| index.org              |
| manuscript_files       |
| manuscript.pdf         |
| manuscript.Rmd         |
| manuscript.Rmd~        |
| manuscript.tex         |
| meemodified.csl        |
| opensoft.pdf           |
| opensoft_workflow.xlsx |
| references.bib         |

* exposure
*** TODO just get the preprocessed drought_futures from garnaut
#+begin_src R :session *shell* :tangle no :exports none :eval no
  qc <- read.csv("~/projects/GARNAUT_CLIMATE_CHANGE_REVIEW/drought_futures/data/drought_future_estimated_dry.csv")
  sd_i <- "Central West"
  str(qc)
  qc$date <- as.Date(qc$date)
  dir()
  qc2 <- qc[qc$sd_group == sd_i,]
  png("graphs/qc_drought_count_central_west.png", width = 1200, height = 600)
  with(qc2,
       plot(date, count, type = "l")
       )
  dev.off()
  # perhaps let's exclude years > 2090 as too the uncertain?
  drt <- qc[qc$year > 2008,]
  qc[qc$year == 1900 & qc$month == 1,]
  
  
  qc <- read.csv("~/projects/GARNAUT_CLIMATE_CHANGE_REVIEW/drought_futures/data/drought_future_estimated_wet.csv")
  sd_i <- "Central West"
  str(qc)
  qc$date <- as.Date(qc$date)
  dir()
  qc2 <- qc[qc$sd_group == sd_i,]
  png("graphs/qc_drought_count_central_west_wet.png", width = 1200, height = 600)
  with(qc2,
       plot(date, count, type = "l")
       )
  dev.off()
  
  drt_wet <- qc[qc$year > 2008,]
  
#+end_src

*** TODO DEPRECATED BoM awap is improved historical, but took too long to do zonal stats per SD
#+name:or use awap grids and csiro access
#+begin_src R :session *shell* :tangle code/awap_grids_on_nswsd.R :exports none :eval no
  #### name:or use awap grids and csiro access ####
  require(swishdbtools)
  if(!require(raster)) install.packages("raster", dependencies = T); require(raster)
  if(!require(rgdal)) install.packages("rgdal", dependencies = T); require(rgdal)
  library(sqldf)  
  
  
  projdir <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  outfile <- "awap_rain_nswsd07.csv"
  
  # load the spatial data for nsw sds
  #args(readOGR2)
  pwd <- getPassword(remote = T)
  shp <- readOGR2(hostip = "gislibrary.anu.edu.au", user = "gislibrary", db = "gislibrary", layer = "abs_sd.aussd07", p = pwd)
  
  #shp <- readOGR2(hostip = "localhost", user = "ivan_hanigan", db = "postgis_hanigan", layer = "abs_sd.aussd07", p = pwd)
  head(shp@data)
  shp <- shp[shp@data$state07==1,]
  writeOGR(shp, "data", "nswsd07", driver = "ESRI Shapefile")
  
  png("graphs/qc_sdmap.png")
  plot(shp)
  dev.off()
  shp@data
  
  # now climate data
  
  indir <- "~/ResearchData/AWAP_GRIDS/AWAP_GRIDS_RAIN_MONTHLY"
  #dir(indir)
  
  setwd(indir)
  cfiles <-  dir(pattern="tif$")
  cfiles[1:10]
  tail(cfiles)
  for(i in seq_len(length(cfiles))){
    #i <- 1 ## for stepping thru
    gridname <- cfiles[[i]]
    r <- raster(gridname)
    e <- extract(r, shp, df=T, fun = mean)
    e1 <- shp
    e1@data$values <- e[,2]
    e1@data$gridname <- gridname
    # e1@data
    # write to to target file
    write.table(e1@data, file.path(projdir,"data", outfile),
      col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE)
  }
  setwd(projdir)
  dat <- read.csv(file.path("data",outfile))
  names(dat)
  names(table(dat$gridname))
  qc_foo <- qc[qc$year == 1900 & qc$month == 1,]
  qc_foo2 <- dat[grep("190001", dat$gridname),]
  with(merge(qc_foo, qc_foo2, by.x = "sd_group", by.y = "sdname07")[,c("sd_group", "avrain", "values")],
       plot(avrain, values)
       )
#+end_src

*** TODO CSIRO ACCESS is improved future
#+name:CSIRO ACCESS is improved future
#+begin_src R :session *shell* :tangle code/csiro_access_nswsd_future.R :exports none :eval no
  #### name:CSIRO ACCESS is improved future ####
  # see 'climate change csiro access'
  library(swishdbtools)
  library(raster)
  library(rgdal)
  library(sqldf)  
  library(ncdf4)
  
  projdir <- "~/projects/opensoftware-restricteddata.github.com/report1_high_level"
  setwd(projdir)
  dir()
  outfile <- "csiro_rain_nswsd07.csv"
  
  # load the spatial data for nsw sds
  #args(readOGR2)
  #get_passwordTable()
  ## pwd <- getPassword(remote = T)
  ## shp <- readOGR2(hostip = "gislibrary.anu.edu.au", user = "gislibrary", db = "gislibrary", layer = "abs_sd.aussd07", p = pwd)
  
  ## #shp <- readOGR2(hostip = "localhost", user = "ivan_hanigan", db = "postgis_hanigan", layer = "abs_sd.aussd07", p = pwd)
  ## head(shp@data)
  ## shp <- shp[shp@data$state07==1,]
  ## writeOGR(shp, "data", "nswsd07", driver = "ESRI Shapefile")
  
  shp <- readOGR("data", "nswsd07")
  # TODO subset so island is gone
  png("graphs/qc_sdmap.png")
  plot(shp)
  dev.off()
  shp@data
  
  # now climate data
  
  indir <- "~/ResearchData/CSIRO-ACCESS-NSW-past-and-future-2100/data_provided"
  dir(indir)
  ## [1] "NSW_pr_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc"
  ## [2] "NSW_pr_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc"     
  
  setwd(indir)
  
  dir(indir)
  infile <- "NSW_pr_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc"
  in_nc <- file.path(indir, infile)
  
  nc <- nc_open(in_nc)
  nc
  nc_close(nc)
      ##  3 dimensions:
      ##     longitude  Size:9
      ##         units: degrees_east
      ##         long_name: longitude
      ##     latitude  Size:8
      ##         units: degrees_north
      ##         long_name: latitude
      ##     value  Size:1872   *** is unlimited ***
      ##         units: unknown
      ##         long_name: value
  
      ## 3 global attributes:
      ##     Conventions: CF-1.4
      ##     created_by: R, packages ncdf and raster (version 2.3-12)
      ##     date: 2015-11-10 11:04:37
  
  
  in_nc
  yy  <- data.frame(yy = 1850:2005)
  mm  <-  data.frame(mm= 1:12)
  mnths <- sqldf("select * from yy join mm order by yy, mm")
  nrow(mnths)
  # 1872
  head(mnths, 24)
  
  for(i in 1:1872){
  #  i = 1
    r <- raster(in_nc, band = i)  
    gridname <- paste(mnths[i,1],mnths[i,2], 1, sep = "-")
  #print(gridname)
  #}
    e <- extract(r, shp, df=T, fun = mean)
  ## Warning message:
  ## In .local(x, y, ...) :
  ##   Transforming SpatialPolygons to the CRS of the Raster
  ## shp is   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
  ## .. .. ..@ projargs: chr "+proj=longlat +ellps=GRS80 +no_defs"
  ## r is   ..@ crs     :Formal class 'CRS' [package "sp"] with 1 slot
  ##.. .. ..@ projargs: chr "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
    
    e1 <- shp@data
    e1$values <- e[,2]
    e1$gridname <- gridname
    # e1
    # NOTE THAT MID NORTH COAST IS NA
    # write to to target file
    write.table(e1,
                file.path(projdir,"data", outfile),
                col.names = i == 1, append = i>1 , sep = ",", row.names = FALSE
                )
  }
  getwd()
  setwd(projdir)
  dat <- read.csv(file.path("data",outfile))
  names(dat)
  dat$gridname <- as.Date(dat$gridname)
  dat$year <- as.numeric(substr(dat$gridname ,1, 4))
  dat$month <- as.numeric(substr(dat$gridname ,6, 7))
  #names(table(dat$gridname))
  str(dat)
  
  str(qc)
  qc_foo <- qc[qc$year >= 1900,]
  qc_foo2 <- dat[dat$year >= 1900,]
  qc_foo3  <- merge(qc_foo, qc_foo2, by.x = c("sd_group", "year", "month"), by.y = c("sdname07", "year" , "month"))[,c("sd_group", "year", "month", "avrain", "values")]
  head(qc_foo3)
  png("graphs/qc_csiro_vs_bom_grids.png")
  with(qc_foo3,
        plot(avrain, values)
        )
  title("qc csiro vs bomgrids 1900-2005")
  dev.off()
  
#+end_src

* baseline outcome
*** COMMENT baseline_outcome
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:baseline_outcome ####
  require(swishdbtools) # get from http://swish-climate-impact-assessment.github.io/tools/swishdbtools/swishdbtools-downloads.html
  ch <- connect2postgres2("delphe")
  data <- dbGetQuery(ch,
  "
  select cast(dthyy || '-' || dthmm || '-' || 1 as date) as time, *
  from ivan_hanigan.suicidedroughtnsw19702007_rates_drought
  ")
  str(data)
  data.frame(table(data$sd_group))
  ##                     Var1 Freq
  ## 1           Central West 6356
  ## 2                 Hunter 6356
  ## 3              Illawarra 6356
  ## 4        Mid-North Coast 6356
  ## 5                 Murray 6356
  ## 6           Murrumbidgee 6356
  ## 7  North and Far Western 6356
  ## 8               Northern 6356
  ## 9         Richmond-Tweed 6356
  ## 10         South Eastern 6356
  ## 11                Sydney 6356
  
#+end_src

* predicted attributable in future
*** clean_set_datatypes.R
#+begin_src R :session *shell* :tangle code/clean_set_datatypes.R :exports none :eval no
  #### name:fit_baseline_model ####
  # create a drought variable for each category
  # ie pre-calculated Drought by Age, Sex and Rural/Urban Region terms, constructed to have the value of the drought index in the specified groups (with Ages grouped by 20 year age brackets) and zero otherwise.
  # NOTE that we initially fitted this model with a drought effect in each 10 year age bracket, however the 20 year age brackets give essentially the same results, and is simpler to calculate.
    
  require(mgcv)
  require(splines)
  
  # Log transform drought variable, see data preparation for that diagnostic
  data$logDroughtCount = log1p(data$avcount)
  
  # set up the formats of these variables
  data$time=as.Date(paste(data$dthyy,data$dthmm,1,sep='-'))
  data$dthmm=as.factor(data$dthmm)
  data$mm=as.numeric(data$dthmm)
  
  # set up timevar for sinusoidal want
  timevar <- as.data.frame(names(table(data$time)))
  index <- 1:length(names(table(data$time)))
  timevar$time2 <- index/ (length(index) / (length(index)/12))
  names(timevar) <- c('time','timevar')
  timevar$time <- as.Date(timevar$time)
  data <- merge(data,timevar)
  data$time <- as.numeric(data$time)
  data$agegp <- as.factor(data$agegp)
  data$sd_group <- as.factor(data$sd_group)
  str(data)
  
  
  data$rural <-ifelse(data$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
    
  data$agegp2 <-ifelse(data$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(data$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(data$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  data$agegp2 <- as.factor(data$agegp2)
    
  ages <- c('10_19','20_29','30_39','40_49','50_59','60_69','70plus')
  ages2 <- c('10_29','30_49','50plus')
    
  # step thru each
  ## for(sexs in 1:2){
  ## # sexs <- c(2)#,2)
  ## if(sexs == 1) {sexid <- 'Males'} else {sexid <- 'Females'}
  ## #sexid <- c('Females')#,'Females')
  ## for(rural in 0:1){
  ## # rural <- c(1)#,0)
  ## if(rural == 0) {ruralid <- c('urban')} else {ruralid<-'rural'} #,'urban')
    
  ## cat(
  ## paste(
  ## 'data$Drt',sexid,ages2,ruralid,' <- ifelse(data$agegp2 == ',ages2,' & data$sex == ',sexs,' & data$rural == ',rural,', data$logDroughtCount, 0)',
  ## collapse = '
  ## ',sep='')
  ## )
  ## cat('
    
  ## ')
  ## }
    
  ## }
    
  # need to add ' to each agegp
  data$DrtMales10_29urban <- ifelse(data$agegp2 == '10_29' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtMales30_49urban <- ifelse(data$agegp2 == '30_49' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtMales50plusurban <- ifelse(data$agegp2 == '50plus' & data$sex == 1 & data$rural == 0, data$logDroughtCount, 0)
    
  data$DrtMales10_29rural <- ifelse(data$agegp2 == '10_29' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtMales30_49rural <- ifelse(data$agegp2 == '30_49' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtMales50plusrural <- ifelse(data$agegp2 == '50plus' & data$sex == 1 & data$rural == 1, data$logDroughtCount, 0)
    
  data$DrtFemales10_29urban <- ifelse(data$agegp2 == '10_29' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtFemales30_49urban <- ifelse(data$agegp2 == '30_49' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
  data$DrtFemales50plusurban <- ifelse(data$agegp2 == '50plus' & data$sex == 2 & data$rural == 0, data$logDroughtCount, 0)
    
  data$DrtFemales10_29rural <- ifelse(data$agegp2 == '10_29' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtFemales30_49rural <- ifelse(data$agegp2 == '30_49' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
  data$DrtFemales50plusrural <- ifelse(data$agegp2 == '50plus' & data$sex == 2 & data$rural == 1, data$logDroughtCount, 0)
#+end_src
*** fit_baseline_model
#+begin_src R :session *shell* :tangle code/do_fit_model.R :exports none :eval no
    
  ######################
  #do,  The final drought model estimates by age, sex and region
  ######################
  # fit the GLM with recommended df
  strt=Sys.time()
  interactionDrtAgeSexRuralModel3 <- glm(deaths ~ sin(timevar*2*pi) + cos(timevar*2*pi)
  + tmax_anomaly
  + DrtMales10_29rural
  + DrtMales30_49rural
  + DrtMales50plusrural
  + DrtFemales10_29rural
  + DrtFemales30_49rural
  + ns(DrtFemales50plusrural, df = 5)
  + ns(DrtMales10_29urban, df = 6)
  + DrtMales30_49urban
  + ns(DrtMales50plusurban, df = 4)
  + DrtFemales10_29urban
  + ns(DrtFemales30_49urban, df = 3)
  + DrtFemales50plusurban
  + agegp2
  + rural
  + sd_group
  + sex
  + agegp
  + agegp*sex*ns(time,3)
  + offset(log(pop)), data=data,family=poisson)
  #save.image()
  endd=Sys.time()
  print(endd-strt)
  
  summary(interactionDrtAgeSexRuralModel3)
  #Rsquared.glm.gsm(interactionDrtAgeSexRuralModel3)
  
  
  
#+end_src

*** predict_attributable_future
#+name:predict_attributable_future
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:predict_attributable_future ####
  # DEPRECATED drt <- read.csv("data/drought_future_estimated_dry.csv", stringsAsFactors = F)
  # newnode get estimate as attributable deaths
  # need to calculate
  # y(attributableToX) = sum((y0 x (exp(beta * X) - 1) x Pop))
  # where y0 is the baseline incidence rate for the health endpoint being quantified;
  # Pop is the population affected and
  # beta is the effect coefficient drawn from the model.
    
    
  # get a test dataset
  
  paste(names(data)[c(2:9,17)],sep='', collapse="','")
  data2 <- data[,c('sd_group','rural','sex','agegp','agegp2','dthyy', 'dthmm','deaths','pop','logDroughtCount')]
  head(data2)
  # use the average rates deaths/person/month
  # newnode get descriptive deaths by age/sex/month/zone groups
  # calculate baseline incidence
    
  names(data)
  desc <- sqldf('
  select sd_group, sex, agegp,avg(deaths) as avgMonthlyDeaths, avg(pop) as avgPop,
  avg(deaths)/avg(pop) as avgRate
  from data
  group by sd_group, sex, agegp
  order by sd_group, sex, agegp
  ', drv = "SQLite")
  head(desc)
  desc[1:40,]
  sqldf(
  'select sd_group, sum(avgMonthlyDeaths), sum(avgPop)
  from desc
  group by sd_group
  order by sd_group
  ', drv = "SQLite")
  subset(desc, sd_group == 'Sydney')
  ## with(subset(data, sd_group == 'Sydney' & sex == 1), plot(agegp,deaths/pop))
  ## with(subset(data, sd_group == 'Sydney' & sex == 1 & agegp == '70plus'),
  ## plot(as.Date(paste(dthyy, dthmm, 1, sep='-')), deaths, type = 'l', col = 'grey')
  ## )
  ## abline(2.3392070,0)
  ## dev.off()
  # ok merge with the test dataset
  str(desc)
  data2 <- merge(data2, desc, by =  c('sd_group', 'sex', 'agegp'))
  subset(desc, sd_group == 'Central West')
  head(data2)
#+end_src
*** COMMENT dry
#+name:dry
#+begin_src R :session *shell* :tangle no :exports none :eval no
   #### name:dry ####



    
  #### Add the future drought estimates (log)
  str(data2)
  str(drt)
  drt$logDroughtCount_future <- log1p(drt$count)
  
  
  # now use the coefficient in
  # y(attributable) = baselineIncidence x (exp(beta * X) - 1) x Pop
  # recall I used
  glmest<-summary(interactionDrtAgeSexRuralModel3)$coefficients
  betai <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),1]
  sei <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),2]
  # estimate only for  DrtMales30_49rural
  attributable <- subset(data2, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable$sd_group)
  str(attributable)
  
  # previous work used the monthly observed incidence
  # for this work I will use the avg incidnce (and pop) over the 38
  # years
  # subset to rural, add age2
  desc$rural <-ifelse(desc$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
  desc$agegp2 <-ifelse(desc$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(desc$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(desc$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  desc$agegp2 <- as.factor(desc$agegp2)
  
  attributable2 <- subset(desc, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable2$sd_group)
  str(attributable2)
  attributable2
  str(drt)
  drt$sd_group <- as.factor(drt$sd_group)
  attributable2 <- merge(drt, attributable2, by = "sd_group")
  str(attributable2)
  attach(attributable2)
    
  attributable2$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCount_future) - 1) * avgPop
  # SE
  #LCI
  attributable2$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCount_future) - 1) * avgPop
  #UCI
  attributable2$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCount_future) - 1) * avgPop
    
  detach(attributable2)
  head(attributable2)
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select year, sum(deathsAttributable) as deathsAttributable
  from attributable2
  group by year
  order by year
  ', drv = "SQLite")
  summaryAttributable
  # plot the estimated deaths
  ## with(summaryAttributable,
  ## plot(dthyy, deathsAttributable/deaths, type = 'l')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, logDroughtCount, type = 'l',col = 'blue')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  ## )
  # calcualte estimate
    
  estOut <- sqldf(
  'select 
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable2
  ', drv = "SQLite")
    
  # The predicted number of rural male suicides aged 30-49 per annum associated with droughts over our study period was 4.01 (95%CI 2.14 to 6.05)
  estOut$deathsAttributable
  length(names(table(attributable2$year)))
  estOut$deathsAttributable / 92
  estOut$deathsAttributableLower / 92
  estOut$deathsAttributableUpper / 92
    
  # DRY scenario given all years 2009-2100 droughts
  ## > estOut$deathsAttributable
  ## [1] 819.4857
  ## > length(names(table(attributable2$year)))
  ## [1] 92
  ## > estOut$deathsAttributable / 92
  ## [1] 8.907453
  ## >  estOut$deathsAttributableLower / 92
  ## [1] 4.563259
  ## >  estOut$deathsAttributableUpper / 92
  ## [1] 14.00149
   
  
#+end_src
*** COMMENT wet
#+name:dry
#+begin_src R :session *shell* :tangle no :exports none :eval no
  
    
  #### Add the future drought estimates (log)
  str(data2)
  str(drt_wet)
  drt <- drt_wet
  drt$logDroughtCount_future <- log1p(drt$count)
  
  
  # now use the coefficient in
  # y(attributable) = baselineIncidence x (exp(beta * X) - 1) x Pop
  # recall I used
  glmest<-summary(interactionDrtAgeSexRuralModel3)$coefficients
  betai <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),1]
  sei <- glmest[which(row.names(glmest)=='DrtMales30_49rural'),2]
  # estimate only for  DrtMales30_49rural
  attributable <- subset(data2, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable$sd_group)
  str(attributable)
  
  # previous work used the monthly observed incidence
  # for this work I will use the avg incidnce (and pop) over the 38
  # years
  # subset to rural, add age2
  desc$rural <-ifelse(desc$sd_group %in% c('Central West','Mid-North Coast','Murray','Murrumbidgee','North and Far Western','Northern','Richmond-Tweed','South Eastern'), 1, 0)
  desc$agegp2 <-ifelse(desc$agegp %in% c('10_19','20_29'), '10_29',
  ifelse(desc$agegp %in% c('30_39','40_49'), '30_49',
  ifelse(desc$agegp %in% c('50_59','60_69','70plus'), '50plus',
  0)))
    
  desc$agegp2 <- as.factor(desc$agegp2)
  
  attributable2 <- subset(desc, rural == 1 & sex ==1 & agegp2 == '30_49')
  table(attributable2$sd_group)
  str(attributable2)
  attributable2
  str(drt)
  drt$sd_group <- as.factor(drt$sd_group)
  attributable2 <- merge(drt, attributable2, by = "sd_group")
  str(attributable2)
  attach(attributable2)
    
  attributable2$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCount_future) - 1) * avgPop
  # SE
  #LCI
  attributable2$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCount_future) - 1) * avgPop
  #UCI
  attributable2$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCount_future) - 1) * avgPop
    
  detach(attributable2)
  head(attributable2)
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select year, sum(deathsAttributable) as deathsAttributable
  from attributable2
  group by year
  order by year
  ', drv = "SQLite")
  summaryAttributable
  # plot the estimated deaths
  ## with(summaryAttributable,
  ## plot(dthyy, deathsAttributable/deaths, type = 'l')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, logDroughtCount, type = 'l',col = 'blue')
  ## )
  ## par(new=T)
  ## with(summaryAttributable,
  ## plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  ## )
  # calcualte estimate
    
  estOut <- sqldf(
  'select 
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable2
  ', drv = "SQLite")
    
  # The predicted number of rural male suicides aged 30-49 per annum associated with droughts over our study period was 4.01 (95%CI 2.14 to 6.05)
  estOut$deathsAttributable
  length(names(table(attributable2$year)))
  estOut$deathsAttributable / 92
  estOut$deathsAttributableLower / 92
  estOut$deathsAttributableUpper / 92
    
  # DRY scenario given all years 2009-2100 droughts
  ## > estOut$deathsAttributable
  ## [1] 269.3154
  ## > length(names(table(attributable2$year)))
  ## [1] 92
  ## > estOut$deathsAttributable / 92
  ## [1] 2.927341
  ## > estOut$deathsAttributableLower / 92
  ## [1] 1.542382
  ## > estOut$deathsAttributableUpper / 92
  ## [1] 4.465849
   
  
#+end_src

*** COMMENT estimates_per_drougth_year
#+name:estimates_per_drougth_year
#+begin_src R :session *shell* :tangle no :exports none :eval no
  #### name:estimates_per_drougth_year ####



    
  # This is not as good a representation as by drought year.
  # to calculate number of drought years get average of the number of drought years by Rural Regions
  # DROUGHT MONTHS DEFINED AS ANY MONTH WHERE THE DROUGHT INDEX IS
  # GREATER THAN OR EQUAL TO 5.
  droughtyears <- sqldf("select sd_group, sum(droughtmonth)/12 as droughtyears
  from
  (
  select sd_group, agegp, sex, time, avcount,
  case when avcount >= 5 then 1 else 0 end as droughtmonth
  from data
  where agegp = '10_19' and sex = 1
  order by sd_group
  ) t1
  group by sd_group
  ")
    
  # sanity check
  qc <- sqldf("select sd_group, agegp, sex, time, avcount,
        case when avcount >= 5 then 1 else 0 end as droughtmonth
  from data
  where agegp = '10_19' and sex = 1 and sd_group = 'Central West'
  order by sd_group
  ")
    
  png(file.path(rootdir,'CentralWestDrought19702007.png'),res=200,width = 2100, height = 1000)
  with(qc, plot(time, avcount, type = 'l', axes=F))
  with(qc, points(time, avcount, pch = 16, cex=.5))
  box();axis(2);
  axis(1,at=as.Date(paste(1970:2007,'-01-01',sep='')),labels=NA)
  axis(1,at=as.Date(paste(seq(1970, 2007,5),'-01-01',sep='')),labels=seq(1970, 2007,5))
       segments(as.Date(paste(1970:2007,'-01-01',sep='')),0,as.Date(paste(1970:2007,'-01-01',sep='')),12,lty=3)
  segments(min(qc$time),5,max(qc$time),5)
    
  # calculate beginning and end of drougths
  indicator <- cbind(qc$avcount,c(NA,qc[1:(nrow(qc)-1),'avcount']))
  drtstrt <- which(indicator[,1] >=5 & indicator[,2] <5)
  #points(qc$time[drtstrt],qc$avcount[drtstrt], col = 'red')
  drtend <- which(indicator[,1] <5 & indicator[,2] >=5)
  #points(qc$time[drtend-1],rep(5,length(drtend)))
    
     cbind(rep(c(min(qc$time)-(5*365),max(qc$time)+(5*365),max(qc$time)+(5*365),min(qc$time)-(5*365)),3),
  c(drtstrt,drtstrt,drtend-1,drtend-1))
    #polygon(c(min(qc$time)-(5*365),max(qc$time)+(5*365),max(qc$time)+(5*365),min(qc$time)-(5*365)),c(4,4,14,14),col='grey')
  for(i in 1:9){
  polygon(c(qc$time[drtstrt[i]],qc$time[drtend[i]-1],qc$time[drtend[i]-1],qc$time[drtstrt[i]]),
  c(5,5,14,14), col='grey')
  }
  with(qc, lines(time, avcount))
  with(qc, points(time, avcount, pch = 16, cex=.5))
  #points(qc$time[drtstrt],qc$avcount[drtstrt], col = 'red')
    legend('topleft',legend=c('droughtIndex','droughtDeclared'),fill=c(NA,'grey'),border=c(NA,'black'),lty=c(1,NA))
  dev.off()
    
  # check against http://www.dpi.nsw.gov.au/agriculture/emergency/drought/planning/climate/advance-retreat
    
    
  # THIS NEXT ONE CALCULATES THE NUMBER PER DROUGHT YEAR AND COMES UP WITH 17
  # INTERESTING ATTEMPT THAT I MIGHT COME BACK TO
  # BUT FOR NOW WE ARE NOT HAPPY TO INCORPORATE THE ARBITRARY DROUGHT THRESHOLDS IN OUR PREDICTION
    
    
  droughtyearsRural <- droughtyears[!droughtyears$sd_group %in% c('Sydney','Hunter','Illawarra'),]
  #                 sd_group droughtyears
  # 1           Central West            3
  # 4        Mid-North Coast            3
  # 5                 Murray            2
  # 6           Murrumbidgee            3
  # 7  North and Far Western            2
  # 8               Northern            2
  # 9         Richmond-Tweed            5
  # 10         South Eastern            4
  mean(droughtyearsRural$droughtyears)
  # 3
  # so 3 out of 38
  (3/38)*100 # 7.9%
    
  table(attributable$sd_group)
  # set drought index to 0 if <5
  attributable$logDroughtCountDeclared <- ifelse(attributable$logDroughtCount >= log1p(5), attributable$logDroughtCount, 0)
  attach(attributable)
  # TODO this is clobbering the previous calculation, it would be best to keep that and make new names?
  attributable$deathsAttributable <-
  (avgMonthlyDeaths/avgPop) * (exp(betai * logDroughtCountDeclared) - 1) * pop
  # SE
  #LCI
  attributable$deathsAttributableLower <-
  (avgMonthlyDeaths/avgPop) * (exp((betai - sei * 1.96) *  logDroughtCountDeclared) - 1) * pop
  #UCI
  attributable$deathsAttributableUpper <-
  (avgMonthlyDeaths/avgPop) * (exp((betai + sei * 1.96) * logDroughtCountDeclared) - 1) * pop
    
  detach(attributable)
  head(subset(attributable, logDroughtCountDeclared != 0))
    
    
  # now summarise by year
  summaryAttributable <- sqldf(
  'select dthyy, sum(deathsAttributable) as deathsAttributable,
  sum(deaths) as deaths,
  sum(pop) as pop,
  round(avg(logDroughtCountDeclared),1) as logDroughtCountDeclared
    
  from attributable
  group by dthyy
  order by dthyy
  ')
  summaryAttributable
  # plot the estimated deaths
  with(summaryAttributable,
  plot(dthyy, deathsAttributable, type = 'b', pch = 16)
  )
  par(new=T)
  with(summaryAttributable,
  plot(dthyy, logDroughtCountDeclared, type = 'l',col = 'blue')
  )
  #   par(new=T)
  #   with(summaryAttributable,
  #    plot(dthyy, deaths, type = 'b',col = 'darkblue', pch=16)
  #    )
  # calcualte estimate
    
  estOut <- sqldf(
  'select sum(deaths) as deaths,
  sum(deathsAttributable) as deathsAttributable,
  sum(deathsAttributableLower) as deathsAttributableLower,
  sum(deathsAttributableUpper) as deathsAttributableUpper
  from attributable
  ')
    
  # The predicted number of rural male suicides aged 30-49 per drought year over our study period was 17.73 (95%CI 9.26 to 27.29)
  estOut$deathsAttributable
  # [1] 53.19648
    
  estOut$deathsAttributable / 3
  # 17.73216
  estOut$deathsAttributableLower / 3
  # 9.260883
  estOut$deathsAttributableUpper / 3
  # 27.28826
    
#+end_src
* report
*** COMMENT go
#+name:go
#+begin_src R :session *R* :tangle no :exports none :eval yes
  #### name:go ####
  setwd("~/projects/opensoftware-restricteddata.github.com/report1_high_level/")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  #rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  dir()
  render("manuscript.Rmd", "pdf_document")
  #browseURL("manuscript.pdf")
  
#+end_src

#+RESULTS: go
: /home/ivan_hanigan/projects/opensoftware-restricteddata.github.com/report1_high_level/manuscript.pdf

*** COMMENT manuscript.Rmd
#+name:manuscript.Rmd
#+begin_src R :session *R* :tangle manuscript.Rmd :exports none :eval no :padline no
  ---
  title: 'Open Software - Restricted Data: A case study.'
  author:  
  - name: Ivan C. Hanigan
    affilnum: 1
    email: ivan.hanigan@anu.edu.au  
  - name: David Fisher
    affilnum: 2
  - name: Steven McEachern
    affilnum: 3
  affiliation:
  - affilnum: 1
    affil: National Centre for Epidemiology and Population Health (ANU) 
  - affilnum: 2
    affil: Information Technology Services (ANU)
  - affilnum: 3
    affil: Australian Data Archives (ANU)
  header-includes:
    - \usepackage{graphicx}
    - \usepackage{url}   
  output:
    pdf_document:
      fig_caption: yes
      keep_tex: yes
      number_sections: yes
      template: components/manuscript.latex
    html_document: null
    word_document: null
  fontsize: 11pt
  capsize: normalsize
  csl: meemodified.csl
  documentclass: article
  classoption: a4paper
  spacing: singlespacing
  linenumbers: no
  bibliography: references.bib
  abstract: no
  ---
  <!--
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Example Manuscript}
  -->
  ```{r, eval = F, echo = F}
  setwd("~/projects/opensoftware-restricteddata.github.com/report1_high_level/")
  library(rmarkdown)
  library(knitr)
  library(knitcitations)
  library(bibtex)
  cleanbib()
  #rm("bib")
  #options("cite_format"="pandoc")
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  
  dir()
  render("manuscript.Rmd", "pdf_document")
  browseURL("manuscript.pdf")
  ```
  ```{r, echo = F, results = 'hide'}
  # load
  if(!exists("bib")){
  bib <- read.bibtex("~/references/library.bib")
  
  for(bibkey in c("SarathiBiswas2012",
    "Mcmichael2002a", "Gelman2013"
  )){
  bib[[bibkey]]$url <- gsub("\\{\\\\_\\}","_", bib[[bibkey]]$url)
  bib[[bibkey]]$url <- gsub("\\{~\\}","~", bib[[bibkey]]$url)
  }
  
  }
  ```
  ```{r setup, include=FALSE, echo=FALSE}
  #Put whatever you normally put in a setup chunk.
  #I usually at least include:
  #devtools::install_github("manuscriptPackage","jhollist")
  #library("manuscriptPackage")
  #Didn't do that here to expedite building of the example vignette
  library("knitr")
  
  opts_chunk$set(dev = 'pdf', fig.width=6, fig.height=5)
  
  # Table Captions from @DeanK on http://stackoverflow.com/questions/15258233/using-table-caption-on-r-markdown-file-using-knitr-to-use-in-pandoc-to-convert-t
  #Figure captions are handled by LaTeX
  
  knit_hooks$set(tab.cap = function(before, options, envir) {
                    if(!before) { 
                      paste('\n\n:', options$tab.cap, sep='') 
                    }
                  })
  default_output_hook = knit_hooks$get("output")
  knit_hooks$set(output = function(x, options) {
    if (is.null(options$tab.cap) == FALSE) {
      x
    } else
      default_output_hook(x,options)
  })
  ```
  
  ```{r analysis , include=FALSE, echo=FALSE, cache=FALSE}
  #All analysis in here, that way all bits of the paper have access to the final objects
  #Place tables and figures and numerical results where they need to go.
  ```
  
  <!-- Abstract is being wrapped in latex here so that all analysis can be run in the chunk above and the results reproducibly referenced in the abstract. -->
  
  Draft: \today
  
  \singlespace
  
  \vspace{2mm}\hrule
  \paragraph*{Background:} This essay was written to accompany the material presented as a
  speedtalk and poster at the National Climate Change Adaptation
  Research Facility Conference 'Climate Adaptation knowledge and
  partnership', June 2013, Sydney.  The poster and slideshow are both available to download from this website: [http://swish-climate-impact-assessment.github.io/opensoftware-restricteddata/presentations-nccarf-2013/](http://swish-climate-impact-assessment.github.io/opensoftware-restricteddata/presentations-nccarf-2013/)
  
  \paragraph*{Methods:} The paper reports on a project to build tools and procedures for enhancing open and transparent analysis of restricted datasets.
  Some datasets such as suicide or climate
  change scenarios need to be accessed in a restricted way.  On the
  other hand scientists need to make their methods, models and
  assumptions transparent and available for scientific debate even
  though the datasets may require authorisation to access.
      
  \paragraph*{Results:} We built a secure server/client computational
  environment for using open software with restricted data.  We
  demonstrate the use of this system using drought and suicide as a case study.  
  We describe the potential use of this system in modelling climate change scenarios.
  
  \paragraph*{Conclusions:} The project shows that restricted data and
  open software can be used in an appropriate way to further the
  progress of scientific enquiry.
  
  \vspace{3mm}\hrule
  \doublespace
  
  # Background 
  ## Open software for restricted data
  
  
  Some datasets such as sensitive personal information about suicide or
  climate change scenarios with protected intellectual property need to
  be accessed in a restricted way.  In the context of reproducible
  research methods, models and assumptions need to be made
  transparent and available for scientific debate even though the
  datasets may require authorisation to access `r citep(bib[["Peng2011"]])`.
  
  Restrictions around access to data have increased recently in
  Australia, especially to the national mortality database after the
  discovery of an incident in which Australian population health
  researcher Dr Stephen Begg was reported to have 
  hacked into information about
  deaths in Australia `r citep(bib[["OKeefe2007"]])`.  The subsequent
  investigations led to a wide ranging modification to the procedures
  for approval and provision of these data that made the access much
  more restricted. No new research applications were approved to access these data between 2009 and 2013.
    
  At the same time the reproducibility crisis `r citep(bib[["Peng2011"]])` has emerged,
  reducing public confidence in statements by scientists. The true
  extent of the problem may turn out to be overstated
  `r citep(bib[["Jager2014"]])` however the concern should be addressed
  to avoid the problems that a lack of confidence in scientific publications
  would entail, especially in respect to evidence-based policy and expenditure of public money. Appropriate access to data and analytic software
  addresses this issue.  We investigated available workflow tools for
  data management and analysis and implemented a range of these products
  on our server.  This server has enhanced our capacity for
  experimentation, reviews, revisions and extensions of work in this
  field.  We present the results of this project and report that it has
  streamlined access to population health and environmental data for
  analysis.
  
  ## Motivating case study: Drought and suicide
  
  The impact of drought on mental health is plausible, however remains a gap in epidemiological knowledge `r citep(bib[["Stanke2013"]])`.  There is concern too that this health risk will rise under future climate change `r citep(bib[["Berry2008"]])`.   As mentioned the number of studies that have examined the relationship
  between suicide and drought is limited to only a handful `r citep(c(bib[["Nicholls2006c"]], bib[["Page2002c"]], bib[["Guiney2012a"]], bib[["Hanigan2012e"]]))`.  The motivating case for this project was to use the historical exposure-response functions to estimate future climate change impacts.
  
  There has been substantial public interest within Australia in recent decades of the putative relationship between drought and rural mental health, including suicide. The topic has frequently been raised by the media, by rural politicians and by mental health support groups `r citep(bib[["AustralianB2006"]])`. There have also been media reports in India indicating substantial concerns about drought and rural suicide in that country `r citep(bib[["SarathiBiswas2012"]])`.
  
  The number of studies that have examined the relationship between suicide and drought is limited. However, many papers explore links between suicide and climate variables other than drought (such as temperature) and there are two major reviews papers available of the literature on climatic influences on suicide `r citep(c(bib[["Dixon2009"]],bib[["Deisenhammer2003b"]]))`. However, very few studies have investigated drought specifically.
  
  There are several mechanisms through which unusually low rainfall, especially if exacerbated by increased soil dryness due to higher temperatures may increase the suicide rate. First, droughts increase the financial stress on farmers and farming communities (even if partially compensated by drought relief welfare payments). Such difficulty may occur in conjunction with other economic stresses, such as rising interest rates, falling commodity prices, or an unfavourable foreign exchange rate.  `r citet(bib[["Vins2015"]])` provide a systematic  literature review of the mental health effects of drought and explore the putative causal mechanisms.
  
  
  # Methodology
  
  The approach we took to meet the challenge of analysing restricted suicide and climate change scenario data in a safe environment was to
  build a new hardware and software stack using open-source software.  We based our
  planning on the realisation that there is a growing need of these
  technologies in the context of reproducible research.  This
  requires that methods, models and assumptions need to be made
  transparent and available for scientific debate even though the
  datasets may require authorisation to access.  This is true not just
  in health data, but also including the context 
  of data  with restrictive intellectual property and licence requirements (such as climate change scenario models).
  
   
  
  To develop an over-all view of
  the issue and analyse the dimensions of the problem we spent the
  initial phase of the project conceptualising a rich picture
  of the issue, and focused on identifying risks that the project might
  face. Several papers that describe similar systems were reviewed `r citep(c(bib[["Evans2012"]], bib[["Fleming2014"]]))` and 
  several recommendations from these papers were adopted in our system. 
  
  Our design responds directly to the primary threat of unintentional release of
  sensitive data so we decided to build a secure server/client environment
  for analysts to develop their software in an open way, while ensuring
  the safety of the datasets.  Other risks we identified were in
  relation to the provision of the server hardware and we were able to
  take advantage of the Australian 'Nectar Research Cloud' (\url{http://nectar.org.au}) for virtual machines to build the servers on.
  
  Then we defined the scope and quality of the project outcomes that we
  were aiming to deliver.  The fact that restrictions around access to
  data have increased recently, coupled with arguments that appropriate
  access to analytic software is needed to address the reproducibility
  crisis meant that the scope of this project was very broad.  We also
  explored the ambitions of our stakeholders to support their publishable
  outputs with open software.  Given that examples of un-reproducible work
  has spread even to the results published in top journals `r citep(bib[["Peng2015"]])`,
  the scope we decided to set for this project was
  for very high levels of open-ness for the evidence being presented for
  peer-review, along with very high levels of restriction on access to
  the data. Luckily however we were able to rule out the need for the
  extreme level of restriction such as getting Australian Defence Science and
  Technology Organisation (DSTO) accreditation for the security of these
  servers against malicious hacking attacks.  Our servers just needed to be tested by the standard 'vulnerability exploits' scanner used by the Australian National University IT Department. 
  
  We also looked at the workflow system Kepler to assess it\'s utility
  for providing access to the data, but found that there were a lot of
  limitations at the time `r citep(bib[["Curcin2008"]])` and decided
  that the R environment for statistical computing and graphics would be
  the platform we would focus on.
  
  During the next phase of the project we dealt with issues of the costs
  associated with developing the software and hosting the hardware at
  different locations, as well as the time needed to test and get user
  acceptance on the services.
  
  ## System design
  
  In this case study we utilise Virtual Machines (VMs) in the cloud. Our
  system requires two VMs so that the storage and processing of data can
  be compartmentalised, with various benefits.  A high level overview of
  the system is shown in Figure \ref{fig:sys}.  Full details including
  Linux commands and configuration specifications are available online at
  [http://swish-climate-impact-assessment.github.io/opensoftware-restricteddata/](http://swish-climate-impact-assessment.github.io/opensoftware-restricteddata/).
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.85\textwidth]{opensoft.pdf}
  \caption{High Level Schematic System Design, colours indicate restrictions (red), open (blue)}
  \label{fig:sys}
  \end{figure}
  
  
  ## Software selections
  We researched a variety of systems and found the following set-up worked best for us.
  
  Linux cluster:
  
  - National Research Cloud [www.nectar.org.au/research-cloud](www.nectar.org.au/research-cloud)
  - Centos 6.4 [www.centos.org](www.centos.org)
  
  Geographical Information Systems (GIS) database server:
  
  - PostgreSQL 9.2 [www.postgresql.org](www.postgresql.org)
  - PostGIS 2.0 [http://postgis.refractions.net](http://postgis.refractions.net)
  
  Statistical analysis server:
  
  -  R language for statistical computing [www.r-project.org](www.r-project.org)
  -  Rstudio server [www.rstudio.com](www.rstudio.com)
  -  OpenGeo Suite [http://opengeo.org](http://opengeo.org)
  
  Information management:
  
  - Projects,UsersDB Oracle XE APEX [www.oracle.com](www.oracle.com)
  - Data Catalogue [http://assda.anu.edu.au/ddiindex.html](http://assda.anu.edu.au/ddiindex.html)
  
  The client side:
  
  -  Any standard web-browser
  -  The Kepler Project [www.kepler-project.org](www.kepler-project.org)
  -  pgAdmin [www.pgadmin.org](www.pgadmin.org)
  -  Git Version Control and GitHub [www.github.com](www.github.com)
  -  Emacs code editor with the starter-kit by Kieran Healy and orgmode [http://kieranhealy.org/resources/emacs-starter-kit/](http://kieranhealy.org/resources/emacs-starter-kit/)
  
  # Case study 
  
  ## Step 1: Historical exposure-response functions
  
  For this case study we replicated the work we had previously conducted
  on our personal desktop computers within the 
  university.  A key result is shown in Figure \ref{fig:Figure1.png}.  That
  work was published already (Hanigan \emph{et al.} 2012), however
  the improved IT infrastructure offered by this project allows the
  analysis to be re-run from a secure web-browser interface.  Such
  improved access allowed a much broader discussion of the data,
  techniques and results because the researcher was able to discuss the
  details of the modelling with other scientists at meetings, conferences and
  workshops, while actually repeating the computations in real time.
  This is a vast improvement over the previous option of leaving the
  data analysis on the secure desktop computers at the university, and
  merely describing the computations to colleagues at the workshops.
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.45\textwidth]{Figure1.png}
  \caption{Drought exposure-response functions Rural Males}
  \label{fig:Figure1.png}
  \end{figure}
  
  ## Step 2: Future drought scenarios and attributable fraction of suicides
  
  Following the methods of `r citet(bib[["Climate2008"]])` we used the climate change scenarios provided for the Garnaut Climate Change Review `r citep(bib[["CSIRO2013"]])` to project estimates of future suicides under various drought conditions.  The statistical method for this calculation is:
  
  $$Y_{ijk}=\sum_{lm}(e^{(\beta_{ijk}} \times { X_{lm})} - 1) \times {BaselineRate_{jkl}} \times { Population_{jklm}}$$
  
  \noindent Where:
  
  $\beta_{ijk}$ = the ExposureVariable coefficient for zone$_i$, age$_j$ and sex$_{k}$ 
  
  ${X_{lm}}$ = Projected Future Exposure Variables  
  
  \emph{BaselineRate$_{jkl}$} = \emph{avgDeathsPerTime}/\emph{avgPopPerTime} in age$_j$, sex$_k$ and zone$_l$ 
  
  \emph{Population$_{jklm}$} = projected populations by age$_j$, sex$_k$, zone$_l$ and time$_m$
  
  
  ```{r, eval = F, echo = F}
  dir()
  dir("~/data")
  indir <- "~/data/GARNAUT mental health archive/1 Data/Rain"
  dir(indir, pattern = "jpg")
  filelist  <- c("A1BDRY_RainSD07.jpg",             "A1BWET_RainSD07.jpg"            )
  for(fi in filelist){
    #file.copy(file.path(indir, fi), fi)
    txt <- sprintf("\\begin{figure}[!h]
  \\centering
  \\includegraphics[width=\\textwidth]{%s}
  \\caption{%s}
  \\label{fig:%s}
  \\end{figure}
  ", fi, fi, fi)
    cat(txt)
  }
  ```
  
  ## Results
  
  In table \ref{tab:table1} below the two rainfall scenarios used by `r citet(bib[["Berry2008"]])` are used to demonstrate this drought impact assessment report.  For full details of the data from the Garnaut Climate Change Review please refer to the original papers. For the purpose of this discussion only a brief summary is required to appreciate the relevance to this case study. The result shown below merely shows that the estimated impact of climate change can vary a lot given the input datasets and the assumptions and methods applied during analysis.  The codes used to fit models, project scenarios and estimate burden of deaths are all available on the system, and can be assessed by request at the project website: \url{http://swish-climate-impact-assessment.github.com/}
  
  \bigskip
  
  ```{r, eval = T, echo = F, results = 'asis'}
  library(xtable)
  tab <- read.csv(textConnection("Scenario, Deaths per annum, Lower 95th CI, Upper 95th percent CI
  Historical (1970-2008), 4.01, 2.14, 6.05
  A1FIR1 (Dry), 8.91, 4.56, 14.00
  A1FIR2 (Wet), 2.93, 1.5, 4.47
  "), strip.white = T)
  #tab
  names(tab) <- gsub("\\.", " ", names(tab))
  print(xtable(tab, caption = 'Estimated number of rural male suicides attributable to drought between 2000 and 2100', label = 'tab:table1'), comment = F, include.rownames = F)
  
  ```
   
  \clearpage
  
  Below are pictorial represenations of the climate change scenarios estimated annual total rainfall for each Statistical Division (SD) census region across the country.  SDs are used because this is the geographical unit at which the suicide rates were analysed.
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.5\textwidth]{A1BDRY_RainSD07.jpg}
  \caption{A1BDRY RainSD07}
  \label{fig:A1BDRYRainSD07}
  \end{figure}
  
  
  \begin{figure}[!h]
  \centering
  \includegraphics[width=.5\textwidth]{A1BWET_RainSD07.jpg}
  \caption{A1BWET RainSD07}
  \label{fig:A1BWET-RainSD07}
  \end{figure}
  
  
  
  # Discussion 
  ## Principal findings

  We demonstrate the use of this system by using
  the climate change scenarios held on the database to project future
  droughts, and use the confidential suicide data that is safely stored
  there to estimate baseline risks and future burden of suicide
  attributable to the droughts.  Because the server system is easy to
  access through a web browser these results are easily reproducible and
  alternate scenarios and assumptions can be tested.  Because the webserver is very secure, this work can be done with confidence that the risk of unauthorised data release is minimised.
  
  The results of our project are applicable more generally than just
  reproducible research. For instance in relation to issues of
  governance and management at the multidisciplinary institutions and
  multi-institutional projects it is vitally important that data sharing
  is enabled, while some data are still restricted because access requires
  approval and authorisation, or because data providers wish to be informed of re-use and identifying the profile of
  users of the data.  It is important that data access
  can be made restrictive WITHOUT impeding the progress of the local
  science agenda (collaborations, workshops, papers etc) and keeping the
  relevant data custodian parties informed about what is happening with
  the release of their datasets.  The open-ness of the analytical
  software also has a positive effect on the value of the data
  infrastructure (through education and outreach activities) without
  risking any unethical or negligent use of these datasets.
  
  #  Conclusions 
  
  In summary we found that:
  
  - Climate data can be attached to suicide and census data in a secure computing environment.
  - Future drought associated deaths can be estimated.
  - These estimates are very uncertain, contentious and difficult to justify.
  - Data management and analysis technology such as that presented is needed to enable rigorous and transparent exploration that challenges or verifies published results.
  
  This system described:
  
  - Enables data analysis in a safe environment.
  - Allows comparison of multiple climate scenarios and assumptions.
  - Demonstrated with a climate/health impact assessment.
  - Addresses the reproducibility crisis.
  
  
  
  # References
  
  ```{r, echo=FALSE, message=FALSE, eval = T}
  write.bibtex(file="references.bib")
  ```
  
  
#+end_src
