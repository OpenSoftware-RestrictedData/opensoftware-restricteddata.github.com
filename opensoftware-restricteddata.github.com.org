#+TITLE:Open Software for Restricted Data: an Environmental Epidemiology example
#+AUTHOR: Ivan Hanigan, Steven McEachern and David Fisher 
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LaTeX_HEADER: \usepackage{verbatim}
#+LaTeX_HEADER: \graphicspath{{./reports/}}
* Abstract
#+BEGIN_abstract
Ivan Hanigan 1, Steven McEachern 2, David Fisher 3.


- 1 National Centre for Epidemiology and Population Health, Australian National University
- 2 Australian Data Archive, Australian National University
- 3 Information Technology Services, Australian National University

Last Updated 8 March 2013

Increasing concerns over privacy in Australia and globally, combined
with the risk from hacking and the accidental release of large-scale
data sets is leading to increased restrictions on the use of
confidential, highly sensitive health data. This is coincident with
increased statistical and computational power, with the potential to
glean many new insights from already collected data. Unfortunately,
however, the two trends risk cancelling each other out. It is thus
imperative that universities and other institutions who have access to
large data sets manage them in ways that maintain organisational and
public confidence in their integrity.

This paper presents the design and development of a Virtual Laboratory
for analysing restricted data using open software.  These tools were
assembled with the aim to allow users to access restricted data in an
appropriate and safe manner whilst allowing use of open software to
enhance reproducibility and accessibility.  The system implementation
is described specifically for the Australian National Research Cloud
http://www.nectar.org.au/research-cloud/.

We present a case study of an application from Environmental
Epidemiology using confidential health records, which was a motivating
reason for us to develop this system.  In the example presented here,
we provide a simple analysis of the distribution of suicides with
drought across NSW and also with votes for the conservative parties;
both of which have previously been found to increase the risk of
suicides in NSW. The paper then concludes with a reflection on the
implications of applying these open software tools to restricted
access data such as the Australian Deaths dataset.
#+END_abstract
#+LATEX: \tableofcontents
#+LATEX: \listoftables
#+LATEX: \listoffigures
-----
#+name:header
#+begin_src R :session *shell* :tangle no :exports none :eval no
  paste(getwd(),'\n',Sys.Date())
#+end_src

#+RESULTS: header
| /home/ivan/projects/OpenSoftware-RestrictedData |
|                                      2013-02-22 |

* COMMENT layout
** default
#+name:aboutus
#+begin_src html :tangle _layouts/default.html :exports none :eval no
  <!doctype html>
  <html>
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="chrome=1">
      <title>OpenSoftware-RestrictedData - {{ page.title }}</title>
  
      <link rel="stylesheet" href="stylesheets/styles.css">
      <link rel="stylesheet" href="stylesheets/pygment_trac.css">
      <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
      <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->
    </head>
    <body>
      <div class="wrapper">
        <header>
          <!--<h1>OpenSoftware</h1>-->
          <strong id="blog-title">
            <a href="http://opensoftware-restricteddata.github.com" rel="home"><h1>Open Software -</h1></a>
            <a href="http://opensoftware-restricteddata.github.com" rel="home"><h1>Restricted Data</h1></a>
          </strong>
          <!--<p>random-website</p>-->
  
          <p>Links:</p>
                <a class="Contact the project" href="mailto:ivan.hanigan@gmail.com">Contact the project →</a>  
                <!--<p></p>-->
                <!--<a class="Font of all wisdom" href="www.google.com">Font of all wisdom →</a>-->  
                <p></p>
                <a class="About" href="/aboutus.html">About →</a>
                <!--<a class="About" href="/aboutus.html">About →</a>-->  
                <p></p>
                <p><a class="Version 1: 'the Compendium'" href="http://ivanhanigan.github.com/OpenSoftware-RestrictedData/">Version 1: 'the Compendium' →</a></p>
                <p></p>
                <p><a class="Site map" href="/sitemap.html">Site map →</a></p>
  
  
  
        </header>
        <section>
          <h3>{{ page.title }}</h3>
  
  <!--<p>This was generated by Github's automatic webpage generator.</p>-->
  
  <p>
            {{ content }}
  </p>
  <div id="disqus_thread"></div>
    <script type="text/javascript" src="http://disentanglethings.disqus.com/embed.js"> </script>
    <noscript>Please enable JavaScript to <a href="http://disentanglethings.disqus.com/?url=ref">view the discussion thread.</a></noscript>
        </section>
        <footer>
          <p>This project is maintained by <a href="https://github.com/ivanhanigan">ivanhanigan</a></p>
          <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
          <!--<p><small><a href="/indexlocal.html">Ivan's temporary local version for debugging</a></small></p>-->
  
        </footer>
      </div>
  
  
  
      <script src="javascripts/scale.fix.js"></script>    
    </body>
  </html>
  
#+end_src


** Open Software - Restricted Data.
*** Index
**** Index-head
#+name:index
#+begin_src markdown :tangle index.md :exports none :eval no :padline no
--- 
name: project-overview
layout: default
title: Project Overview 
---
#+end_src
**** Index Abstract
#+name:Index-prose
#+begin_src markdown :tangle index.md :exports reports :eval no
## NB This is a test!
I am in the process of rewriting the original document into these web-pages.
Please see "Version 1" link on left side bar for the document as of early 2013.
Current work is availale from the "Site map" link ont the sidebar, with pages that have not been transferred yet labelled as "TODO".

## The Rationale: 
The reason this site exists is because analysing restricted data (such as on human health) is fraught with security issues that hamper statistical analysis and subsequent evidence based policy.

An important concept for modern applied statistics is the Replication Standard [as originally described by King 1995](http://www.jstor.org/stable/10.2307/420301) and called for by [Peng in his 2011 editorial for Science 'Reproducible research in computational science' (334;6060)](http://www.sciencemag.org/content/334/6060/1226.full).  

Increasing concerns over privacy in Australia and globally, combined
with the risk from hacking and the accidental release of large-scale
data sets is leading to increased restrictions on the use of
confidential, highly sensitive health data. This is coincident with
increased statistical and computational power, with the potential to
glean many new insights from already collected data. Unfortunately,
however, the two trends risk cancelling each other out. It is thus
imperative that universities and other institutions who have access to
large data sets manage them in ways that maintain organisational and
public confidence in their integrity.

We demonstrate that with the appropriate IT Infrastructure this can be achieved even in cases where the source data used in analyses are restricted.  The pages of this site presents the design and development of a Virtual Laboratory
for analysing restricted data using open software.  These tools were
assembled with the aim to allow users to access restricted data in an
appropriate and safe manner whilst allowing use of open software to
enhance reproducibility and accessibility.  The system implementation
is described specifically for the [Australian National Research Cloud
provided by the NeCTAR group](http://www.nectar.org.au/research-cloud/).

#+end_src
**** COMMENT DEPRECATED TEXT
# We present a case study of an application from Environmental
# Epidemiology using confidential health records, which was a motivating
# reason for us to develop this system.  In the example presented here,
# we provide a simple analysis of the distribution of suicides with
# drought across NSW and also with votes for the conservative parties;
# both of which have previously been found to increase the risk of
# suicides in NSW. The paper then concludes with a reflection on the
# implications of applying these open software tools to restricted
# access data such as the Australian Deaths dataset.

**** COMMENT Index-code
#+name:asdf
#+begin_src markdown :session *R* :tangle no :exports code :eval no
  #### Test Code Chunk ####
      x <- rnorm(100,1,2)
      png("images/hist_x.png")
      hist(x)
      dev.off()
  
#+end_src
**** COMMENT Index place mark
#+begin_src markdown :tangle no :exports reports :eval no
The R code produces the plot:
#+end_src

**** COMMENT Index-graph-code
#+name:Index-graph
#+begin_src markdown :tangle no :exports none :eval no
<!--![plot](/images/hist_x.png)-->
![plot](/images/hist_x.png)
#+end_src
**** COMMENT Index-refs-code
#+name:Index-refs
#+begin_src markdown :tangle no :exports reports :eval no
[1]: http://www.nectar.org.au/research-cloud/ "Nectar"
#+end_src

* Site map
#+name:introduction
#+begin_src html :tangle sitemap.md :exports none :eval no :padline no
  --- 
  name: sitemap
  layout: default
  title: Site Map
  ---
  
  <div id="table-of-contents">
  <!-- <h2>Table of Contents</h2> -->
  <div id="text-table-of-contents">
  <ul>
  <li><a href="/aboutus.html">1 About us</a></li>
  <li><a href="/introduction.html">2 Introduction </a></li>
  <li><a href="#sec-3">TODO 3 Deploying Virtual Machines</a>
  <ul>
  <li><a href="#sec-3-1">TODO 3.1 Description</a></li>
  <li><a href="#sec-3-2">TODO 3.2 Accessing Virtual Machines on the Australian  Research Cloud</a>
  <!-- <ul> -->
  <!-- <li><a href="#sec-3-2-1">TODO 3.2.1 Launch an instance</a></li> -->
  <!-- <li><a href="#sec-3-2-2">TODO 3.2.2 requesting help</a></li> -->
  <!-- <li><a href="#sec-3-2-3">TODO 3.2.3 connect using ssh</a></li> -->
  <!-- </ul> -->
  t</li>
  <li><a href="#sec-3-3">TODO 3.3 Setting up the basic server framework</a>
  <!-- <ul> -->
  <!-- <li><a href="#sec-3-3-1">TODO 3.3.1 Install any updates using the yum package manager</a></li> -->
  <!-- </ul> -->
  </li>
  <li><a href="#sec-3-4">3.4 Security measures</a>
  <ul>
  <li><a href="/iptables.html">3.4.1 Restrict firewall to a specific ip address/range</a></li>
  <li><a href="/setting-the-host-tcp-wrappers.html">3.4.2 Setting the hosts.* TCP wrappers </a></li>
  <li><a href="#sec-3-4-3">TODO 3.4.2 Security Enhanced Linux (selinux)</a></li>
  <!-- <li><a href="#sec-3-4-3">TODO 3.4.3 Install some base packages</a></li> -->
  </ul>
  </li>
  <li><a href="#sec-3-5">TODO 3.5 Hardware set-up</a>
  <ul>
  <li><a href="/swapon.html">3.5.1 Swap space </a></li>
  <li><a href="#sec-3-5-2">TODO 3.5.2 Persistent Net Rules Should Be Avoided On Centos</a></li>
  <li><a href="#sec-3-5-3">TODO 3.5.3 Disk Storage</a></li>
  </ul></li>
  </ul>
  </li>
  <li><a href="#sec-4">TODO 4 The Brawn</a>
  <ul>
  <li><a href="/postgresql.html">4.1 PostgreSQL </a>
  <ul>
  <li><a href="#sec-4-1-1">TODO 4.1.1 Configure PostgreSQL connection settings</a></li>
  <li><a href="#sec-4-1-2">TODO 4.1.2 Allow connection to postgres through the firewall</a></li>
  </ul>
  </li>
  <li><a href="/postgis.html">4.2 PostGIS 2.0 </a>
  <ul>
  <li><a href="/postgis.html">4.2.1 Postgis</a></li>
  <li><a href="#sec-4-2-2">TODO 4.2.2 GDAL, PROJ and GEOS</a></li>
  <li><a href="#sec-4-2-3">TODO 4.2.3 Create Database</a></li>
  <li><a href="#sec-4-2-4">TODO 4.2.4 Create a GIS user and a group</a></li>
  <li><a href="#sec-4-2-5">TODO 4.2.5 Specific transformations grid for Australian projections AGD66 to GDA94</a></li>
  <li><a href="#sec-4-2-6">TODO 4.2.6 Test transform</a></li>
  </ul>
  </li>
  <li><a href="/postgres-migrate.html">4.3 PostgreSQL Migration </a>
  <ul>
  <li><a href="/postgres-migrate.html">4.3.1 Migrate the data</a></li>
  <li><a href="#sec-4-3-2">TODO 4.3.2 Set up backups</a></li>
  </ul>
  </li>
  <li><a href="/sharedmemory.html">4.4 Important shared memory settings </a></li>
  <li><a href="#sec-4-5">TODO 4.5 Test loading some shapefiles</a></li>
  </ul>
  </li>
  <li><a href="#sec-5">TODO 5 The Brains</a>
  <ul>
  <li><a href="#sec-5-1">TODO 5.1 R Server</a>
  <ul>
  <li><a href="#sec-5-1-1">TODO 5.1.1 R</a></li>
  <li><a href="#sec-5-1-2">TODO 5.1.2 package management and R updates</a></li>
  <li><a href="#sec-5-1-3">TODO 5.1.3 Rstudio</a></li>
  <li><a href="#sec-5-1-4">TODO 5.1.4 firewall access</a></li>
  <li><a href="#sec-5-1-5">TODO 5.1.5 SSL/HHTPS and running a proxy server</a></li>
  <li><a href="#sec-5-1-6">TODO 5.1.6 git</a></li>
  <li><a href="#sec-5-1-7">TODO 5.1.7 ssh for github</a></li>
  <li><a href="#sec-5-1-8">TODO 5.1.8 gdal</a></li>
  <li><a href="#sec-5-1-9">TODO 5.1.9 geos</a></li>
  <li><a href="#sec-5-1-10">TODO 5.1.10 or under ubuntu</a></li>
  <li><a href="#sec-5-1-11">TODO 5.1.11 test readOGR</a></li>
  <li><a href="#sec-5-1-12">TODO 5.1.12 rgraphviz</a></li>
  <li><a href="#sec-5-1-13">TODO 5.1.13 test</a></li>
  <li><a href="#sec-5-1-14">TODO 5.1.14 install just the postgres bits required for RPostgreSQL package</a></li>
  <li><a href="#sec-5-1-15">TODO 5.1.15 postgis utilities</a></li>
  <li><a href="#sec-5-1-16">TODO 5.1.16 unixODBC</a></li>
  </ul>
  </li>
  <li><a href="#sec-5-2">TODO 5.2 Test the Backups of this Minimal R Sever.</a>
  <ul>
  <li><a href="#sec-5-2-1">TODO 5.2.1 Backup the 2nd Disc</a></li>
  <li><a href="#sec-5-2-2">TODO 5.2.2 Launch from this snapshot and test the R server and 2nd Disc</a></li>
  <li><a href="#sec-5-2-3">TODO 5.2.3 the permissions of the user on their home directory cause issues for logging in.</a></li>
  </ul>
  </li>
  <li><a href="#sec-5-3">TODO 5.3 Oracle XE Permissions and Users System</a>
  <ul>
  <li><a href="#sec-5-3-1">TODO 5.3.1 backup local ubuntu version</a></li>
  <li><a href="#sec-5-3-2">TODO 5.3.2 INIT</a></li>
  <li><a href="#sec-5-3-3">TODO 5.3.3 SWAP</a></li>
  <li><a href="#sec-5-3-4">TODO 5.3.4 DOWNLOAD AND SCP</a></li>
  <li><a href="#sec-5-3-5">TODO 5.3.5 Install the database</a></li>
  <li><a href="#sec-5-3-6">TODO 5.3.6 import application and set Security</a></li>
  <li><a href="#sec-5-3-7">TODO 5.3.7 explain the table creation script</a></li>
  <li><a href="#sec-5-3-8">TODO 5.3.8 set up R, RJDBC and ROracle</a></li>
  </ul>
  </li>
  <li><a href="/opengeosuite.html">5.4 OpenGeo Suite </a>
  <ul>
  <li><a href="/opengeosuite.html">5.4.1 The OpenGeo Suite Installer</a></li>
  <li><a href="/opengeosuite-upgrade-tomcat6.html">5.4.2 Upgrade to latest tomcat6 version</a></li>
  <li><a href="#sec-5-4-3">TODO 5.4.3 Otherwise just install normal geoserver</a></li>
  <li><a href="#sec-5-4-4">TODO 5.4.4 configure geoserver</a></li>
  <li><a href="#sec-5-4-5">TODO 5.4.5 expose spatial data</a></li>
  </ul>
  </li>
  <li><a href="#sec-5-5">TODO 5.5 DDIindex</a>
  <ul>
  <li><a href="#sec-5-5-1">TODO 5.5.1 TOMCAT</a></li>
  <li><a href="#sec-5-5-2">TODO 5.5.2 TOMCAT upgrade 6 to 7</a></li>
  <li><a href="#sec-5-5-3">TODO 5.5.3 Unsuccessfully did 7, try latest 6</a></li>
  <li><a href="#sec-5-5-4">TODO 5.5.4 UPLOAD THE DDIINDEX</a></li>
  <li><a href="#sec-5-5-5">TODO 5.5.5 add explanation of the ddiindex.zip file in lib</a></li>
  <li><a href="#sec-5-5-6">TODO 5.5.6 MySQL</a></li>
  <li><a href="#sec-5-5-7">TODO 5.5.7 make sure ddiindex can connect to mysql as ddiindex user</a></li>
  <li><a href="#sec-5-5-8">TODO 5.5.8 Check the security implications of allowing write permissions here</a></li>
  <li><a href="#sec-5-5-9">TODO 5.5.9 Running the Indexer</a></li>
  <li><a href="#sec-5-5-10">TODO 5.5.10 personalise the ddiindex</a></li>
  </ul>
  </li>
  <li><a href="#sec-5-6">TODO 5.6 Set up a private git lab for data and code</a>
  <ul>
  <li><a href="#sec-5-6-1">TODO 5.6.1 Firewall</a></li>
  <li><a href="#sec-5-6-2">TODO 5.6.2 SSH server</a></li>
  <li><a href="#sec-5-6-3">TODO 5.6.3 Encrypted files</a></li>
  </ul>
  </li>
  <li><a href="#sec-5-7">TODO 5.7 True-Crypt encrypted volumes</a></li>
  <li><a href="#sec-5-8">TODO 5.8 ResearchData storage</a></li>
  </ul>
  </li>
  <li><a href="#sec-6">TODO 6 Procedures for add users to the system.</a>
  <ul>
  <li><a href="#sec-6-1">TODO 6.1 Description of the access procedure</a>
  <ul>
  <li><a href="#sec-6-1-1">TODO 6.1.1 Getting Access</a></li>
  <li><a href="#sec-6-1-2">TODO 6.1.2 Managing Access</a></li>
  <li><a href="#sec-6-1-3">TODO 6.1.3 Ending Access</a></li>
  </ul>
  </li>
  <li><a href="#sec-6-2">TODO 6.2 The process user administrators go through to set up users</a>
  <ul>
  <li><a href="#sec-6-2-1">TODO 6.2.1 lodge request in user db</a></li>
  <li><a href="#sec-6-2-2">TODO 6.2.2 r-nceph</a></li>
  </ul>
  </li>
  <li><a href="#sec-6-3">TODO 6.3 brains</a>
  <ul>
  <li><a href="#sec-6-3-1">TODO 6.3.1 linux user</a></li>
  <li><a href="#sec-6-3-2">TODO 6.3.2 mysql (check ddiindex)</a></li>
  </ul>
  </li>
  <li><a href="#sec-6-4">TODO 6.4 brawn</a>
  <ul>
  <li><a href="#sec-6-4-1">TODO 6.4.1 postgres</a></li>
  <li><a href="#sec-6-4-2">TODO 6.4.2 geoserver</a></li>
  <li><a href="#sec-6-4-3">TODO 6.4.3 alliance wiki</a></li>
  <li><a href="#sec-6-4-4">TODO 6.4.4 text message the set password</a></li>
  </ul>
  </li>
  <li><a href="#sec-6-5">TODO 6.5 edits to oraphi via sql</a>
  <ul>
  <li><a href="#sec-6-5-1">TODO 6.5.1 revocation</a></li>
  </ul></li>
  </ul>
  </li>
  <li><a href="#sec-7">TODO 7 Backups</a>
  <ul>
  <li><a href="#sec-7-1">TODO 7.1 General</a>
  <ul>
  <li><a href="#sec-7-1-1">TODO 7.1.1 maintenance</a></li>
  <li><a href="#sec-7-1-2">TODO 7.1.2 nearline for potential restore</a></li>
  <li><a href="#sec-7-1-3">TODO 7.1.3 archive and remove</a></li>
  </ul>
  </li>
  <li><a href="#sec-7-2">TODO 7.2 General concerns</a></li>
  <li><a href="#sec-7-3">TODO 7.3 Brains</a>
  <ul>
  <li><a href="#sec-7-3-1">TODO 7.3.1 Backup oraphi</a></li>
  </ul>
  </li>
  <li><a href="#sec-7-4">7.4 Brawn</a>
  <ul>
  <li><a href="/brawn-dbsize.html">7.4.1 Find out how big is it?</a></li>
  <li><a href="/backup-brawn-filesystem.html">7.4.2 Dump and download it to a secure computer</a></li>
  <li><a href="/brawn-dump-restore.html">7.4.3 Restore databse dump into a new database on another machine.</a></li>
  <li><a href="#sec-7-4-5">TODO 7.4.4 launch a new Nectar VM from the snapshot image</a></li>
  <li><a href="#sec-7-4-6">TODO 7.4.5 mount the 2nd disc and load/restore the postgres db and data into it</a></li>
  </ul>
  </li>
  <li><a href="#sec-7-5">TODO 7.5 Disaster Recovery Plan</a>
  <ul>
  <li><a href="#sec-7-5-1">TODO 7.5.1 test a snapshot</a></li>
  <li><a href="#sec-7-5-2">TODO 7.5.2 60GB disk is not being saved in snapshots</a></li>
  <li><a href="#sec-7-5-3">TODO 7.5.3 Restore ORAPHI</a></li>
  </ul>
  </li>
  </ul>
  </li>
  </ul>
  </div>
  </div>
  
#+end_src
** old version
#+begin_src html
  --- 
  name: sitemap
  layout: default
  title: Site Map
  ---
  <div id="table-of-contents">
  <!--<h2>Table of Contents</h2>-->
  <div id="text-table-of-contents">
  <ul>
    <li><a href="/introduction.html">1 Introduction</a></li>
    <li><a href="/introduction.html">2 TODO Build and Deploy</a>
      <ul>
      <li><a href="/introduction.html">2.1 TODO Description</a></li>
      <li><a href="#sec-2-2">2.2 TODO Accessing Virtual Machines on the Australian  Research Cloud</a>
        <ul>
          <li><a href="#sec-2-2-1">2.2.1 TODO Launch an instance</a></li>
          <li><a href="#sec-2-2-2">2.2.2 TODO requesting help</a></li>
          <li><a href="#sec-2-2-3">2.2.3 TODO connect using ssh</a></li>
        </ul>
        <li><a href="#sec-2-3">2.3 Setting up the basic server framework</a></li>
        <ul>
          <li><a href="/swapon.html">2.3.1 Swap space</a></li>
          <li><a href="#sec">2.3.1 TODO Disk Storage</a></li>
        </ul>
        </li>
    </ul> 
    
    <ul> 
    <li><a href="#sec-3">3 The Brawn</a>
      <ul>
        <li><a href="/postgresql.html">3.1 PostgreSQL</a></li>
        <li><a href="/postgis.html">3.2 PostGIS</a></li>
        <li><a href="/postgres-migrate.html">3.3 PostgreSQL Migration</a></li>
        <li><a href="/sharedmemory.html">3.4 Important shared memory settings</a></li>
        <li><a href="#sec-2-3-3">3.5 TODO Test loading some shapefiles</a></li>
      </ul>
      </li>
        <li><a href="#sec-2-4">2.4 The Brains</a>
        <ul>
          <li><a href="#sec-2-4-1">2.4.1 TODO R Server</a></li>
          <li><a href="#sec-2-4-2">2.4.2 TODO Test the Backups of this Minimal R Sever.</a></li>
          <li><a href="#sec-2-4-3">2.4.3 TODO Oracle XE Permissions and Users System</a></li>
          <li><a href="/opengeosuite.html">2.4.4 OpenGeo suite</a></li>
          <li><a href="#sec-2-4-4">2.4.5 TODO DDIindex</a></li>
          <li><a href="#sec-2-4-5">2.4.6 TODO Set up a private git lab for data and code</a></li>
          <li><a href="#sec-2-4-6">2.4.7 TODO True-Crypt encrypted volumes</a></li>
          <li><a href="#sec-2-4-7">2.4.8 TODO ResearchData storage</a></li>
        </ul></li>
        </ul>
        </li>
        <li><a href="#sec-3">3 TODO Procedures for add users to the system.</a>
        <ul>
        <li><a href="#sec-3-1">3.1 TODO Description of the access procedure</a>
        <ul>
          <li><a href="#sec-3-1-1">3.1.1 TODO Getting Access</a></li>
          <li><a href="#sec-3-1-2">3.1.2 TODO Managing Access</a></li>
          <li><a href="#sec-3-1-3">3.1.3 TODO Ending Access</a></li>
        </ul>
        </li>
        <li><a href="#sec-3-2">3.2 TODO The process user administrators go through to set up users</a>
        <ul>
          <li><a href="#sec-3-2-1">3.2.1 TODO lodge request in user db</a></li>
          <li><a href="#sec-3-2-2">3.2.2 TODO r-nceph</a></li>
          <li><a href="#sec-3-2-3">3.2.3 TODO brains</a></li>
          <li><a href="#sec-3-2-4">3.2.4 TODO brawn</a></li>
          <li><a href="#sec-3-2-5">3.2.5 TODO alliance wiki</a></li>
          <li><a href="#sec-3-2-6">3.2.6 TODO text message the set password</a></li>
          <li><a href="#sec-3-2-7">3.2.7 TODO edits to oraphi via sql</a></li>
          <li><a href="#sec-3-2-8">3.2.8 TODO revocation</a></li>
        </ul></li>
        </ul>
        </li>
        <li><a href="#sec-4">4 TODO Backups</a>
        <ul>
        <li><a href="#sec-4-1">4.1 TODO General</a>
        <ul>
          <li><a href="#sec-4-1-1">4.1.1 TODO maintenance</a></li>
          <li><a href="#sec-4-1-2">4.1.2 TODO nearline for potential restore</a></li>
          <li><a href="#sec-4-1-3">4.1.3 TODO archive and remove</a></li>
        </ul>
        </li>
        <li><a href="#sec-4-2">4.2 TODO Specific</a>
        <ul>
        <li><a href="#sec-4-2-1">4.2.1 TODO Brains</a></li>
        <li><a href="#sec-4-2-2">4.2.2 TODO Brawn</a></li>
        </ul>
        </li>
        <li><a href="#sec-4-3">4.3 TODO Disaster Recovery Plan</a>
        <ul>
        <li><a href="#sec-4-3-1">4.3.1 TODO Test a snapshot</a></li>
        <li><a href="#sec-4-3-2">4.3.2 TODO 60GB disk is not being saved in snapshots</a></li>
        <li><a href="#sec-4-3-3">4.3.3 TODO Restore ORAPHI</a></li>
        </ul>
      </li>
      </ul>
    </li>
    </ul>
  </div>
  </div>
  
#+end_src

* COMMENT TODO-list
- TODO check http://forum.worldwindcentral.com/showthread.php?t=21409
#http://anotherdatabaseblog.blogspot.com.au/2012/04/installing-mapserv-on-centos-6.html
- TODO Tomcat is primary security concern, don't host on Postgres box (also due to resource sharing)
- TODO recommend regular updates and patchs to Tomcat
- TODO recommendations for password management (KeyPassX?)
- TODO Logging, Syslogging and correlation analysis
- TODO Postgres Crypto contributed module allows better security of data
- TODO SELinux is still a question
- TODO Nectar Public/Private keypair system can be switched off.  Ensure strong passwords
- TODO Recommend to restrict root log in, add secondary root as sudoer
- TODO Investigate SSL option on postgres

* COMMENT TODOLIST
** TODO ddiindex
** TODO summarise the stackoverflow questions
http://stackoverflow.com/questions/2006097/good-examples-of-build-and-deployment-documentation

** TODO gh-pages
http://oli.jp/2011/github-pages-workflow/
#+name:gh-pages
#+begin_src txt :tangle no :exports none :eval no
###########################################################################
# newnode: gh-pages
# TIP this requires flipping between branches so having the main org file open can cause confusion.  Close this before going thru the maintainence section
# INIT
cd /path/to/fancypants
git symbolic-ref HEAD refs/heads/gh-pages
rm .git/index
git clean -fdx
echo "My GitHub Page" > index.html
git add .
git commit -a -m "First pages commit"
git push origin gh-pages

# This makes a new gh-pages branch with nothing in it, then adds a file and pushes it to GitHub. Now you have two branches with differing content:

#     master — your project’s code
#     gh-pages — your project’s website, hosted by GitHub using GitHub Pages

# In Quick tip: git checkout specific files from another branch, Nicolas Gallagher covers how to add or update files on gh-pages from the master branch (this assumes you’re working in master):

# MAINTENANCE

git add .
git status 
# to see what changes are going to be committed
git commit -m "Some descriptive commit message"
git push 
# push the master branch changes to GitHub
git checkout gh-pages 
# go to the gh-pages branch
git checkout master -- OpenSoftware-RestrictedData.html 
# add/update file1-3 with changes from master branch
cp OpenSoftware-RestrictedData.html index.html
git add .
git commit -m "Update html from master"
git push 
# push the gh-pages branch changes to GitHub Pages
git checkout master 
# return to the master branch
#+end_src


** TODO add section on physical security

** TODO RCurl dependency
If you want to install RCurl, or anything which depends on it like twitteR, you’ll need to install libcurl & friends first:
sudo yum -y install libcurl libcurl-devel
http://jeffreybreen.wordpress.com/category/sysadmin/

** TODO security groups can be set on a running instance?

** TODO contact criminology virtual lab
Professor Anna Stewart A.Stewart@griffith.edu.au Griffith University Professsor 	
Crime data laboratory
Virtual Laboratories eResearch Tools 	
Empirical crime research in Australia is rare compared to overseas. The primary reason is lack of access to spatially recorded crime data. We propose to establish a research data laboratory at Griffith University that meets national standards for data security, allowing for the storage and analysis of police recorded crime data within a secure university environment. The Queensland Police Service is enthusiastic at the production of analytic products that are operationally relevant, but underpinned by academic rigor. This project will focus on adapting and developing systems, software tools, techniques and methods to transform operational data from police agencies into a range of cleansed, processed and pre-analysed data sets for research use. The data lab is modelled on two similar facilities at Simon Fraser University (SFU) in Canada and the Jill Dando Institute of Crime Science, University College London (UK). The Griffith University researchers have a strong record of collaborating with these research groups and both have agreed to provide support for the establishment of our laboratory including the sharing tools and processes for adaption to meet Australian requirements. 	

The proposed research laboratory would attract the research attention of many disciplines, including criminologists, psychologists, geospatial and urban researchers, computer scientists, economists, public health and information technology engineers. 	

160200 Criminology 	

The proposed secure datalab would have four major impacts on crime research in Australia. First, access to geospatially coded police crime data will facilitate the ongoing development of advanced crime analysis methods and tools. These tools will benefit operational policing and crime control resulting in a reduction in crime and safer communities.Second, incorporating additional geocoded databases with police crime data combined with sophisticated analytical techniques (ie place, route and temporal algorithms, spatial autocorrelation and regression, spatial interpolation and interaction and multiple-point geostatistics analyses) will provide better depictions of the criminal environment. These understandings will enhance policing practice. Third, this facility will provide opportunities for training and skill development both for students and scholars as well as operational police analysts. Access to these datasets will provide opportunities for HDR students and the professional development of crime analysts in police agencies across Australia. Finally, the data laboratory, the first of its kind in Australia and developed in conjunction with international partners, will establish data sharing protocols and agreements, data standards and metadata, data architecture and future enterprise architecture. While at this stage the proposal is limited to one jurisdiction (Queensland) researchers and police agencies in other jurisdictions will access these data and facilities. Eventually, police data from all jurisdictions in Australia could be accommodated in this facility. 

Griffith University 	
Queensland Police Service, Australian Institute of Criminology, 
ARC Centre of Excellence in Policing and Security 	
Queensland Cyber Infrastructure Foundation (QCIF), 
Griffith eResearch Unit 	yes 	  	  	  	  	 

* COMMENT R functions
** main
#+name:main
#+begin_src R :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:main
  if(!require(ProjectTemplate)) install.packages('ProjectTemplate'); require(ProjectTemplate)
  load.project()
  
  
#+end_src

** init
#+name:init
#+begin_src R :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:init
  # set load_libraries: on
  sink('config/global.dcf')
  print("data_loading: on
  cache_loading: on
  munging: on
  logging: off
  load_libraries: off
  libraries: reshape, plyr, ggplot2, stringr, lubridate
  as_factors: on
  data_tables: off")
  sink()
  
  
  for(i in c('reshape', 'plyr', 'ggplot2', 'stringr', 'lubridate')){
   cat(
     paste("if(!require(",i,")) install.packages('",i,"'); require(",i,");\n",
           sep = "")
     )
  }
  if(!require(reshape)) install.packages('reshape'); require(reshape);
  if(!require(plyr)) install.packages('plyr'); require(plyr);
  if(!require(ggplot2)) install.packages('ggplot2'); require(ggplot2);
  if(!require(stringr)) install.packages('stringr'); require(stringr);
  if(!require(lubridate)) install.packages('lubridate'); require(lubridate);
  
#+end_src
** connect2postgres
#+name:connect2postgres
#+begin_src R :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:connect2postgres
  # available at github.com/ivanhanigan
  source('~/disentangle/src/connect2postgres.r')
  ewedb <- connect2postgres(hostip =  '115.146.94.209', db = 'ewedb')
  
#+end_src

** newnode
#+begin_src R :session *R* :tangle no :exports none :eval no
  ################################################################
  # name:nodes
  # see disentangle available at github.com/ivanhanigan
  source('~/tools/disentangle/src/newnode.r')
  # test
  #nodes <- newnode(name = 'test', inputs = 'testin', newgraph = T)
  
#+end_src
** readOGR2
#+name:readOGR2
#+begin_src R :session *shell* :tangle no :exports none :eval no
  ################################################################
  # name:readOGR2
  
  readOGR2 <- function(hostip=NA,user=NA,db=NA, layer=NA, p = NA) {
   # NOTES
   # only works on Linux OS
   # returns uninformative error due to either bad connection or lack of record in geometry column table.  can check if connection problem using a test connect?
   # TODO add a prompt for each connection arg if isna
   if (!require(rgdal)) install.packages('rgdal', repos='http://cran.csiro.au'); 
   if(is.na(p)){ 
   pwd=readline('enter password (ctrl-L will clear the console after): ')
   } else {
   pwd <- p
   }
   shp <- readOGR(sprintf('PG:host=%s
                           user=%s
                           dbname=%s
                           password=%s
                           port=5432',hostip,user,db,pwd),
                           layer=layer)
  
   # clean up
   rm(pwd)
   return(shp)
   }
  
#  tassla06 <- readOGR2(hostip='115.146.94.209',user='gislibrary',db='ewedb', layer='abs_sla.tassla06')
#+end_src

* COMMENT Introduction to orgmode doc

This is a compendium \cite{Gentleman2007} using the orgmode approach
\cite{Schulte}. A compendium contain the documentation and actual application code. The advantage is that you can generate the content from various sources. In other kinds of source code documentation it is quite hard to keep the documentation up-to-date with the actual application code.  The solution used here is to generate the document from the same source file as is used to execute the code/configuration files.  

As an example the following code chunk is evaluated in-line within the document being authored, and returns the value shown below:

**** A Code Chunk
#+name:whoami
#+begin_src sh :session *shell* :tangle no :eval no
whoami
#+end_src

**** The Result
| ivan |

* COMMENT Plan
* Introduction /introduction.html

#+name:introduction
#+begin_src markdown :tangle introduction.md :exports none :eval no :padline no
--- 
name: introduction
layout: default
title: Introduction
---
#+end_src

#+begin_src markdown :tangle introduction.md :exports reports :eval no :padline no
We present an environment for analysing restricted data using open
software.  The system is described using an analysis of the historical
association of suicides with drought in Australia; and extrapolate
this under climate change and adaptation scenarios.  These tools were
assembled to allow users to access restricted data in a manner that
protects confidentiality of sensitive data, whilst also allowing use
of open software for reproducibility. 

Recently restrictions on access to confidential health records have
increased, especially for  sensitive data on suicide used in our
case study.  Previous solutions to this challenge make access 
so restricted that usability is compromised. We aimed to build a
collection of tools for the conduct of many types of health and social
science research. The starting point for users is the data catalogue,
which provides for finding data available from the store of
unrestricted and restricted data for approved use. Once data are
discovered, the researcher has capacity to manipulate the datasets on
the secure server. The PostgreSQL database integrates and Geoserver
visualises, while statistical tools are available in the R-studio
server browser.

Such analytical tools will enhance the
ability of adaptive management practitioners to assess the potential
influence of adaptations.  The use of the system shows the ease with
which multiple data sources (some restricted) can be analysed in a
secure way using open software.  This will build capacity to answer
complex research questions and compare multiple climate change
scenarios or adaptation assumptions; achieving simultaneous vision of
potential future outcomes from different standpoints.
#+end_src
** COMMENT Old intro
We aimed to build and deploy a service comprising a collection of research tools for the conduct of heath and social science research. The starting point for users is the integrated data catalogue, which provides an ideal access point for
finding and exploring spatial data available through the service.

Once data are discovered, the researcher then has the capacity to readily
access the relevant spatial data through the  database. The integration of the Postgres/PostGIS database and Geoserver web service for visualisation, along with the streamlined access to the spatial data through the Rstudio server
environment, enable the integration of geospatial data with other survey
and administrative data sources.

This integration capability allows us to easily bring together data
sources that have not previously been considered in common, due to the
level of knowledge required, covering multiple disciplines and research
methods. 

In the example presented here, we provide a simple analysis of
the association between suicide rates and the distribution of drought across NSW in 2006, derived from Bureau of Meteorology data, the vote for the Liberal Party in the same electorates in 2010, drawn from the Australian Electoral Commission
election results website. 

The integration of the system with GitHub, the DDIIndex data catalogue
and the data registry system also enable the research to be fully
documented, published and then available for reanalysis, further
demonstrating the potential of the system for supporting reproducible
research. All of the analysis presented here is available through the
project GitHub repository.

While the analysis is exploratory only, the use of the system
shows the ease with which multiple data sources (some restricted) can be brought together,
and hence to be able to answer more complex research questions, and at
increasingly specific levels of geography. 
* Deploying Virtual Machines
** Description
We want to deploy the service as a symbiotic pair of virtual images that together will provide integrated data storage and analysis services in the cloud.
Our recent review of the system prototype I developed suggested a need for two servers to comprise this system; to provide a more robust and powerful combination of structural and functional aspects.

The services hosted by the pair of servers will be:
- The Brains: a Statistical Analysis engine (running the R environment) integrated with a metadata registry, and
- The Brawn: a PostgreSQL Geographical Information System (GIS) Database server.

** Accessing Virtual Machines on the Australian  Research Cloud
*** Launch an instance
- Log on to the research cloud using your Australian University details http://www.nectar.org.au/research-cloud/.
- create two security groups for brawn and brains, this will be the special firewall settings for each
- add ssh port 22 (add http(s): 80 etc, postgres 5432, icmp -1 as needed)
- Under images and snapshots launch the centos 6.2 amd64 image
- give your server a name, add security group
- specify a ssh keypair http://support.rc.nectar.org.au/demos/launching_123.html
- launch and note the ip address
- go to the access and security section and edit the rules (ie for http add a rule to allow access from port 8080 to 9000 with 0.0.0.0/0 (CIDR))
- Note that this allows access to the whole world, we will think about securing the server later
*** requesting help
To: rc-support@nectar.org.au
Subject: Request about snapshot status

Hi,

My AAF email I login to the cloud with is: ivan.hanigan@anu.edu.au

What is currently happening:
I took a snapshot of one VM and it said success but now in the status column it has 'shutoff' and a spinning wheel.

What should happen:
It should return to status 'Active'.

Cheers,
Ivan Hanigan
Data Management Officer.
Thursday PhD scholar (ANU/CSIRO).
National Centre for Epidemiology and Population Health.
College of Medicine, Biology and Environment.
Australian National University Canberra, ACT, 0200.
Ph: +61 2 6125 7767.
Fax: +61 2 6125 0740.
Mob: 0428 265 976.
CRICOS provider #00120C.
*** connect using ssh
If using Windows make sure you have git or WinSCP installed.  Opening a bash
shell from these will enable you to connect to your server using ssh. Then the contents of the following orgmode chunks can be evaluated on the remote server.
If your on linux then orgmode can execute these chunks using C-c C-c.
#+name:whoami local
#+begin_src sh :session *shell* :exports reports :eval no :results silent
whoami
#+end_src
In the next chunk, insert the relevant ip address and you may have to answer yes to the question about adding this RSA fingerprint to your list.
# NB actually don't run this using C-c C-c as it won't work.  Also NB you don't really need the -i keypairname either for some reason... see security section below
#+name:ssh
#+begin_src sh :session *shell*  :eval no :results silent
  cd ~/.ssh
  ssh -i keypairname root@your.new.ip.address
  # it is prudent to set a hideously long password for root
  # passwd root
#+end_src
*** COMMENT hidden codes
#+begin_src sh :session *shell*  :eval no :results silent
  cd ~/.ssh
  ssh -i foobarkey root@115.146.95.xx
# passwd root
#+end_src
**** TODO find out about ssh without using -i keypairname?
I've discovered that on Centos VMs at least, once the root password is set any machine without the ssh key can access this. Restricting access to a dedicated ip address through the firewall should suffice.

** Setting up the basic server framework
This is done in a similar way for both the Brains and the Brawn servers.
*** Install any updates using the yum package manager
This is recommended to do every week to maintain the server in good condition, especially regarding security software. 
#+name:yumupdate
#+begin_src sh :session *shell* :exports reports :eval no :results silent
yum update 
#+end_src
** Security measures
The following section sets some restrictions on the server.
I would like to know how important it is to restrict root login and also if we can permit login via ssh and port 22 if it is only open to the NCEPH VPN range?  If so I just leave the below as yes yes yes?
I had to get a bit of advice from a sysadmin at work about the following but I am sure it is still pretty unsecure.  
There are various TODOs hidden in the source code document.

#+name:security
#+begin_src sh :tangle no :exports reports :eval no
################################################################
# name:security
#visit 
/etc/ssh/sshd_config
#under authentication remark out 
#RSAAuthentication yes
#PubkeyAuthentication yes
#AuthorizedKeysFile	.ssh/authorized_keys
#+end_src
*** COMMENT  setting password authentication to no?
**** TODO TODO Sounds like a bad idea, I decided not to follow the advice here:
#+name:security
#+begin_src sh :tangle no :exports reports :eval no
# scroll down to the text:
# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
PasswordAuthentication yes
# change to no?
#+end_src
*** COMMENT get a few examples from other servers
**** TODO check out best practice
#+name:security
#+begin_src sh :tangle no :exports none :eval no
# Example of one of Dave Fisher's (MHS CMBE).
Port 13456
#Protocol 2,1
Protocol 2
#AddressFamily any
#ListenAddress 0.0.0.0
#ListenAddress ::

# Authentication:

#LoginGraceTime 2m
PermitRootLogin yes
#StrictModes yes
#MaxAuthTries 6

#RSAAuthentication yes
#PubkeyAuthentication yes
AuthorizedKeysFile      .ssh/authorized_keys

# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts
#RhostsRSAAuthentication no
# similar for protocol version 2
#HostbasedAuthentication no
# Change to yes if you don't trust ~/.ssh/known_hosts for
# RhostsRSAAuthentication and HostbasedAuthentication
IgnoreUserKnownHosts no
# Don't read the user's ~/.rhosts and ~/.shosts files
#IgnoreRhosts yes

# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
PasswordAuthentication no
#+end_src
*** Restrict ssh access to a specific ip address/range
#+name:iptables-header
#+begin_src markdown :tangle iptables.md :exports none :eval no :padline no
  ---
  name: iptables
  layout: default
  title: iptables
  ---
  
  <li><a href="/iptables.html">Previous: Security measures</a></li>
  <li><a href="/setting-the-host-tcp-wrappers.html">Next: Setting the host.* TCP Wrappers</a></li>
  <p></p>
      
  In the security group we allowed access via port 22 however we will now restrict the firewall to this port to allow access only from specified ip addresses.
  
  #### Code:iptables
      # newnode: setting-ports
      vi /etc/sysconfig/iptables 
      # and modify the line
      -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
      # to 
      -A INPUT -s your.desk.ip.address -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
      # might want to add other port now if you are familiar with this,
      # otherwise see below for specific modifications.
      service iptables restart
#+end_src
*** COMMENT restrict to VPN?
**** TODO I'd like to restrict ssh access to the ncephgis VPN group, and
#add the restriction to use keypair's.
# -A INPUT -s vpn.ip.range.0/255.255.255.0 -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
# NB only works for servers on your VPN network
# service iptables restart
# instead just allow VPN access to the Office machine, and then ssh from there.
# the office machine is only accessible via ANU VPN group 'ncephgis' on port 22
*** COMMENT set a domain name
**** TODO get a domain name
I haven't attempted this yet but found the following info at the blog http://helmingstay.blogspot.com.au/2012/02/adduser-myusername-adduser-myusername.html
there is a well described process to set up an R server on the amazon EC2 cloud.
They had registered their own domain and added it to the amazon system. Then used a script file made specifically to work with AWS -- "it's very self-explanatory".  
#+name:domainname
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
###########################################################################
# newnode: domainname
## see that blog
## change hostname to match afraid.org entry
sudo vi /etc/hostname
sudo /etc/init.d/hostname restart
#+end_src
** Setting the host.* TCP Wrappers
*** Setting the host.* TCP Wrappers header
#+name:Setting the host.* TCP Wrappers-header
#+begin_src markdown :tangle setting-the-host-tcp-wrappers.md :exports none :eval no :padline no
  ---
  name: setting-the-host-tcp-wrappers
  layout: default
  title: Setting the host.* TCP Wrappers
  ---
  
    <li><a href="/iptables.html">Previous: Restrict firewall to a specific ip address/range</a></li>
    <li><a href="#sec-3-4-3">Next: TODO Security Enhanced Linux (selinux)</a></li>
    <p></p>
  
  It is advisable to use the hosts.* TCP wrappers when using SSH to add a further layer of protection. The following site has an excellent example at the bottom that shows how to deny everything that's not explicitly allowed: 
  
  [http://www.akadia.com/services/ssh_tcp_wrapper.html](http://www.akadia.com/services/ssh_tcp_wrapper.html)
  
  #### Code:Setting the host.* TCP Wrappers
      vi /etc/hosts.deny
      # hosts.deny 
      # 
      # This file describes the names of the hosts which are
      # not allowed to use the local INET services, as decided
      # by the '/usr/sbin/tcpd' server.
      ALL: ALL EXCEPT 127.0.0.1
  
  Then, explicitly list in the hosts.allow file all hosts/domains you want access to your machine. A recommended hosts.allow looks like:
  
  
  #### Code
      vi /etc/hosts.allow
      # hosts.allow 
      #
      # This file describes the names of the hosts which are
      # allowed to use the local INET services, as decided
      # by the '/usr/sbin/tcpd' server.
      sshd: my.desk.ip.address, another.trusted.ip.address
  
   
#+end_src

*** Security Enhanced Linux (selinux)
To run the Database and the Rstudio server it is best to disable the selinux.  
**** TODO Find out if this is necessary for PostgreSQL as well as Rstudio.
#+name:seconfig
#+begin_src sh :session *shell* :tangle no  :eval no
# selinux config
vi /etc/selinux/config
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# Change SELINUX=enforcing to disabled
#+end_src

*** COMMENT Some other things, these might be deprecated
**** TODO deprecated?
# and you must reboot the server after applying the change.
# also the following 
# chkconfig httpd on
# so  when log back in must restart httpd

To check what is going on with services use this
#+name:check
#+begin_src sh :session *shell* :tangle no  :eval no
# check what's on
chkconfig --list | grep on
acpid           0:off   1:off   2:on    3:on    4:on    5:on    6:off
auditd          0:off   1:off   2:on    3:on    4:on    5:on    6:off
cgconfig        0:off   1:off   2:off   3:off   4:off   5:off   6:off
cups            0:off   1:off   2:on    3:on    4:on    5:on    6:off
fcoe            0:off   1:off   2:on    3:on    4:on    5:on    6:off
httpd           0:off   1:off   2:on    3:on    4:on    5:on    6:off
ip6tables       0:off   1:off   2:on    3:on    4:on    5:on    6:off
iptables        0:off   1:off   2:on    3:on    4:on    5:on    6:off
iscsi           0:off   1:off   2:off   3:on    4:on    5:on    6:off
iscsid          0:off   1:off   2:off   3:on    4:on    5:on    6:off
lldpad          0:off   1:off   2:on    3:on    4:on    5:on    6:off
lvm2-monitor    0:off   1:on    2:on    3:on    4:on    5:on    6:off
messagebus      0:off   1:off   2:on    3:on    4:on    5:on    6:off
netconsole      0:off   1:off   2:off   3:off   4:off   5:off   6:off
netfs           0:off   1:off   2:off   3:on    4:on    5:on    6:off
network         0:off   1:off   2:on    3:on    4:on    5:on    6:off
nfslock         0:off   1:off   2:off   3:on    4:on    5:on    6:off
portreserve     0:off   1:off   2:on    3:on    4:on    5:on    6:off
restorecond     0:off   1:off   2:off   3:off   4:off   5:off   6:off
rpcbind         0:off   1:off   2:on    3:on    4:on    5:on    6:off
rpcgssd         0:off   1:off   2:off   3:on    4:on    5:on    6:off
rpcidmapd       0:off   1:off   2:off   3:on    4:on    5:on    6:off
rstudio-server  0:off   1:off   2:on    3:on    4:on    5:on    6:off
rsyslog         0:off   1:off   2:on    3:on    4:on    5:on    6:off
sshd            0:off   1:off   2:on    3:on    4:on    5:on    6:off
udev-post       0:off   1:on    2:on    3:on    4:on    5:on    6:off
# then 
chkconfig `servicename' on
# or
chkconfig `servicename' off
#+end_src
*** Install some base packages
There are a few commonly used packages recommended for both Brawn and Brains.
# kudos 2 http://rlamp.blogspot.com.au/2010/03/getting-started-setting-up-rapache.html
#+name:foundations
#+begin_src sh :session *shell* :results silent :reports none :eval no 
yum install gcc-gfortran gcc-c++ readline-devel libpng-devel libX11-devel libXt-devel texinfo-tex.x86_64 tetex-dvips docbook-utils-pdf cairo-devel java-1.6.0-openjdk-devel libxml2-devel make unzip hdparm
#+end_src
** Hardware set-up
Now we will note the size and number of the disc partitions.
#+name:partitions
#+begin_src sh :session *shell* :exports reports :results silent  :eval no
df -h
#+end_src
*** Swap space /swapon.html
#+begin_src markdown :tangle swapon.md :exports reports :eval no :padline no
--- 
name: swapon
layout: default
title: Swap space
---

For some reason the Research Cloud Centos VMs do not have any swap space.
I added one GB swapfile, but was advised to enable about the same amount as we have RAM.
I will come back and review this, also I was too lazy to add to boot so just do swapon every time?
#+end_src

#+name:swap
#+begin_src sh :session *shell* :tangle swapon.md :exports reports :eval no
#### Code
    free -m | grep Swap
#+end_src

#+begin_src markdown :tangle swapon.md :exports reports :eval no
Add swap file with this:
#+end_src

#+begin_src sh :session *shell* :tangle swapon.md :exports reports :eval no
  #### Code
      # Create an empty file called /swapfile (here over 2 GB is required for oracle but for just one GB it is count = 1024, need to come back and review)
      dd if=/dev/zero of=/swapfile bs=1024000 count=3000
      #Format the new file to make it a swap file
      mkswap /swapfile
      #Enable the new swapfile. 
      swapon /swapfile
      free -m | grep Swap
#+end_src

*** Persistent Net Rules Should Be Avoided On Centos
The current "CentOS 6.2 amd64" image (ID 21401), if launched and
snapshotted, is not connectible when launched from this snapshot due
to networking.  (davidjb, Mon Mar 26, 2012 support.rc.nectar.org.au/forum).

#+name:netrulses
#+begin_src R :session *R* :tangle no :exports reports :eval no
################################################################
# name:netrulses
# Just remove the /lib/udev/write_net_rules file (and
# /etc/udev/rules.d/70-persistent-net.rules for good measure), and then
# any further instances will always have their networking adapter as
# eth0. There's probably a better way to do this, but that's working for
# me now
rm /lib/udev/write_net_rules
rm /etc/udev/rules.d/70-persistent-net.rules
#+end_src
**** COMMENT source
http://support.rc.nectar.org.au/forum/viewtopic.php?f=6&t=191
CentOS 6.2 image snapshot networking issue

Postby davidjb » Mon Mar 26, 2012 11:06 am Just an FYI in case anyone
hits the same issue as I did: the current "CentOS 6.2 amd64" image (ID
21401), if launched and snapshotted, is not connectible when launched
from this snapshot due to networking.

Effectively, as best I could tell, the issue boiled down to the
'persistent net' rule generator for udev. The script at
/lib/udev/write_net_rules creates/adds to the
/etc/udev/rules.d/70-persistent-net.rules file, and if the image is
snapshotted, this file persists. When a new instance is launched from
said snapshot, udev appends the 'new' networking interface and thus
the new interface becomes eth1 rather than eth0. No configuration is
present for eth1 and thus networking connectivity fails.

What I've done in my running instance is to just remove the
/lib/udev/write_net_rules file (and
/etc/udev/rules.d/70-persistent-net.rules for good measure), and then
any further instances will always have their networking adapter as
eth0. There's probably a better way to do this, but that's working for
me now.

davidjb
     
    Posts: 5
    Joined: Mon Mar 26, 2012 9:39 am
    Location: JCU Townsville

Top
Re: CentOS 6.2 image snapshot networking issue

Postby support » Mon Mar 26, 2012 2:30 pm
Thanks for your really useful feedback.

Most guides to building VM Images should mention that persistent net rules should be avoided when creating the Image.
Clint Walsh
Research Cloud Support

*** Disk Storage
Every VM on the Research Cloud has a 10GB primary disk which is used for the image you launch. The Primary disk is copied in a snapshot, so anything on this primary disk can be backed up via snapshots.  In addition every Virtual Machine will get secondary storage. This secondary disk is not copied or backed up via snapshots.  If you reboot your virtual machine, the secondary disk data remains in tact. If you shut down your VM, the data disappears (it is not persistent).
#+name:format
#+begin_src sh :session *shell* :tangle no :exports none :results silent :eval no
######################################################################
# newnode: format
mke2fs -j /dev/vdb
# mke2fs -j  creates ext2 with a journal - which is ext3 effectively.
#+end_src

#+begin_src sh :session *shell* :tangle no :exports reports :results silent :eval no
######################################################################
# Mount the drive to /home eg.
mount -t ext3 /dev/vdb /home
# df -h
# once successful edit /etc/fstab to mount the drive at boot e.g.
# /dev/vdb    /home    ext3     defaults    0 0
# Reboot the server to ensure that the drive mounts on boot.
# check performance
hdparm -tT /dev/vdb
#+end_src


**** TODO find out recommended practice for backups, and purpose of 'object storage'
- http://support.rc.nectar.org.au/technical_guides/object_storage.html
- go to settings (topright), EC2 credentials, select project  and download zip
- unzip and open ec2rc.sh in text editor
- sudo apt-get install python-boto
- list existing buckets
import boto.s3.connection

connection = boto.s3.connection.S3Connection(
aws_access_key_id='7b7a0a6f71994e42a07c8e9b0de0f8ca',
aws_secret_access_key='2d54f6a679cd4b06820550459451cb50',
port=8888,
host='swift.rc.nectar.org.au',
is_secure=True,
calling_format=boto.s3.connection.OrdinaryCallingFormat())

buckets = connection.get_all_buckets()
print buckets
- downside is you have to use python to work with the data?
**** COMMENT references regarding storage
***** main page: instance storage
http://support.rc.nectar.org.au/technical_guides/instance_storage.html
On-Instance Storage
Primary Disk (10Gb)

    Every Instance has a 10GB primary disk which is used for the image you launch. The Primary disk is copied in a snapshot, so anything on this primary disk can be backed up via snapshots 

On Instance Storage: (Secondary or Ephemeral Disk, 30GB to 480GB)

    For every CPU a Virtual Machine will get 30GB of additional secondary storage.
    This secondary disk is not copied or backed up via snapshots.
    Some Operating Systems will automatically format and mount the secondary disk

        (eg Ubuntu creates an ext3 partition and mounts it at /mnt) 

    Each Virtual Machine Instance comes with a certain amount of on-instance storage. This appears as a second hard disk in your VM, that you can format and use as you wish. Here's what they actually look like inside a running VM:

#+name:fdisk
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
######################################################################
# newnode: fdisk
fdisk -l
#+end_src

/dev/vda is the mounted root device (/), with a limit of 10GB.
/dev/vdb is the on-instance storage, which is of the size listed above. You can format it directly, eg
#+name:format
#+begin_src sh :session *shell* :tangle no :exports none :eval no
######################################################################
# newnode: format
# mke2fs /dev/vdb
# or better to do  
mke2fs -j /dev/vdb
# because 
# mke2fs -j /dev/hda1 creates ext2 with a journal - which is ext3 effectively.
#+end_src

# http://www.linuxquestions.org/questions/linux-from-scratch-13/how-to-create-ext3-filesystem-using-mke2fs-j-430284/

One task you might like to do on startup is format and mount /dev/vdb 
#+name:daveFisherEg
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
######################################################################
# newnode: mountStorage
# Mount the drive to /home eg.
mount -t ext3 /dev/vdb /home
#df -h
# once successful edit /etc/fstab to mount the drive at boot e.g.
#/dev/vdb    /home    ext3     defaults    0 0
#Reboot the server to ensure that the drive mounts on boot.
#+end_src

#130.56.102.xx:/mnt/Rserver/ /home 
#130.56.102.xx:/mnt/Rserver /home nfs defaults 0 0
***** wiki page
http://support.rc.nectar.org.au/wiki/ResearchCloudStorage
****** On-instance Disk

This is 10GB in size, which is the same for every size of VM (Small, Medium, Extra Large and Extra Extra Large). If you reboot your virtual machine, the data remains in tact. When you take a point-in-time snapshot of your VM, what you're saving is a copy of this 10GB disk (the copy is actually made on the Object Store). If you shut down your virtul machine without taking a snapshot of it first, the data will be lost.

****** On-instance Block Storage (Secondary or Ephemeral Drives)

This storage volume behaves like a raw, unformatted block device which you have to mount from within your VM yourself. It varies in size according to the size of VM you're running (from 30GB to 480GB in size). If you reboot your virtual machine, the data remains in tact. Taking a 'snapshot' of your virtual machine does not also keep a copy of this extra volume. And if you shut down your VM, the data disappears (it is not persistent).

****** Object store

The Research Cloud Object Storage is not a traditional file system, but rather a distributed storage system for static data such as virtual machine images, photo storage, email storage, backups and archives. Having no central "brain" or master point of control provides greater scalability, redundancy and durability. When you put a file in the Research Cloud Object Store, 3 copies of your data are distributed to different hardware for extra data safety and performance.

In general, the object store is great for data you write once and read many times, but not suitable for applications like databases. It's the safest place to put your data on the NeCTAR Research Cloud as multiple redundant copies of your data are made, and it has great performance. You can access the object store from anywhere on the internet, and data from Object Storage can be transferred to and from your VM with a variety of http-capable tools.

Object Storage is completely decoupled from your VMs, so even if you reboot, delete or crash your VMs, your Object Storage files will remain safe (unless you remove them yourself). Object Storage persists independently of the life of an instance.

More details on how to use the Object Store are available here. 
***** TODO object storage
http://support.rc.nectar.org.au/technical_guides/object_storage.html
Security Warning: Swift does NOT provide encryption of the data it stores.
If you have sensitive data that requires encryption you must encrypt the data files before upload. 
Object Storage is not a traditional file-system or real-time data storage system. It's designed for mostly static data that can be retrieved, leveraged, and then updated if necessary. It is independant of a particualr Virtual Machine and can be updated and used without having any Virtual Machine running. It is designed to be redundant and scalable.

*** COMMENT Add administrative users 
NB adding general users is covered in the context of managing both users and data in a restricted manner in a section further down.

Primarily you will want to do this from using a 'secure password generator' (NB I have an R function to generate passwords from a word list and random letters, numbers and symbols.  I don't want to share it here for fear that it will create a security flaw.)
#+name:add users
#+begin_src sh :session *shell* :exports reports :eval no :results silent
adduser newuser1
passwd newuser1
adduser newuser2
passwd newuser2
#+end_src

**** TODO disable root login
DON'T DO THIS YET.  THIS IS NOT WORKING (LOCKED MYSELF OUT).
If you can set this up do it, as some sysadmins recommend restricting root login.
#+name:disableRoot
#+begin_src R :session *shell* :tangle no :exports reports :eval no
###########################################################################
# newnode: disableRoot
vi /etc/ssh/sshd_config 
# disable root login
/etc/init.d/ssh restart
# now log in as myusername via another terminal to make sure it works, and then log out as root
#+end_src

Questions to Dave.
Q1 Should I disable root log in?
Yes, because of 
- Emacs over X Windows and SSH, 
- R console access via SSH etc. 
 On most of my systems only my ip address have 22 access though the firewall so in those circumstances that is fine. 

Q2 from your previous build I saw you modified your user with vi /etc/passwd # and change david:x:500:500::/home/david:/bin/bash to ::/home/david:/sbin/nologin #is this necessary for all users?

Depends on yourself.  Guessing that RStudio will be visible to the outside world without the need to VPN in?  If so and a external user who only needs to access to RStudio you would then change their login in /etc/passwd to provided example.  Users who will be accessing the server with EMacs, plain R Console vi SSH etc, you would not change.

**** TODO sudo rights
NB this isn't necessary unless you disable root
add sudoers
kudos2 http://helmingstay.blogspot.com.au/2012/02/adduser-myusername-adduser-myusername.html
#+name:add sudoer
#+begin_src sh :session *shell* :exports reports :eval no :results silent
adduser super_user
passwd super_user
#+end_src

#+name:sudoer
#+begin_src sh :session *shell* :exports reports :eval no :results silent
# adduser super_user sudoers
## add correct key to ~myusername/.ssh/authorized_keys
# NB this didnt work , might be ubuntu only?
# ASKED DAVE FOR ADVICE
yum list sudo*
# says installed packages sudo.x86_64
# Create the local user accounts e.g.
useradd super_user
# useradd super_user2
# and them to a particular group e.g. nceph_admins
groupadd nceph_admins
usermod -G nceph_admins super_user
# usermod -G nceph_admins super_user2

# don't forget assign passwords

passwd super_user
#passwd super_user2

#then enter

sudoedit /etc/sudoers

root ALL=(ALL) ALL
%nceph_admins ALL=ALL

# or 
# To avoid creating and adding users to a group
#root ALL=(ALL) ALL
#super_user ALL=ALL
#super_user2 ALL=ALL

#+end_src

* The Brawn
The Brawn is a PostgreSQL Geographical Information System (GIS) Database server.
** PostgreSQL /postgresql.html
#+begin_src markdown :tangle postgresql.md :exports none :eval no :padline no
--- 
name: postgresql
layout: default
title: PostgreSQL
---
<li><a href="/postgresql.html">Previous: Brawn</a></li>
<li><a href="/postgis.html">Next: PostGIS</a></li>

#+end_src

#+begin_src markdown :tangle postgresql.md :exports reports :eval no :padline no
## Install PostgreSQL 9.2 
PostgreSQL is an Open Source database that can be extended with GIS functionality using the PostGIS tools.  The latest version is 9.2.  Please see [this link](http://people.planetpostgresql.org/devrim/index.php?/archives/70-How-to-install-PostgreSQL-9.2-on-RHELCentOSScientific-Linux-5-and-6.html)  for the orginial documentation I used to install this on Centos or Redhat 6.4.  Check the correct download from [this link](http://yum.postgresql.org/repopackages.php#pg92). Please note that this didn't work on Ubuntu 12.04 LTS for me in early 2013 because PostGIS 2.0 was not included in their repositories.  I ended up rolling back to PostgreSQL 9.1 on that machine.

#+end_src

#+begin_src markdown :tangle postgresql.md :exports code :eval no :padline no
  #### Code
      # install the PostgreSQL 9.2 repo package
      rpm -ivh http://yum.postgresql.org/9.2/redhat/rhel-6-x86_64/pgdg-centos92-9.2-6.noarch.rpm
      # install PostgreSQL 9.2 with single command:
      yum groupinstall "PostgreSQL Database Server PGDG"
      # This will install PostgreSQL 9.2 server, along with -contrib subpackage.
      # Once it is installed, first initialize the cluster:
      service postgresql-9.2 initdb
      # Now, you can start PostgreSQL 9.2:
      service postgresql-9.2 start
      # If you want PostgreSQL 9.2 to start everytime on boot, run this:
      chkconfig postgresql-9.2 on
#+end_src


*** COMMENT for 9.1
**** TODO DEPRECATED  upgrade from 8.4 using opengeo
not installing tomcat6-admin-webapps
"You have to subscribe to the "RHEL Server Optional" channel in order to get this package available through yum." commented by anonymous at
http://dougbunger.blogspot.com.au/2010/05/redhat-tomcat-6-with-web-manager.html
I had an old redhat server running 8.4 with crappy data
decided to just remove it
sudo su
service postgresql stop
vi /etc/init.d/postgresql 
# check location of PG_DATA
cd /var/lib/pgsql/data
ls
# ok just erase
cd /
df -h 
# note used 6 GB
yum erase postgresql*
#yum remove gdal* not there?
yum remove proj*
yum remove geos*
#yum remove tomcat* not there? but I know tomcat7 is running
service tomcat stop



package-cleanup --cleandupes

rm -r -f /var/lib/pgsql/data

cd /etc/yum.repos.d
#CentOS 6, 64 bit
wget http://yum.opengeo.org/suite/v3/centos/6/x86_64/OpenGeo.repo
yum install opengeo-suite
**** TODO upgrade 9.1 to 9.2 Ubuntu
The upgrade process I'm following is:

 sudo add-apt-repository ppa:pitti/postgresql
 sudo apt-get update
 sudo apt-get install postgresql-9.2
 sudo pg_dropcluster --stop 9.2 main
 sudo pg_upgradecluster 9.1 main /var/lib/postgresql/9.2
# Success. Please check that the upgraded cluster works. If it does,
# you can remove the old cluster with
#   pg_dropcluster 9.1 main
#http://stackoverflow.com/questions/12944830/error-upgrading-postgresql-cluster-from-9-1-to-9-2

**** TODO delete the deprecated postgres 9.1 junk
**** install pg9.1
kudos2 http://wiki.postgresql.org/wiki/YUM_Installation
#+name:postgres
#+begin_src R :session *shell* :tangle no :exports reports :eval no
######################################################################
# newnode: postgres
vi /etc/yum.repos.d/CentOS-Base.repo
append: exclude=postgresql* to [base] and [updates] sections
# On Red Hat: /etc/yum/pluginconf.d/rhnplugin.conf [main] section 
# find rpm at http://yum.postgresql.org/
curl -O http://yum.postgresql.org/9.1/redhat/rhel-6-x86_64/pgdg-centos91-9.1-4.noarch.rpm

# curl -O http://yum.postgresql.org/9.1/redhat/rhel-6-x86_64/pgdg-redhat91-9.1-5.noarch.rpm


rpm -ivh pgdg-centos91-9.1-4.noarch.rpm

# rpm -ivh pgdg-redhat91-9.1-5.noarch.rpm
#kudos2
#http://www.davidghedini.com/pg/entry/install_postgresql_9_on_centos

# yum list postgres*
# install a basic PostgreSQL 9.1 server:
yum install postgresql91-server postgresql91 postgresql91-devel postgresql91-libs postgresql91-contrib
#+end_src

**** init postgres9.1
#+name:initialise postgres
#+begin_src sh :session *shell* :exports reports :eval no :results silent
service postgresql-9.1 initdb
service postgresql-9.1 start

# NB skipped 4 Placing the binary directory in the path for postgres will allow you to invoke pg_ctl and other commands from the shell.
#Set postgres Password
su - postgres
psql postgres postgres
alter user postgres with password 'password';
CREATE ROLE gislibrary LOGIN PASSWORD 'gislibrary';
#+end_src
**** deprecated?
# failed to start.
#GIVING UP 
#yum erase postgresql91*
# show listen_addresses;
# show port;

*** Configure PostgreSQL connection settings
#+begin_src markdown :tangle postgresql.md :exports reports :eval no
  #### Code
      #edit your pg_hba.conf file under /var/lib/pgsql/9.2/data
      #I added a super user from my ip address and allowed all the local ip addresses access
      host    all             postgres        my.desk.ip.address/32       md5
      # if you want to allow data sharing on a specific database then create a public user
      # host    dbname        publicdata      0.0.0.0/0                   md5
      # if you want people to access from a subnet at your work
      # host    dbname        username        ip.address.range.0/24        md5
#+end_src

#+begin_src markdown :tangle postgresql.md :exports reports :eval no
  #### Code
      #connect to psql
      #Set postgres Password
      su - postgres
      psql postgres postgres
      alter user postgres with password 'password';
      select pg_reload_conf();
      # close the psql using \q
      # change back to root
      exit
#+end_src

#+begin_src markdown :tangle postgresql.md :exports reports :eval no
Now make the server listen for any connection requests from anywhere in the world.
#+end_src

#+begin_src markdown :tangle postgresql.md :exports reports :eval no
  #### Code
      # First locate the postgresql.conf file under /var/lib/pgsql/9.2/data.
      # uncomment and change from localhost
      # listen_addresses = '*'
      # then restart the server
      sudo service postgresql-9.2 restart
      #then reboot and confirm postgres started
#+end_src
*** Allow connection to postgres through the firewall

#+begin_src sh :session *shell* :tangle postgresql.md :exports reports :eval no
#### Code: setting-ports
    vi /etc/sysconfig/iptables 
    # and add the line
    -A INPUT -m state --state NEW -m tcp -p tcp --dport 5432 -j ACCEPT
    service iptables restart
#+end_src

** PostGIS 2.0 /postgis.html
*** Postgis
#+begin_src markdown :tangle postgis.md :exports reports :eval no :padline no
--- 
name: postgis
layout: default
title: PostGIS
---

<li><a href="/postgresql.html">Previous: PostgreSQL</a></li>
<li><a href="/postgres-migrate.html">Next: PostgreSQL Migrate</a></li>


## Install PostGIS 2.0
The PostGIS suite enables a PostgreSQL database with spatial data types and analysis functions.

#+end_src

#+begin_src markdown :tangle postgis.md :exports reports :eval no
#### References   
 [http://www.davidghedini.com/pg/entry/postgis_2_0_on_centos]([http://www.davidghedini.com/pg/entry/postgis_2_0_on_centos])
    [http://people.planetpostgresql.org/devrim/index.php?/archives/64-PostGIS-2.0.0,-RPMs-and-so..html](http://people.planetpostgresql.org/devrim/index.php?/archives/64-PostGIS-2.0.0,-RPMs-and-so..html)
    [http://people.planetpostgresql.org/devrim/index.php?/archives/65-Installing-PostGIS-2.0.X-on-RHELCentOSScientific-Linux-5-and-6-Fedora-That-is-easy!.html](http://people.planetpostgresql.org/devrim/index.php?/archives/65-Installing-PostGIS-2.0.X-on-RHELCentOSScientific-Linux-5-and-6-Fedora-That-is-easy!.html)
#+end_src

#+name:install-postgis2
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
#### code: install-postgis2
    yum list postgis*
    yum install postgis2_92.x86_64 
    yum install postgis2_92-devel.x86_64
  
  
#+end_src


*** GDAL, PROJ and GEOS
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no :results silent
  #### Code
      sudo rpm -Uvh http://elgis.argeo.org/repos/6/elgis/x86_64/elgis-release-6-6_0.noarch.rpm
      sudo rpm -Uvh http://mirror.as24220.net/pub/epel/6/i386/epel-release-6-7.noarch.rpm
      # yum list gdal*
      yum install gdal-devel.x86_64 
      yum install proj-devel.x86_64
      yum install proj-nad.x86_64
      yum install geos-devel.x86_64
      #to update
      yum remove gdal*
      yum install gdal-devel.x86_64 
#+end_src

*** Create Database
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
  #### Code: Create Database
      su - postgres 
      createdb mydb
      psql -d mydb -U postgres  
      CREATE EXTENSION postgis;  
      CREATE EXTENSION postgis_topology;  
   
#+end_src


*** Create a GIS user and a group
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
  #### Code: Create a GIS user and a group
      CREATE ROLE public_group;
      CREATE ROLE ivan_hanigan LOGIN PASSWORD 'password';
      GRANT public_group TO ivan_hanigan;
  
      grant usage on schema public to public_group;
      GRANT select ON ALL TABLES IN SCHEMA public TO public_group;
      grant execute on all functions in schema public to public_group;
      grant select on all sequences in schema public to public_group;
      grant select on table geometry_columns to public_group;
      grant select on table spatial_ref_sys to public_group;
      grant select on table geography_columns to public_group;
      grant select on table raster_columns to public_group;
      grant select on table raster_overviews to public_group;
      \q
      exit
#+end_src

*** Specific transformations grid for Australian projections AGD66 to GDA94
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
#### Additional Reprojection File
A special transformations grid file is required to be added to the PROJ.4 files for reprojecting the Australian projections AGD66 to GDA94.

Thanks to [Joe Guillaume](https://github.com/josephguillaume) and [Francis Markham](http://stackoverflow.com/users/103225/fmark) for providing this solution.
#+end_src

#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
  #### Code: transformations grid for Australian projections
      cd /usr/share/proj
      # the original was moved
      # wget http://www.icsm.gov.au/icsm/gda/gdatm/national66.zip
      wget http://www.icsm.gov.au/gda/gdatm/national66.zip
      # if it moves again a version of it is included with this repo
      # from your local scp to your server
       
      yum install unzip
      unzip national66.zip
      mv "A66 National (13.09.01).gsb" aust_national_agd66_13.09.01.gsb
  
      su - postgres 
      psql -d mydb
  
      UPDATE spatial_ref_sys SET
      proj4text='+proj=longlat +ellps=aust_SA +nadgrids=aust_national_agd66_13.09.01.gsb +wktext'
      where srid=4202;
      \q
      exit
#+end_src

*** Test transform
#+begin_src sh :session *shell* :tangle postgis.md :exports reports :eval no
  #### Code: test transformation from AGD66 to GDA94
      select geocode, geoname, st_transform(geom, 4283) as the_geom
      into schema.gda94_table
      from  schema.agd66_table;
#+end_src

#+begin_src R :session *R* :tangle no :exports reports :eval no  
  #### FROM YOUR R SERVER OR LOCAL ####
  #R
  require(devtools)
  install_github("swishdbtools", "swish-climate-impact-assessment")
  require(swishdbtools)
  pwd <- getPassword(remote=F)
  ch <- connect2postgres2("django")
  #("ip.address", "dbname", "postgres", p = pwd)
  ## dbSendQuery(su,
  ## "UPDATE spatial_ref_sys SET
  ## proj4text='+proj=longlat +ellps=aust_SA +nadgrids=aust_national_agd66_13.09.01.gsb +wktext'
  ## where srid=4202;
  ## ")
  # now you can go ahead and convert AGD66 (4202) to GDA94 (4283)
  sql <- sql_subset_into(ch, "schema.agd66_table", select =
             'geocode, geoname, st_transform(geom, 4283) as the_geom',
             into_schema = "schema", into_table = "gda94_table", eval = F, check = F)
  cat(sql)
  
#+end_src
*** COMMENT DEPRECATED Specific transformations grid for Australian projections AGD66 to GDA94
#+name:proper transforms
#+begin_src sh :session *shell* :exports reports :eval no :results silent
cd /usr/share/proj
wget  http://www.icsm.gov.au/icsm/gda/gdatm/national66.zip
yum install unzip
unzip national66.zip
mv "A66 National (13.09.01).gsb" aust_national_agd66_13.09.01.gsb
#+end_src

*** COMMENT 2.0 was not working 
**** TODO remove redundant old crap re postgis not working
http://www.davidghedini.com/pg/entry/postgis_2_0_on_centos

#+name:postgis2
#+begin_src R :session *shell* :tangle no :exports reports :eval no
################################################################
# name:postgis2
# yum list postgis*  
yum install postgis2_91.x86_64 postgis2_91-devel.x86_64
#+end_src
failed due to depends?
# #not working proj-devel?
# yum erase proj XXX
# wget http://elgis.argeo.org/repos/6/elgis/x86_64/gdal-devel-1.8.1-1.el6.x86_64.rpm
# yum install gdal-devel-1.8.1-1.el6.x86_64.rpm
# # fail?
# wget http://elgis.argeo.org/repos/6/elgis/x86_64/proj-devel-4.7.0-2.el6.x86_64.rpm
# yum install proj-devel-4.7.0-2.el6.x86_64.rpm

# # and now
# yum install gdal-devel.x86_64 
# # success!
# # but now postgis not working.  reinstall and it works

**** TODO remove PostGIS 1.5
Requires postgres, gdal, geos http://postgis.refractions.net/docs/postgis_installation.html

http://www.davidghedini.com/pg/entry/install_postgresql_9_on_centos
#+begin_src R :session *shell* :tangle no :exports reports :eval no
################################################################
# name:postgis1.5
# yum list postgis*  
yum install postgis91.x86_64 postgis91-utils.x86_64
#+end_src
**** TODO remove Create Database
#+begin_src R :session *shell* :tangle no :exports reports :eval no
su - postgres 
createdb ewedb
psql -d ewedb -f /usr/pgsql-9.1/share/contrib/postgis-1.5/postgis.sql 
psql -d ewedb -f /usr/pgsql-9.1/share/contrib/postgis-1.5/spatial_ref_sys.sql
psql ewedb postgres
grant usage on schema public to gislibrary;
GRANT select ON ALL TABLES IN SCHEMA public TO gislibrary;
grant execute on all functions in schema public to gislibrary;
grant select on all sequences in schema public to gislibrary;
grant all on table geometry_columns to gislibrary;
#+end_src
** PostgreSQL Migration /postgres-migrate.html
*** Migrate the data
#+begin_src markdown :tangle postgres-migrate.md :exports reports :eval no :padline no
---
name: postgres-migrate
layout: default
title: PostgreSQL migration from default location
---

<li><a href="/postgis.html">Previous: PostGIS</a></li>
<li><a href="/sharedmemory.html">Next: Important shared memory settings</a></li>

To take advantage of the extra storage on the secondary disk we mounted in the initial configuration then do the following.
#+end_src

#+begin_src sh :session *shell* :tangle postgres-migrate.md :exports reports :eval no
  #### Code: Migrate PostgreSQL Data
      service postgresql-9.2 stop
      #     Copy the pgsql directory from /var/lib (or customer install directory) location to another location
      cp -r /var/lib/pgsql /home/pgsql
      chown -R postgres:postgres /home/pgsql

      #     Edit the start script 'postgresql'
      vi /etc/init.d/postgresql-9.2
      #     Search for parameter PGDATA which would be entered as "PGDATA=/var/lib/pgsql"
      #     Edit the line such that PGDATA points to the new location. For e.g. "PGDATA=/newloc/pgsql"
      #     ALSO DO PGLOG, and PGUPLOG
      #     Save and exit the file
      #     Start PostgreSQL Service 
      service postgresql-9.2 start
      # tidy up but not too much, just data?
      rm -r -f /var/lib/pgsql/9.2/data?
  
#+end_src

*** Set up backups
From here to a secure location at my work.
**** COMMENT  REFERENCES
# http://docs.fedoraproject.org/en-US/Fedora/13/html/Managing_Confined_Services/sect-Managing_Confined_Services-PostgreSQL-Configuration_Examples.html
# If you want to move the postgres database after installation then this should work.
# How do I move the PostgreSQL database from one location to another on Linux?
# http://www-01.ibm.com/support/docview.wss?uid=swg21324272

# Question

# How do I move the PostgreSQL database from one location to another on Linux?
# Answer

# The PostgreSQL data is contained in one folder named 'data' that is located under pgsql. The database service is started pointing to this data folder. To move the database you can move the data folder or the complete pgsql directory and change the path in the service start script.

#+name:movePostgres
#+begin_src sh :session *shell* :tangle no :exports no :eval no
###########################################################################
# newnode: movePostgres

# Perform the following steps to move database from one location to another:

#     Stop Apache Tomcat and PostgreSQL Services
#     Copy the pgsql directory from /var/lib (or customer install directory) location to another location
vi /etc/init.d/postgresql
#     Search for parameter PGDATA which would be entered as "PGDATA=/var/lib/pgsql"
#     Edit the line such that PGDATA points to the new location. For e.g. "PGDATA=/newloc/pgsql"
#     Save and exit the file
#     Start PostgreSQL Service followed by Tomcat Service

#     If the location /etc/init.d/ does not contain any start script then once the database is moved to the new location switch as postgres user.
#     Navigate to one level above the bin folder of the new location for pgsql folder
#     Execute "$ bin/pg_ctl -D ./data -l data/logfile start" command to start PostgreSQL Service
#     Then Start Tomcat Service 
#+end_src

** Important shared memory settings /sharedmemory.html

#+begin_src markdown :tangle sharedmemory.md :exports reports :eval no :padline no
--- 
name: sharedmemory
layout: default
title: Important Shared Memory Settings
---

## Managing memory settings
The default settings in PostgreSQL are usually pretty good but these memory settings are conservative to start with and often need modifications.

## References

[Kernal memory limitations](http://michael.otacoo.com/postgresql-2/take-care-of-kernel-memory-limitation-for-postgresql-shared-buffers/)

#+end_src

#+begin_src sh :session *shell* :tangle sharedmemory.md :exports reports :eval no
  #### Code: sharedmemory
      vi /home/pgsql/9.2/data/postgresql.conf 
      # shared_buffers
      
      # PostgreSQL has a default shared_buffers value at 32MB, what is
      # enough for small configurations but it is said that this
      # parameter should be set at 25% of the system’s RAM. This allows
      # your system to keep a good performance in parallel with the
      # database server.  So in the case of a machine with 4GB of RAM,
      # you should set shared_buffers at 1GB.
      2GB = 2048MB
       
      #also look at max_locks_per_transaction.  tried setting to 1000???
       
      ################################################################
       
      # http://www.postgresql.org/docs/9.2/static/kernel-resources.html
      # Linux
       
      #     The default maximum segment size is 32 MB, which is only
      #     adequate for very small PostgreSQL installations. The
      #     default maximum total size is 2097152 pages. A page is
      #     almost always 4096 bytes except in unusual kernel
      #     configurations with "huge pages" (use getconf PAGE_SIZE to
      #     verify). That makes a default limit of 8 GB, which is often
      #     enough, but not always.
       
      #     The shared memory size settings can be changed via the
      #     sysctl interface. For example, to allow 16 GB:
       
      sysctl -w kernel.shmmax=17179869184
      sysctl -w kernel.shmall=4194304
       
      #     In addition these settings can be preserved between reboots
      #     in the file /etc/sysctl.conf. Doing that is highly
      #     recommended.
       
       
      #    The remaining defaults are quite generously sized, and
      # usually do not require changes.  also
      # http://www.linux.com/learn/tutorials/394523-configuring-postgresql-for-pretty-good-performance
      # work mem 4MB
#+end_src

** Test loading some shapefiles
on your ubuntu desktop install postgis and gdal (see above)
#+begin_src R :session *shell* :tangle no :exports reports :eval no
################################################################
sudo apt-get install postgis
#+end_src
then let's demo the Tasmanian SLAs:
**** download the shapefiles
#+name:tassla01
#+begin_src R :session *R* :tangle no :exports reports :eval no
    ################################################################
    # name:tassla06
    # ABS spatial units are available at http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1259.0.30.0022006?OpenDocument
  setwd('..')
  dir.create('data')
  setwd('data')
  dir.create('abs_sla')
  setwd('abs_sla')
  
  download.file('http://www.abs.gov.au/AUSSTATS/subscriber.nsf/log?openagent&1259030002_sla06aaust_shape.zip&1259.0.30.002&Data%20Cubes&18E90A962EFD4D7ECA25795D00244F5A&0&2006&06.12.2011&Previous',
                  'SLA06.zip', mode = 'wb')
    unzip('SLA06.zip',junkpaths=T)
    
    sink('readme.txt')
      cat(paste('Australian Bureau of Statistics Statistical Local Areas 2006
      downloaded on', Sys.Date(),
      '
      from http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1259.0.30.0022006?OpenDocument')
      )
    sink()
    
    # and load spatial data (sd)
    install.packages('rgdal')
    require(rgdal)
    sd <- readOGR('SLA06aAUST.shp', layer = 'SLA06aAUST')
    # might take a while
    head(sd@data)
    plot(sd)
    dev.off()
    save.image('aussd.Rdata')
    
    ######################
    # tas
    sd2 <-  sd[ sd@data$STATE_CODE == 6,]
     plot(sd2)
     axis(1);axis(2); box()
    # plot(sd, add = T)
     names(sd2@data)
     writeOGR(sd2,'tassla06.shp','tassla06','ESRI Shapefile')
     test <- readOGR(dsn = 'tassla06.shp', layer = 'tassla06')
     plot(test, col = 'grey')
     rm(sd)
    # save.image('tassd.Rdata')
    
#+end_src

**** upload the shp2psql
#+begin_src sh :session *shell* :exports reports :eval no
  # psql -d ewedb -U postgres -h 115.146.94.209
  # CREATE SCHEMA abs_sla;
  # grant ALL on schema abs_sla to gislibrary;
  cd data
  shp2pgsql -s 4283 -D tassla06.shp public.tassla06 > tassla06.sql
  # psql -d ewedb -U gislibrary -W -h 115.146.94.209 -f tassla06.sql
  # warning terminal not fully functional?  ran from normal terminal
  # now on the remote server run
  # psql ewedb postgres
  # GRANT select ON ALL TABLES IN SCHEMA public TO gislibrary;
#+end_src

**** COMMENT DEPRECATED, TODO change to swishdbtools version = add metadata using df2ddi
#+name:add_ddi
#+begin_src R :session *shell* :tangle no :exports reports :eval no
  ################################################################
  # name:add_ddi
  setwd('~/disentangle')
  source('src/df2ddi.r')
  if(!require(rgdal)) install.packages('rgdal'); require(rgdal)
  if(!require(RJDBC)) install.packages('RJDBC'); require(RJDBC)
  drv <- JDBC("oracle.jdbc.driver.OracleDriver",
              '/u01/app/oracle/product/11.2.0/xe/jdbc/lib/ojdbc6.jar')
  p <- readline('enter password: ')
  h <- readline('enter target ipaddres: ')
  d <- readline('enter database name: ')
  ch <- dbConnect(drv,paste("jdbc:oracle:thin:@",h,":1521",sep=''),d,p)
  
  dir.create('metadata')
  s <- add_stdydscr(ask=T)
  #write.table(s,'metadata/stdydscr.csv',sep=',',row.names=F)
  
  s$PRODDATESTDY=format(as.Date( substr(s$PRODDATESTDY,1,10),'%Y-%m-%d'),"%d/%b/%Y")
  s$PRODDATEDOC=format(as.Date( substr(s$PRODDATEDOC,1,10),'%Y-%m-%d'),"%d/%b/%Y")
  
  dbSendUpdate(ch,
  # cat(
  paste('
  insert into STDYDSCR (',paste(names(s), sep = '', collapse = ', '),')
  VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(s),'',s)),sep='',collapse="', '"),"'",sep=''),')',sep='')
  )
  
  f <- add_filedscr(fileid = 1, idno = 'ABS_SLA', ask=T)
  f$FILELOCATION <- 'abs_sla.'
  f
  dbSendUpdate(ch,
  # cat(
  paste('
  insert into FILEDSCR (',paste(names(f), sep = '', collapse = ', '),')
  VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(f),'',f)),sep='',collapse="', '"),"'",sep=''),')',sep='')
  )
  setwd('../data')
  setwd('abs_sla')
  test <- readOGR(dsn = 'tassla06.shp', layer = 'tassla06')
  fid <- dbGetQuery(ch,
  #                  cat(
                    paste("select FILEID
                    from filedscr
                    where filelocation = '",f$FILELOCATION,"'
                    and filename = '",f$FILENAME,"'",
                    sep=''))
  d <- add_datadscr(data_frame = test@data, fileid = fid[1,1], ask=T)
  
  
  for(i in 1:nrow(d)){
  dbSendUpdate(ch,
  #i = 1
  # cat(
  paste('
  insert into DATADSCR (',paste(names(d), sep = '', collapse = ', '),')
  VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(d[i,]),'',d[i,])),sep='',collapse="', '"),"'",sep=''),')',sep='')
  )
  }
  
  
  ###################################################
  # make xml
  s <- dbGetQuery(ch, "select * from stdydscr where idno = 'ABS_SLA'")
  s
  f <- dbGetQuery(ch, "select * from filedscr where idno = 'ABS_SLA'")
  f
  for(fi in f){
  d <- dbGetQuery(ch,
                  paste("select * from datadscr where FILEID = ",f$FILEID,
                        sep = '')
                  )
  ddixml <- make_xml(s,f,d)
  }
  file.copy('abs_sla_3.xml', '/xmldata')
  setwd('~/disentangle')
  
#+end_src
    




* The Brains
The Brains is a Statistical Analysis engine (running the R environment) integrated with a metadata registry.
** R Server
*** R
#+name:R
#+begin_src sh :session *shell* :exports reports :results silent :eval no
#rpm -Uvh http://mirror.as24220.net/pub/epel/6/i386/epel-release-6-7.noarch.rpm
rpm -Uvh http://mirror.overthewire.com.au/pub/epel/6/i386/epel-release-6-7.noarch.rpm
yum install R R-devel
#+end_src

# NB on redhat 6.3 build we had to register to the optional channel the following command needs to be issued.
# rhn-channel --add --channel=rhel-x86_64-server-optional-6
# then yum install texinfo-tex.x86_64
To update R as ‘root’ on your system simply type
#+name:updateR
#+begin_src R :session *shell* :tangle no :exports reports :eval no
yum update R
#+end_src


*** TODO package management and R updates
Kudos2
http://zvfak.blogspot.com.au/2012/06/updating-r-but-keeping-your-installed.html
The problem is that when you update R you usually need to re-install your libraries or change .libPaths() to point to a location that has your previous libraries.

The solution below will work for unix-like operating systems including Mac OS X.
#+name:packageManagement
#+begin_src R :session *shell* :tangle no :exports reports :eval no
###########################################################################
# newnode: packageManagement

#First, we need a location to install all our packages from now
#on. This can be any directory, and location of this directory should
#be indicated in ~/.Renviron file. Let's create that directory now:

mkdir ~/Rlibs

#We created Rlibs directory in our home directory. Now, create the
#.Renviron file in your home directory and enter the following line
#and save the .Renviron file:

R_LIBS=~/Rlibs

# We can now start R and install any library. The libraries will be
# installed to ~/Rlibs, and when we update R, R will still look for
# libraries in ~/Rlibs directory so we don't need to re-install the
# libraries. However, we will need to update the libraries in ~/Rlibs
# directory to their most recent versions. All we need to do is to run
update.packages() 
# in R console, and the libraries will be updated.
#+end_src
*** Rstudio
check out the RSudio versions at: http://rstudio.org/download/server
#+begin_src sh :session *shell* :exports reports :eval no :results silent
# on RHEL6 ran into dependencies: libcrypto.so.6()(64bit) is needed by rstudio-server-0.96.331-1.x86_64
#	libgfortran.so.1()(64bit) is needed by rstudio-server-0.96.331-1.x86_64
#	libssl.so.6()(64bit) is needed by rstudio-server-0.96.331-1.x86_64
# http://www.linuxquestions.org/questions/linux-software-2/need-libcrypto-so-6-64bit-and-libssl-so-6-64bit-for-redhat-6-a-873068/
# yum list openssl098e*
# yum install openssl098e.x86_64
# yum list compat-libgfortran*
# yum install compat-libgfortran-41.x86_64 
wget http://download2.rstudio.org/rstudio-server-0.97.173-x86_64.rpm
sudo yum install --nogpgcheck rstudio-server-0.97.173-x86_64.rpm
rstudio-server verify-installation
#+end_src
*** firewall access
#+name:firewall
#+begin_src sh :session *shell* :exports reports :eval no :results silent
# kudos2 http://slinsmeier.wordpress.com/2012/05/19/creating-a-lab-environment-with-rstudio/
# It is necessary to open the firewall port to allow the browser
# access to RStudio: edit the 
vi /etc/sysconfig/iptables 
# file and add the line
# -A INPUT -m state --state NEW -m tcp -p tcp --dport 8787 -j ACCEPT
# directly after the opening of the ssh port 22 (or copy that line and change the port 22 to 8787).
# reminder that you need to have the security group set up on the research cloud to allow tcp from 8787 to 8787 cidr 0.0.0.0/0
service iptables restart
#+end_src
Check that you can log on via port 8787.  Note that this is an unsecure R server and the following steps are required to make this a secure server.  We will need to block port 8787 later on.
*** SSL/HHTPS and running a proxy server 
# Perhaps an alternative? http://www.investuotojas.eu/2012/08/10/rstudio-server-through-ssh/
# Or  http://rstudio.org/docs/server/running_with_proxy is only for ubuntu?

#+name:install apache
#+begin_src sh :session *shell* :exports reports :eval no :results silent
sudo yum install httpd.x86_64

# run the following interactively
sudo openssl genrsa -out /etc/pki/tls/private/rstudio.ivan.com.key 1024
# the next is one line
sudo openssl req -new -key /etc/pki/tls/private/rstudio.ivan.com.key -x509 -out /etc/pki/tls/certs/rstudio.ivan.com.crt -days 365

sudo yum install mod_ssl.x86_64 
#+end_src

#+begin_src sh :session *shell* :exports reports :eval no :results silent

#vi /etc/httpd/conf.d/ssl.conf 
# Change the paths to match where the Key file is stored. 
SSLCertificateFile /etc/pki/tls/certs/rstudio.ivan.com.crt
# Then set the correct path for the Certificate Key File a few lines below. 
SSLCertificateKeyFile /etc/pki/tls/private/rstudio.ivan.com.key

mkdir /etc/httpd/sites

# vi /etc/httpd/conf/httpd.conf 
# and add 
Include /etc/httpd/sites/
# as the last line.

# vi /etc/httpd/sites/rstudio-ivan.com

# insert
<VirtualHost *:80>

  ServerName rstudio.ivan.com
  RedirectMatch ^(.*)$ https://rstudio.ivan.com$1

</VirtualHost>
# goodo
# vi /etc/httpd/conf.d/ssl.conf
# add

  <Proxy *>
    Allow from localhost
  </Proxy>

  ProxyPass        / http://localhost:8787/
  ProxyPassReverse / http://localhost:8787/


# before </VirtualHost>


/etc/init.d/httpd restart

# weird error? ignore?
# Stopping httpd: [60G[FAILED]
# Starting httpd: httpd: apr_sockaddr_info_get() failed for i-00002979
# httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName
# [60G[  OK  ]

sudo chkconfig httpd on
# sudo vi /etc/sysconfig/iptables 
# to the previously added line for 8787 modify to 
# -A INPUT -m state --state NEW -m tcp -p tcp --dport 443 -j ACCEPT
# change the research cloud firewall rules to reflect this change

sudo service iptables restart

# sudo vi /etc/rstudio/rserver.conf
 www-address=127.0.0.1


sudo /etc/init.d/rstudio-server restart

# now going to https://your.new.ip.address/
# should ask you to add an exception
# can also try sudo reboot?
#+end_src

# NB couldn't follow dave's install here as can't find distcache in lib

# NOT RUN vi +/SSLCertificateFile /etc/httpd/conf.d/ssl.conf

*** COMMENT test Sweave
log on to Rstudio at https://your.new.ip.address and copy the following into a new RNW sweave file (might need to modify the sweave options in tools, something to do with tex2dvi?)
#+name:learnR
#+begin_src R :session *R* :tangle no :exports reports :eval no
\documentclass[a4paper]{article}
\usepackage{fancyhdr} %For headers and footers
\pagestyle{fancy} %For headers and footers
\usepackage{lastpage} %For getting page x of y
\usepackage{float} %Allows the figures to be positioned and formatted nicely
\floatstyle{boxed} %using this
\restylefloat{figure} %and this command
\usepackage{url} %Formatting of yrls
\usepackage{verbatim}
\usepackage{cite} 
\usepackage{hyperref} 
%Define all the headers and footers
\lhead{}
\chead{NCEPH Working Paper}
\rhead{}
\lfoot{Ivan C Hanigan}
\cfoot{\today}
\rfoot{\thepage\ of \pageref{LastPage}}
\usepackage{Sweave}
\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
%\input{learnR-concordance}
\title{Example Sweave Document}
\author{Ivan C. Hanigan$^{1}$}
\date {\today}
\maketitle
\begin{itemize}
\item [$^1$] National Centre for Epidemiology and Population Health, \\Australian National University.
\end{itemize}

\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents 
\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}
This is an introduction to some resources that are useful for learning R.  
\section{The R code that produced this report}
It is important to appreciate that R is free and open source software.  This means that any code you write can be viewed and modified by others.  In some cases we need to protect our Intellectual Property and the following statement is an attempt to ascribe copyright to our work, even though it remains open source.

``I support the philosophy of Reproducible Research \cite{Peng2011}, and where possible I provide data and code in the statistical software R that will allow analyses to be reproduced.  This document is prepared automatically from the associated Sweave (RNW) file.  If you do not have access to the RNW file please contact me.''
<<eval=FALSE,echo=FALSE,keep.source=TRUE>>=
cat('
 #######################################################################
 ## The R code is free software; please cite this paper as the source.  
 ## Copyright 2012, Ivan C Hanigan <ivan.hanigan@gmail.com> 
 ## This program is free software; you can redistribute it and/or modify
 ## it under the terms of the GNU General Public License as published by
 ## the Free Software Foundation; either version 2 of the License, or
 ## (at your option) any later version.
 ## 
 ## This program is distributed in the hope that it will be useful,
 ## but WITHOUT ANY WARRANTY; without even the implied warranty of
 ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 ## GNU General Public License for more details.
 ## Free Software
 ## Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
 ## 02110-1301, USA
 #######################################################################
')
@ 


\subsection{func}
I'll use the following packages:
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=  
if(!require(xtable)) install.packages('xtable', repos = 'http://cran.csiro.au')
require(xtable)
#require(ggplot2)
#require(ProjectTemplate)
@
<<eval=FALSE,echo=FALSE,keep.source=TRUE>>=  
create.project('analysis', minimal = TRUE)
dir.create('analysis/reports')
# the plan
@
\subsection{Some Code}
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=
x<-rnorm(100,10,5)
y<-rnorm(100,20,15)
fit <- lm(y~x)
summary(fit)
@
Using the xtable package allows results to be displyed in tables and has built in support for some R objects, so summrising the linear fit above in Table ~\ref{ATable}.
<<eval=TRUE,echo=FALSE,results=tex>>=
require(xtable)
xtable(fit, caption="Example Table",digits=4,table.placement="H",label="ATable")
@
\subsection{A Plot}
 
Plots intergrate easily, using the \LaTeX float package as can be seen in figure ~\ref{aPlot.png}.  However I like to make them as pngs and then include.

<<eval=TRUE,echo=FALSE,keep.source=TRUE>>=  
png('aPlot.png', res=200,width = 600, height = 600)
plot(x,y,main="Example Plot",xlab="X Variable",ylab="Y Variable")
abline(fit,col="Red")
dev.off()
@
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{aPlot.png}
\caption{aPlot.png}
\label{fig:aPlot.png}
\end{figure}
\clearpage
\section{Remembering the points}
This blog post \url{http://www.win-vector.com/blog/2012/04/how-to-remember-point-shape-codes-in-r/} says:

I suspect I am not unique in not being able to remember how to control the point shapes in R. Part of this is a documentation problem: no package ever seems to write the shapes down. All packages just use the usual set that derives from S-Plus and was carried through base-graphics, to grid, lattice and ggplot2. The quickest way out of this is to know how to generate an example plot of the shapes quickly. We show how to do this in ggplot2. This is trivial- but you get tired of not having it immediately available.


I like it but it is not as complate as the plot shown in Figure \ref{fig:pchopts.png} from the `R for Beginners' document by Emmanuel Paradis \cite{Paradis2002}.  I also find I often get disoriented using ggplot2.

<<eval=TRUE, echo=FALSE>>=
# it had to be fixed
# sum <- ggplot()
# for(i in 1:25) {
#    sum <- sum +
#       geom_point(data=data.frame(x=c(i)),aes(x=x,y=x),shape=i)
# }
# sum
# but this still doesn't work properly
# ggplot(data=data.frame(x=as.factor(1:16))) + geom_point(aes(x=x,y=x)) +
#     facet_wrap(~x,scales='free')
# I like base graphics anyway
png('pchopts.png')
par(mfrow=c(3,10), mar=c(0,0,2,0))
for(i in c(1:25)){
 plot(1,1,pch=i, axes=F, cex = 3, col = 'blue', bg = 'yellow')
 title(i)
 }
for(i in c("*", "?", ".", "X", "a")){
 plot(1,1,pch=i, axes=F, cex = 3, col = 'blue', bg = 'yellow')
 title(i)
 }
dev.off()
@
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{pchopts.png}
\caption{pchopts.png}
\label{fig:pchopts.png}
\end{figure}


\section{Conclusion}
In conclusion, sweave rocks.


\begin{thebibliography}{1}
\bibitem{Paradis2002}
Emmanuel Paradis.
\newblock {R for Beginners}.
\newblock 2002.

\bibitem{Peng2011}
Roger~D Peng.
\newblock {Reproducible research in computational science.}
\newblock {\em Science (New York, N.Y.)}, 334(6060):1226--7, December 2011.

\end{thebibliography}

\section{System State}
<<eval=TRUE,echo=TRUE,keep.source=TRUE>>=
sessionInfo()
@




\end{document}

#+end_src

*** git
#+name:git
#+begin_src sh :session *shell* :exports reports  :results silent :eval no
yum install git
# restart R studio
#+end_src
*** ssh for github
- in rstudio
- tools / options / version control
- create rsa key, ok, ok
- view pub key, copy, paste to your github account
*** gdal
#+name:gdal
#+begin_src sh :session *shell* :exports reports :results silent :eval no
sudo rpm -Uvh http://elgis.argeo.org/repos/6/elgis/x86_64/elgis-release-6-6_0.noarch.rpm
# yum list gdal*
yum install gdal-devel.x86_64 proj-devel.x86_64
#+end_src

*** COMMENT if proj not working
#+begin_src sh :session *shell* :exports reports :results silent :eval yes
#not working proj-devel?
yum erase proj XXX
wget http://elgis.argeo.org/repos/6/elgis/x86_64/gdal-devel-1.8.1-1.el6.x86_64.rpm
yum install gdal-devel-1.8.1-1.el6.x86_64.rpm
# fail?
wget http://elgis.argeo.org/repos/6/elgis/x86_64/proj-devel-4.7.0-2.el6.x86_64.rpm
yum install proj-devel-4.7.0-2.el6.x86_64.rpm

# and now
yum install gdal-devel.x86_64 
# success!
# but now postgis not working.  reinstall and it works
#+end_src
*** geos
#+begin_src sh :session *shell* :exports reports :results silent :eval no
yum install geos-devel.x86_64
#+end_src
*** or under ubuntu
#+name:gdal
#+begin_src sh :exports reports
  sudo apt-get update
  sudo apt-get install libgdal1-dev
  sudo apt-get install libproj-dev
  # OR
  # You need the development packages of GDAL and proj4. Probably easier to
  #install from repository than from source. Try:
  
  # sudo apt-get install libgdal1-dev libproj-dev
  # sudo R
  # install.packages("rgdal")
  
#+end_src

#+RESULTS: gdal


*** test readOGR
NB only possible once the PostGIS install is complete, see below.
#+name:readOGR2
#+begin_src R :session *shell* :tangle no :exports reports :eval no
  ################################################################
  # name:readOGR2
  
  readOGR2 <- function(hostip=NA,user=NA,db=NA, layer=NA, p = NA) {
   # NOTES
   # only works on Linux OS
   # returns uninformative error due to either bad connection or lack of record in geometry column table.  can check if connection problem using a test connect?
   # TODO add a prompt for each connection arg if isna
   if (!require(rgdal)) install.packages('rgdal', repos='http://cran.csiro.au'); require(rgdal)
   if(is.na(p)){
   pwd=readline('enter password (ctrl-L will clear the console after): ')
   } else {
   pwd <- p
   }
   shp <- readOGR(sprintf('PG:host=%s
                           user=%s
                           dbname=%s
                           password=%s
                           port=5432',hostip,user,db,pwd),
                           layer=layer)
  
   # clean up
   rm(pwd)
   return(shp)
   }
  
  tassla06 <- readOGR2(hostip='115.146.95.82',user='gislibrary',db='ewedb', layer='tassla06')
  
                                          # func
  if (!require(rgdal)) install.packages('rgdal'); require(rgdal)
  if(!require(ggmap)) install.packages('ggmap'); require(ggmap)
  epsg <- make_EPSG()
  # load
  latlong <- read.table(tc <- textConnection(
    "ID  POINT_Y   POINT_X
    1  150.5556 -35.09305
    2  150.6851 -35.01535
    3  150.6710 -35.06412
    4  150.6534 -35.08666
    "), header = TRUE); close(tc)
  # do
  for(i in 1:nrow(latlong)){
    coords <- as.numeric(latlong[i,c('POINT_Y', 'POINT_X')])
    e <- as.data.frame(cbind(i, t(coords), revgeocode(coords)))
    write.table(e, "test.csv", sep = ',', append = i > 1, col.names = i == 1, row.names = F)
  }
  d <- read.csv('test.csv')
  head(d)
  ## Treat data frame as spatial points
  pts <- SpatialPointsDataFrame(cbind(d$V2,d$V3),d,
                                proj4string=CRS(epsg$prj4[epsg$code %in% '4283']))
  writeOGR(pts, 'test.shp', 'test', driver='ESRI Shapefile')
  
#+end_src

*** rgraphviz

#+name:gviz
#+begin_src sh :session *shell* :exports reports :results silent :eval no 
wget http://www.graphviz.org/graphviz-rhel.repo
mv graphviz-rhel.repo /etc/yum.repos.d/ 
yum list available 'graphviz*'
yum install 'graphviz*'

# as root
R
source('http://bioconductor.org/biocLite.R')
biocLite("Rgraphviz")
q()
#+end_src

*** COMMENT under ubuntu
# if on ubuntu kudos2 http://vladinformatics.blogspot.com.au/2012/03/my-experience-with-installing-rgraphviz.html 
make sure libgraphviz-dev is installed. It is needed for some header files (e.g. gvc.h)
sudo apt-get install libgraphviz-dev
then
sudo R
source('http://bioconductor.org/biocLite.R')
biocLite("Rgraphviz", configure.args=c("--with-graphviz=/usr"))
the reason is that at least on my comp the dot program was in /usr/bin, but not in /usr/local/bin as Rgraphviz defaults 

*** test
#+name:test-rgraphviz
#+begin_src R :session *shell* :tangle no :exports reports :eval no
###########################################################################
# newnode: test-rgraphviz
try newnode_test from
git@github.com:ivanhanigan/disentangle.git
#+end_src

*** install just the postgres bits required for RPostgreSQL package
#+name:rpostgresql
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
###########################################################################
# newnode: rpostgresql
vi /etc/yum.repos.d/CentOS-Base.repo
# append: exclude=postgresql* to [base] and [updates] sections
curl -O http://yum.postgresql.org/9.1/redhat/rhel-6-x86_64/pgdg-centos91-9.1-4.noarch.rpm
rpm -ivh pgdg-centos91-9.1-4.noarch.rpm
# kudos2 http://www.davidghedini.com/pg/entry/install_postgresql_9_on_centos
# Many, if not most, third party software and modules are still be set to look for PoistgreSQL's conf file and data directory under their old (pre-version 9) locations.
# You can address this, and make life easier for yourself, by creating a few symlinks from the new locations to the old.
# Symlink 1: Symlink for the binary directory. This is particularly useful as this is the location of the pg_config file
# view plaincopy to clipboardprint?
# so install the basic packages for a database
# install a basic PostgreSQL 9.1 server:
yum install postgresql91-server postgresql91 postgresql91-devel postgresql91-libs postgresql91-contrib
# THIS LINE HERE
ln -s /usr/pgsql-9.1/bin/pg_config /usr/bin  
# now check
R
install.packages('RPostgreSQL')
# works?

#+end_src
*** postgis utilities
#+name:postgisutils
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
################################################################
# name:postgisutils
yum list postgis2*
yum install postgis2_91.x86_64 postgis2_91-devel.x86_64 
# try 
/usr/pgsql-9.1/bin/raster2pgsql

# to use this and psql via CMD line need to add using R
su - user
R
sink('~/.pgpass')
cat('hostname:port:database:username:password')
sink()
q()
exit
chmod 0600 /home/user/.pgpass
# see http://www.postgresql.org/docs/current/static/libpq-pgpass.html
# and other methods

#+end_src


*** TODO unixODBC
#+name:unixODBC
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
################################################################
# name:unixODBC
yum list unixODBC*
yum install unixODBC.x86_64  unixODBC-devel.x86_64 unixODBC-kde.x86_64 
R
install.packages('RODBC')
require(RODBC)
#+end_src

** Test the Backups of this Minimal R Sever.
To test that this will restore successfully if there is a crash or the VM becomes corrupted take a snapshot from the ResearchCloud dashboard website and launch it to a new VM.  Note that this will only restore the 10GB primary disc which has our software but not the user data which is on the secondary disc so try mounting that and check that the new VM will be a good replacement of the old one.
*** Backup the 2nd Disc
#+name:2ndDisc
#+begin_src sh :session *sh* :tangle no :exports reports :eval no
################################################################
# name:2ndDisc
# on your secure local machine
mkdir /projects/OpenSoftware-RestrictedData/Archives
# on your remote server

ssh root@ip.address.of.server
cd /
mkdir backups
cd backups
zip -r homebackup_yyyymmdd /home
#scp homebackup_yyyymmdd.zip root@ip.address.of.server:/Archives
#or use the R studio download file function
# upload to the new R server
exit
scp /Archives/homebackup.zip root@ipaddress:/home
# and unzip (TODO need to mv these up to the /home/ level after unzip)
ssh root@ip.address.of.server
cd home
unzip homebackup.zip
rm homebackup.zip
#+end_src
# zip -r homebackup_20121121 /home
# scp /homebackup_20121121.zip ivan_hanigan@150.203.74.179:/home/ivan_hanigan/projects/OpenSoftware-RestrictedData/Archives



*** Launch from this snapshot and test the R server and 2nd Disc

*** TODO the permissions of the user on their home directory cause issues for logging in.  
I've just used userdel -r username, then useradd etc and use their own user account to scp the backup files across.
One solution is to not offer backup services, then users will have to undertake to load all data again from a fresh server if it failed.  Data could be backed up on Brawn only?

** Oracle XE Permissions and Users System
Installing Oracle XE, this is used as a data registry and I have a HTML-DB application available from XXXXXX to be used.  It is called OraPUS because the Oracle XE license restricts any references to Oracle in the name of the application.  There are other restrictions on this licence such as XXXXX.
*** backup local ubuntu version
I have a ubuntu pc as my main desktop and also backups
to create oracle on this I needed a centos virtual Machines
use the instructions at http://extr3metech.wordpress.com/2012/10/25/centos-6-3-installation-in-virtual-box-with-screenshots/
and also the comments ie:
@Gunwant, One way for configuring the network, open your virtual box settings and go to the “Network” tab and choose the option “Attached to” to “Bridged Adapter”.

Then boot your centos virtual machine, and type the following in your terminal:

# vi /etc/sysconfig/network-scripts/ifcfg-eth0

And then change the file to the following:

DEVICE=”eth0″
BOOTPROTO=”dhcp”
ONBOOT=”yes

And save the file and exit. After that you need to restart the network service and you can do that by typing the following in the terminal:

# service network restart

Now, you can check your command

# yum install httpd

Now, it should work. I will be posting a detailed article soon for my readers soon. Thank you for visiting my blog. Feel free to ask any questions in the comments section. Have fun!

# NB noticed this needs a cable plugged in or network service failed to start

*** COMMENT Dave did a bit different
**** TODO document Dave's different build
Oracle 11g XE

Pre-requirements

Either via yum or rpm off the install media.

[root@oracle Server]# rpm -ivh libaio-0.3.106-5.x86_64.rpm \
> bc-1.06-21.x86_64.rpm \
> flex-2.5.4a-41.fc6.x86_64.rpm \
> unzip-5.52-3.el5.x86_64.rpm

scp oracle-xe-11-* to /tmp

On the server

[root@oracle ~]# cd /tmp
[root@oracle tmp]# unzip oracle-xe-11.2.0-1.0.x86_64.rpm.zip
[root@oracle tmp]# cd Disk1/
[root@oracle Disk1]# rpm -ivh oracle-xe-11.2.0-1.0.x86_64.rpm
Preparing...                ########################################### [100%]
   1:oracle-xe              ########################################### [100%]
Executing post-install steps...
You must run '/etc/init.d/oracle-xe configure' as the root user to configure the database.
[root@oracle Disk1]# /etc/init.d/oracle-xe configure

Oracle Database 11g Express Edition Configuration
-------------------------------------------------
This will configure on-boot properties of Oracle Database 11g Express
Edition.  The following questions will determine whether the database should
be starting upon system boot, the ports it will use, and the passwords that
will be used for database accounts.  Press <Enter> to accept the defaults.
Ctrl-C will abort.

Specify the HTTP port that will be used for Oracle Application Express [8080]:8180

Specify a port that will be used for the database listener [1521]:

Specify a password to be used for database accounts.  Note that the same
password will be used for SYS and SYSTEM.  Oracle recommends the use of
different passwords for each database account.  This can be done after
initial configuration:
Confirm the password:

Do you want Oracle Database 11g Express Edition to be started on boot (y/n) [y]:y

Starting Oracle Net Listener...Done
Configuring database...Done
Starting Oracle Database 11g Express Edition instance...Done
Installation completed successfully.

Comment - Optional: To avoid a bit of typing

[root@oracle ~]# cd /u01/app/oracle/
[root@oracle oracle]# ln -s /u01/app/oracle/product/11.2.0/xe/bin/oracle_env.sh set_env.sh
[root@oracle oracle]# su - oracle
-bash-3.2$ . set_env.sh
-bash-3.2$ sqlplus /nolog

SQL*Plus: Release 11.2.0.2.0 Production on Mon Nov 5 12:25:08 2012

Copyright (c) 1982, 2011, Oracle.  All rights reserved.

SQL> connect sys / password as sysdba
Connected.

End of Comment


[root@oracle /]# su - oracle
-bash-3.2$ . /u01/app/oracle/product/11.2.0/xe/bin/oracle_env.sh
-bash-3.2$ sqlplus /nolog

SQL*Plus: Release 11.2.0.2.0 Production on Mon Nov 5 11:20:45 2012

Copyright (c) 1982, 2011, Oracle.  All rights reserved.

SQL> connect sys/password as sysdba
Connected.

SQL> EXEC DBMS_XDB.SETLISTENERLOCALACCESS(FALSE);

PL/SQL procedure successfully completed.

SQL> @  /u01/app/oracle/product/11.2.0/xe/apex/apxchpwd.sql
Enter a value below for the password for the Application Express ADMIN user.

Enter a password for the ADMIN user              []

Session altered.

...changing password for ADMIN

PL/SQL procedure successfully completed.


Commit complete.

SQL> exit
Disconnected from Oracle Database 11g Express Edition Release 11.2.0.2.0 - 64bit Production
-bash-3.2$ exit

Comments 

- Discuss with Ivan the IP ranges that should be able to connect to apex, e.g. NCEPH Network, NCEPH VPN etc. and edit iptables accordingly	
	 
- In your browser naviagate to http://<ipaddress/hostname>:8180/apex and make sure that the page is displaying.  If satisifed supply Ivan with the temporary password for the internal workspace.

- Discuss with Ivan if ArchiveLog mode needs to be enable or is offline backups of the database(s) sufficent.

*** INIT
#+name:init
#+begin_src sh :session *shell* :tangle no :eval no
################################################################
# name:init
yum update
# SElinux config
# vi /etc/selinux/config
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# Change SELINUX=enforcing to disabled and you must reboot the server after applying the change.
# vi /etc/hosts
127.0.0.1  localhost.localdomain localhost ADD-VMNAME 
#+end_src
# for eg i-00002979
# NB saw this on a youtube demo, not done10.10.10.1 oradb.localdomain oradb
*** SWAP
MAKE SURE YOU HAVE 2GB SWAP, this is required for install but I have deleted it aftward and things still work
http://bvirtual.nl/2009/08/add-more-swap-space-to-running-linux.html
I SET TO 3000, didn't set to start at boot so have to 
swapon /swapfile
#+name:swap
#+begin_src sh :session *shell* :tangle no  :eval no
################################################################
# name:swap
#Create an empty file called /swapfile from about 2GB
dd if=/dev/zero of=/swapfile bs=1024000 count=3000
#Format the new file to make it a swap file
mkswap /swapfile
#Enable the new swapfile. Only the swapon command is needed, but with
#the free command you can clearly see the swap space is made available
#to the system.
free -m | grep Swap
swapon /swapfile
free -m | grep Swap
# TOO LAZY TO ADD TO BOOT SO JUST DO SWAPON EVERY TIME?
#+end_src

*** DOWNLOAD AND SCP
create free oracle account.  download to local and then upload to server.
#+name:upload-via-scp
#+begin_src R :session *R* :tangle no :eval no
scp oracle-xe-11.2.0-1.0.x86_64.rpm.zip user@ip.address.of.server:/home/
#+end_src
*** Install the database
I followed these online guides:
http://youtu.be/DRZg8tfmldA (this needs VNC or other)
http://www.davidghedini.com/pg/entry/install_oracle_11g_xe_on

#+name:install-oracle
#+begin_src R :session *R* :tangle no  :eval no
yum install libaio bc flex unzip 
cd /home
mkdir OracleXE
unzip oracle-xe-11.2.0-1.0.x86_64.rpm.zip -d OracleXE 
cd OracleXE/Disk1
rpm -ivh oracle-xe-11.2.0-1.0.x86_64.rpm  
/etc/init.d/oracle-xe configure
# accept defaults
#set up the etc/group file so that oracle is in dba (and oinstall? not there?)
id -a oracle
#log on as oracle and then 
su - oracle
# set the environment
cd /u01/app/oracle/product/11.2.0/xe/bin  
. ./oracle_env.sh 
# NOT DONE, which users needs this?  which dot file?  both?  To set
# the environment permanently for users, add the following to the
# .bashrc or .bash_profile of the users you want to access the
# environment:
     . /u01/app/oracle/product/11.2.0/xe/bin/oracle_env.sh 
sqlplus /nolog
connect sys/password as sysdba
#To allow remote access to Oracle 11g XE GUI (as well as Application Express GUI) issue the following from SQL*Plus
EXEC DBMS_XDB.SETLISTENERLOCALACCESS(FALSE);  
exit
su - root
# modify firewall
# vi /etc/sysconfig/iptables
# copy -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
# twice and modify for 8080 and 1521
# will also need the port 8080 enabled on the research cloud firewall
service iptables restart
#+end_src
Now we can check the database service is running by pointing a browser at
http://ip.address.of.server:8080/apex

Although Apex is already installed, we still need to set the Internal Admin password. 
#+name:config
#+begin_src R :session *R* :tangle no  :eval no
# To do so, run the apxchpwd.sql located under /u01/app/oracle/product/11.2.0/xe/apex:
su - oracle
. /u01/app/oracle/product/11.2.0/xe/bin/oracle_env.sh
sqlplus /nolog
connect sys/password as sysdba
@/u01/app/oracle/product/11.2.0/xe/apex/apxchpwd.sql 
#Note: pick something simple like Password123! as you will be prompted to change it on first log in anyways.
# access the Application Express GUI at:
http://ip.address.of.server:8080/apex/f?p=4550:1
#+end_src

Now log on:
- Workspace: Internal
- User Name: admin
- Password: (whatever you selected above).
- log in, change password
- log in again
Now create Workspace:
- DDIINDEXDB
- 100MB
- admin user IVAN
*** import application and set Security
import the application 102
then set authentication 
I followed the advice from the oracle documentation http://docs.oracle.com/cd/E17556_01/doc/user.40/e15517/sec.htm#BCGICEAH
Changing the Authentication Scheme Associated with an Application.
To change the authentication scheme for an application:

Navigate to the Authentication Schemes:
- On the Workspace home page, click the Application Builder icon.
- Select an application.
- On the Application home page, click Shared Components.
- The Shared Components page appears.
- Under Security, select Authentication Schemes.
- Click the Change Current tab at the top of the page.
- From Available Authentication Schemes, select a new authentication scheme and  click Next.
- (Application Express is good)
- Click Make Current.

*** COMMENT or an alternative reference?
http://docs.oracle.com/cd/E17556_01/doc/user.40/e15517/sec.htm
13 Managing Application Security
Setting Up Application Express Account Credentials
To set up Application Express Account Credentials:
- On the Workspace home page, click the Application Builder icon.
- Select an application.
- On the Application home page, click Shared Components.
- The Shared Components page appears.
- Under Security, select Authentication Schemes.
- On the Authentication Schemes page, click Create.
- Select Based on a pre-configured scheme from the gallery.
- From Gallery, select Show Login Page and Use Application Express Account Credentials.
- Specify a login page and click Next.
- Enter a name and click Create Scheme.

*** TODO explain the table creation script 
now go to lib/setup_oracle_of_delphe.r
to build the tables
*** set up R, RJDBC and ROracle
Using RJDBC is what I recommend.  I found ROracle suboptimal.

# might need oracle installed and then
# on shell
# . /u01/app/oracle/product/11.2.0/xe/bin/oracle_env.sh
# export LD_LIBRARY_PATH=/u01/app/oracle/product/11.2.0/xe/lib:$LD_LIBRARY_PATH
# install.packages('ROracle')
# require('ROracle')

#+name:using-ROracle
#+begin_src R :session *shell* :tangle no  :eval no
  su root
  #make sure env is set
  export LD_LIBRARY_PATH=/u01/app/oracle/product/11.2.0/xe/lib:$LD_LIBRARY_PATH
  
  R
  connect2oracle <- function(){
  if(!require(RJDBC)) install.packages('RJDBC'); require(RJDBC)
  drv <- JDBC("oracle.jdbc.driver.OracleDriver",
              '/u01/app/oracle/product/11.2.0/xe/jdbc/lib/ojdbc6.jar')
  p <- readline('enter password: ')
  h <- readline('enter target ipaddres: ')
  d <- readline('enter database name: ')
  ch <- dbConnect(drv,paste("jdbc:oracle:thin:@",h,":1521",sep=''),d,p)
  return(ch)
  }  
  ch <- connect2oracle()
  
  
  # OR
  #install.packages('ROracle')
#+end_src

*** TODO maintenance
** unlock
*** unlock header
#+name:unlock-header
#+begin_src markdown :tangle no :exports none :eval no :padline no
  ---
  name: unlock
  layout: default
  title: unlock
  ---
  
  http://riaschissl.blogspot.com.au/2010/07/oracle-unexpire-and-unlock-accounts.html
  
  Some of our customers' applications are built around Oracle, so we have to fight the beast from time to time. Unfortunately, some of the surprizes the beast has to offer are quite random and rare, and due to this we tend to simply forget how we fixed and/or circumvented the issues previously.
  
  As usual, google is your friend and one of the most valuable resources on the net we've found is www.orafaq.com and most notably its security FAQs [1]
  
  So this is just an attempt of a small cheat sheat to help our overloaded brains :-)
  expired and locked accounts - the basics
  Now, as of version 11g, Oracle has enabled account expiration per default for many vital accounts (such as SYSMAN, SYS, ...). Quite a weird idea in my view, but who knows what hyper security things some wise engineer had in mind when doing so.
  
  Beware that we run Oracle under various Linux flavours, so things might be different for you.
  
  become database admin
  First log into your host running oracle and become the oracle user.
  
  #### Code:unlock
      sqlplus /nolog
      connect / as SYSDBA
  
  
  
  find out which accounts are expired
  select username, account_status from dba_users where ACCOUNT_STATUS LIKE '%EXPIRED%';
  unexpire an account
  once an account has been expired, it can only be revived by assigning it a new password:
  
  #### code
      ALTER USER scott IDENTIFIED BY password;
  
  
  unlock an account
  ALTER USER scott ACCOUNT UNLOCK;
  disable default password expiry [2]
  this all depends on the profile a user belongs to, to disable password expiry for all users assigned the default user profile do this:
  ALTER PROFILE DEFAULT LIMIT PASSWORD_LIFE_TIME UNLIMITED;    
#+end_src

** OpenGeo Suite /opengeosuite.html
*** The OpenGeo Suite Installer
#+begin_src markdown :tangle opengeosuite.md :exports reports :eval no :padline no
--- 
name: opengeosuite
layout: default
title: OpenGeo Suite
---

  <li><a href="#sec-5-3-8">TODO Previous: set up R, RJDBC and ROracle</a></li>
  <li><a href="/opengeosuite-upgrade-tomcat6.html">Next: Upgrade to latest tomcat6 version</a></li>
<p></p>

The OpenGeo Suite provides a complete spatial data server software stack.  I use this for the geoserver functionality as it enables batch uploades for many spatial tables from the Brawn database at once.  [Installation instructions are here](http://suite.opengeo.org/opengeo-docs/installation/linux/centos/suite.html) but beware that the Redhat version requires the optional channel to be enabled. 

#+end_src

#+begin_src markdown :tangle opengeosuite.md :exports reports :eval no :padline no
#### Code
    cd /etc/yum.repos.d
    wget http://yum.opengeo.org/suite/v3/centos/6/x86_64/OpenGeo.repo
    yum install opengeo-suite
#+end_src  


*** Upgrade to latest tomcat6 version


#+begin_src markdown :tangle opengeosuite-upgrade-tomcat6.md :exports none :eval no :padline no
---
name: opengeosuite-upgrade-tomcat6
layout: default 
title: Opengeosuite upgrade tomcat6
---

  <li><a href="/opengeosuite.html">Previous: The OpenGeo Suite Installer</a></li>
  <li><a href="#sec-5-4-3">TODO Next: Otherwise just install normal geoserver</a></li>
<p></p>

The installer pulls down tomcat6 version 6.0.24 (even if there is a later version installed and running (as at April 2013) which has [vulnerabilities](http://tomcat.apache.org/security-6.html#Apache_Tomcat_6.x_vulnerabilities) so upgrade that to the latest tomcat6 (NOT tomcat 7).

Once you are up-to-date you can bulk load layers from the postgis store with [these instructions](http://workshops.opengeo.org/suiteintro/geoserver/importdb.html#import-from-postgis-store).

#+end_src

#+begin_src markdown :tangle opengeosuite-upgrade-tomcat6.md :exports reports :eval no 
  #### Code
      service tomcat6 stop
      chkconfig tomcat6 off
            
      wget http://mirror.overthewire.com.au/pub/apache/tomcat/tomcat-6/v6.0.36/bin/apache-tomcat-6.0.36.tar.gz
      mv apache-tomcat-6.0.36.tar.gz  /usr/share/
      cd /usr/share
      tar -xzf apache-tomcat-6.0.36.tar.gz
      #To start Tomcat, go to the bin folder of Tomcat installation and then run the startup.sh script,
      cd /usr/share/apache-tomcat-6.0.36
      vi conf/server.xml
      # change <Connector port="8181"/> from 8080
      vi conf/tomcat-users.xml
      #add a user with roles of admin,manager like this
      <user name="tomcat" password="password" roles="admin,manager" />
      
      #To start Tomcat, go to the bin folder of Tomcat installation and then run the startup.sh script,
      ./bin/startup.sh     
      ./bin/shutdown.sh
  
      # make a script for starting
      cd /etc/init.d  
      vi tomcat
      # add this  
      # to find java_home "update-alternatives --display java" and remove bin/java
      #!/bin/bash  
      # description: Tomcat Start Stop Restart  
      # processname: tomcat  
      # chkconfig: 234 20 80  
      JAVA_HOME=/usr/lib/jvm/jre-1.6.0-openjdk.x86_64
      export JAVA_HOME  
      PATH=$JAVA_HOME/bin:$PATH  
      export PATH  
      CATALINA_HOME=/usr/share/apache-tomcat-6.0.36  
        
      case $1 in  
      start)  
      sh $CATALINA_HOME/bin/startup.sh  
      ;;   
      stop)     
      sh $CATALINA_HOME/bin/shutdown.sh  
      ;;   
      restart)  
      sh $CATALINA_HOME/bin/shutdown.sh  
      sh $CATALINA_HOME/bin/startup.sh  
      ;;   
      esac      
      exit 0  
      # now
      chmod 755 tomcat  
      chkconfig --add tomcat  
      chkconfig --level 234 tomcat on  
      # verify
      chkconfig --list tomcat  
      service tomcat start  
      # test stop, start, restart
      more /usr/share/apache-tomcat-6.0.36/logs/catalina.out  
      # to check for errors, but is all greek to me
      # so assuming ddiindex was previously running on tomcat6
      service tomcat stop
  
  
      cd /usr/share/tomcat6/webapps
      cp -r dashboard /usr/share/apache-tomcat-6.0.36/webapps
      cp -r geowebcache /usr/share/apache-tomcat-6.0.36/webapps
      cp -r geoserver /usr/share/apache-tomcat-6.0.36/webapps    
      cp -r opengeo-docs /usr/share/apache-tomcat-6.0.36/webapps        
      cp -r geoexplorer /usr/share/apache-tomcat-6.0.36/webapps            
      cp -r recipes /usr/share/apache-tomcat-6.0.36/webapps                
      cd /usr/share/apache-tomcat-6.0.36
      service tomcat start

      #now you have installed the suite go into the geoserver website  
      # http://ip.address.of.server:8181/dashboard
      # then go to the geoserver and log in as admin with password geoserver
      # follow instructions there to enhance the security

      # for The default user/group service should use digest password encoding see
      #  http://suite.opengeo.org/docs/geoserver/security/tutorials/digest/index.html
#+end_src  

*** COMMENT notes re OpenGeo Suite installer
# broke, couldn't stop
reboot now
chkconfig tomcat off
reboot now
service tomcat6 start
chkconfig --add tomcat6
chkconfig --level 234 tomcat6 on  
# verify
chkconfig --list tomcat6
service tomcat6 stop  
service tomcat6 start  
#service tomcat6 restart  
#failed
reboot
# check website OK? yes
service tomcat6 restart  

http://workshops.opengeo.org/suiteintro/geoserver/importdb.html#import-from-postgis-store
http://dev.horizon.opengeo.org/opengeo-docs/installation/linux/centos/suite.html#installation-linux-centos-suite
http://workshops.opengeo.org/suiteintro/postgis/createdb.html
#+name:opengeo-suite
#+begin_src sh :tangle no :exports none :eval no
yum remove gdal*
yum remove proj*
yum remove geos*
cd /etc/yum.repos.d
#CentOS 6, 64 bit
wget http://yum.opengeo.org/suite/v3/centos/6/x86_64/OpenGeo.repo
yum install opengeo-suite
# it installs tomcat 6.0.24 and claims: 
# http://workshops.opengeo.org/postgis-spatialdbtips/installation.html
# The Tomcat install for the workshop is not the default install you can download from the Tomcat web site. We have taken Tomcat 6 and added in extra software and configuration for the workshop:

# - We have configured a JNDI database connection so that we can easily connect to our “medford” database in PostgreSQL.
# - We have added a copy of GeoServer and configured it to pull layers from our “medford” database.
# - We have added the scripts needed for our workshop examples.
# - We have added a recent copy of OpenLayers.
# So should I give up on tomcat 6.0.36 and just try to secure 6.0.24?
chkconfig tomcat off
service tomcat stop
# also noticed NOTICE run /usr/share/opengeo-suite/geoserver-setup.sh?
# go to http://server:8181/dashboard, click geoserver
login as admin, geoserver
# change password
su - postgres

# TODO 
# describe create postgis database from postgis_template

# Please read the file /usr/share/opengeo-suite-data/geoserver_data/security/masterpw.info and remove it afterwards. This file is a security risk.

# Please remove the file /usr/share/opengeo-suite-data/geoserver_data/security/users.properties.old because it contains user passwords in plain text. This file is a security risk.

# The default user/group service should use digest password encoding.
# http://suite.opengeo.org/docs/geoserver/security/tutorials/digest/index.html

cd /usr/share/tomcat6/webapps
cp -avr geowebcache /usr/share/apache-tomcat-6.0.36/webapps/
# 3cmd, geowebcache, recipes, geoexplorer,  dashboard, geoserver, opengeo-docs
service tomcat restart
#+end_src

*** Otherwise just install normal geoserver
#+begin_src R :session *shell* :tangle no :exports reports :eval no
################################################################

# java
# yum install java-1.6.0-openjdk-devel
http://coastalrocket.blogspot.com.au/2012/08/installing-geostack-on-centos-6-64bit.html
yum -y install tomcat6 tomcat6-webapps tomcat6-admin-webapps 
# or
# http://newpush.com/2011/10/how-to-install-tomcat-6-on-rhel-6-or-centos-6/
# yum install yum-priorities
# rpm -Uvh http://apt.sw.be/redhat/el6/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm
# #rpm -Uvh http://download.fedora.redhat.com/pub/epel/6/x86_64/epel-release-6-5.noarch.rpm
# rpm -Uvh http://mirrors.dotsrc.org/jpackage/6.0/generic/free/RPMS/jpackage-utils-5.0.0-7.jpp6.noarch.rpm
# #yum -y install java
# yum -y install tomcat6 tomcat6-webapps tomcat6-admin-webapps

vi /etc/tomcat6/tomcat-users.xml
#add a user with roles of admin,manager
chkconfig tomcat6 on
# now to geoserver
cd /home/<user>
wget http://downloads.sourceforge.net/geoserver/geoserver-2.1.4-war.zip
unzip geoserver-2.1.4-war.zip
mv geoserver.war /usr/share/tomcat6/webapps
# start up tomcat6
service tomcat6 startexit
# test geoserver at
http://localhost:8080/geoserver/web/

# and you'll probably want access across your network
iptables -A INPUT -p tcp --dport 8080 -j ACCEPT
/sbin/service iptables save
iptables -F
cleanaway 62601472

# note used the same iptables as those above
# also did service tomcat6 start and 
reboot
login as admin, geoserver



#+end_src
*** TODO configure geoserver
# DEADLINK? http://www.gistutor.com/geoserver/21-intermediate-geoserver-tutorials/38-configuring-geoserver-proxy-for-public-and-remote-data-access.html
NB is the default memory setting too low?  Ran out of memory pretty quick.
*** expose spatial data
# publish
http://docs.geoserver.org/stable/en/user/gettingstarted/postgis-quickstart/index.html

** DDIindex
The DDIindex is a java based catalogue that was written by the Australian Data Archives at the ANU (Formerly the ASSDA http://assda.anu.edu.au/ddiindex.html). The original application source is still available from the ASSDA site but I am using the one I have archived because it has some configurations that I don't remember who to make again at the moment.
*** TOMCAT
http://coastalrocket.blogspot.com.au/2012/08/installing-geostack-on-centos-6-64bit.html
#+name:install-tomcat
#+begin_src R :session *shell* :tangle no :eval no
###########################################################################
# newnode: install-tomcat
yum -y install tomcat6 tomcat6-webapps
# not tomcat6-admin-webapps 
vi /etc/tomcat6/tomcat-users.xml
#add a user with roles of admin,manager like this
<user name="tomcat" password="password" roles="admin,manager" />
#+end_src

#+name:for-restarts
#+begin_src sh :session *shell* :tangle no  :eval no
###########################################################################
# newnode: for-restarts
chkconfig tomcat6 on
#+end_src

changing the Tomcat Port
- Locate server.xml in {Tomcat installation folder}\ conf \
- change <Connector port="8181"/> from 8080
- and connector executor too?  only if it is not commented out (was on centos, not on RHEL)
- change iptables and restart
#+name:restart
#+begin_src R :session *R* :tangle no :eval no
service tomcat6 restart
#+end_src
*** COMMENT DEPRECATED TOMCAT upgrade 6 to 7
I found out that tomcat 6 presents a security risk.  here is the testing upgrade to tomcat7

service tomcat6 stop
chkconfig tomcat6 off
# following instructions from http://www.distrotips.com/centos/installing-tomcat-7-on-centos-6.html
# and http://www.davidghedini.com/pg/entry/install_tomcat_7_on_centos
#404 not found, visit website wget http://mirrors.gigenet.com/apache/tomcat/tomcat-7/v7.0.22/bin/apache-tomcat-7.0.22.tar.gz
#wget http://mirror.ventraip.net.au/apache/tomcat/tomcat-7/v7.0.37/bin/apache-tomcat-7.0.37-fulldocs.tar.gz
wget http://mirror.ventraip.net.au/apache/tomcat/tomcat-7/v7.0.37/bin/apache-tomcat-7.0.37.tar.gz
#md5sum apache-tomcat-7.0.37-fulldocs.tar.gz
md5sum apache-tomcat-7.0.37.tar.gz
#Compare the output above to the MD5 Checksum provided next to the download link and you used above and check that it matches
#cc52c0e99cac0fe4ee7135292afd4fb1  apache-tomcat-7.0.37-fulldocs.tar.gz
#cc52c0e99cac0fe4ee7135292afd4fb1 *apache-tomcat-7.0.37-fulldocs.tar.gz
#1f65e0806cc2a3fc7a93017ef2252b76  apache-tomcat-7.0.37.tar.gz
#1f65e0806cc2a3fc7a93017ef2252b76 *apache-tomcat-7.0.37.tar.gz
#mv apache-tomcat-7.0.37-fulldocs.tar.gz  /usr/share/
mv apache-tomcat-7.0.37.tar.gz  /usr/share/
#tar -xzf apache-tomcat-7.0.37-fulldocs.tar.gz
tar -xzf apache-tomcat-7.0.37.tar.gz
#To start Tomcat, go to the bin folder of Tomcat installation and then run the startup.sh script,
cd /usr/share/apache-tomcat-7.0.37
vi conf/server.xml
# change <Connector port="8181"/> from 8080
./bin/startup.sh
# [root@i-00002264 apache-tomcat-7.0.37]# ./bin/startup.sh
# Using CATALINA_BASE:   /usr/share/apache-tomcat-7.0.37
# Using CATALINA_HOME:   /usr/share/apache-tomcat-7.0.37
# Using CATALINA_TMPDIR: /usr/share/apache-tomcat-7.0.37/temp
# Using JRE_HOME:        /usr
# Using CLASSPATH:       /usr/share/apache-tomcat-7.0.37/bin/bootstrap.jar:/usr/share/apache-tomcat-7.0.37/bin/tomcat-juli.jar

./bin/shutdown.sh

# make a script for starting
cd /etc/init.d  
vi tomcat
# add this  
# to find java_home "update-alternatives --display java" and remove bin/java
#!/bin/bash  
# description: Tomcat Start Stop Restart  
# processname: tomcat  
# chkconfig: 234 20 80  
JAVA_HOME=/usr/lib/jvm/jre-1.6.0-openjdk.x86_64
export JAVA_HOME  
PATH=$JAVA_HOME/bin:$PATH  
export PATH  
CATALINA_HOME=/usr/share/apache-tomcat-7.0.37  
  
case $1 in  
start)  
sh $CATALINA_HOME/bin/startup.sh  
;;   
stop)     
sh $CATALINA_HOME/bin/shutdown.sh  
;;   
restart)  
sh $CATALINA_HOME/bin/shutdown.sh  
sh $CATALINA_HOME/bin/startup.sh  
;;   
esac      
exit 0  
# now
chmod 755 tomcat  
chkconfig --add tomcat  
chkconfig --level 234 tomcat on  
# verify
chkconfig --list tomcat  
service tomcat start  
# test stop, start, restart
more /usr/share/apache-tomcat-7.0.37/logs/catalina.out  
# to check for errors, but is all greek to me
# so assuming ddiindex was previously running on tomcat6
service tomcat stop
cd /usr/share/tomcat6/webapps
cp -r ddiindex /usr/share/apache-tomcat-7.0.37/webapps
service tomcat start
*** COMMENT Unsuccessfully did 7, try latest 6
#yum -y install tomcat6 tomcat6-webapps
# service tomcat6 stop
#chkconfig tomcat6 off

chkconfig tomcat off
service tomcat stop

wget http://mirror.overthewire.com.au/pub/apache/tomcat/tomcat-6/v6.0.36/bin/apache-tomcat-6.0.36.tar.gz
mv apache-tomcat-6.0.36.tar.gz  /usr/share/
cd /usr/share
tar -xzf apache-tomcat-6.0.36.tar.gz
#To start Tomcat, go to the bin folder of Tomcat installation and then run the startup.sh script,
cd /usr/share/apache-tomcat-6.0.36
vi conf/server.xml
# change <Connector port="8181"/> from 8080
vi conf/tomcat-users.xml
#add a user with roles of admin,manager like this
<user name="tomcat" password="password" roles="admin,manager" />
cd /usr/share/tomcat6/webapps
cp -r ddiindex /usr/share/apache-tomcat-6.0.36/webapps
cd /usr/share/apache-tomcat-6.0.36
./bin/startup.sh
./bin/shutdown.sh

# edit
cd /etc/init.d  
vi tomcat
chkconfig --add tomcat  
chkconfig --level 234 tomcat on  
# verify
chkconfig --list tomcat  
service tomcat start  


*** UPLOAD THE DDIINDEX
*** COMMENT TODO add explanation of the ddiindex.zip file in lib
#+name:upload-ddiindex-source
#+begin_src sh :session *shell* :tangle no :eval no
cd ~/Dropbox/projects/0.3\ Catalogue/versions/ddiindex/ddiindex/code
scp ddiindex_2010-04-27-1259.tar.gz root@ip.address.of.server:/usr/share/tomcat6/webapps

cd ~/Dropbox/projects/0.3\ Catalogue/versions/ddiindex/ddiindex/xmldata
scp xmldata_2010-04-27-1259.tar.gz root@ip.address.of.server:/

cd ~/Dropbox/projects/0.3\ Catalogue/versions/ddiindex/ddiindex/opt
scp ddiindex_2010-04-27-1259.tar.gz root@ip.address.of.server:/opt

cd /home/ivan/Dropbox/projects/0.3\ Catalogue/versions/ddiindex/ddiindex/mysql
scp security_2010-04-27-1259.sql root@ip.address.of.server:/

# on remote server
ssh root@ip.address.of.server
cd /usr/share/tomcat6/webapps/
tar -zxvf ddiindex_2010-04-27-1259.tar.gz
cd /
tar -zxvf xmldata_2010-04-27-1259.tar.gz
cd /opt
tar -zxvf ddiindex_2010-04-27-1259.tar.gz

service tomcat6 restart
#+end_src
*** MySQL
http://www.if-not-true-then-false.com/2010/install-mysql-on-fedora-centos-red-hat-rhel/
#+name:mysql
#+begin_src R :session *R* :tangle no  :eval no
rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
yum --enablerepo=remi,remi-test list mysql mysql-server
yum --enablerepo=remi,remi-test install mysql mysql-server
/etc/init.d/mysqld start ## use restart after update
## OR ##
service mysqld start ## use restart after update

chkconfig --levels 235 mysqld on

mysqladmin -u root password [your_password_here]
mysql -h localhost -u root -p
then create database, 
database user and
database password 
## CREATE DATABASE ##
mysql> CREATE DATABASE security;
 
## CREATE USER ##
mysql> CREATE USER 'ddiindex'@'localhost' IDENTIFIED BY 'BlahBlahBlah';
 
## GRANT PERMISSIONS ##
mysql> GRANT ALL ON security.* TO 'ddiindex'@'localhost';
 
##  FLUSH PRIVILEGES, Tell the server TO reload the GRANT TABLES  ##
mysql> FLUSH PRIVILEGES;

# nd then use next at command line:

mysql -h localhost -u root -p security < security_2010-04-27-1259.sql; 
# Enter password: 
# deprecated let thru iptables port 3306?
#+end_src
*** make sure ddiindex can connect to mysql as ddiindex user
#+name:modify-dburl
#+begin_src sh :session *shell* :tangle no  :eval no
# now go to /usr/share/tomcat/webapps/ddiindex/WEB-INF/classes/index-conf.xml
# dburl change to 
    <dbUrl>jdbc:mysql://localhost:3306/security</dbUrl>
    <dbUserName>ddiindex</dbUserName>
    <dbPassword>BlahBlahBlah</dbPassword>
# other personalisations here such as <dataServer>
mkdir /opt/ddiindex/ewedb
mkdir /opt/ddiindex/ewedb_s

#+end_src
from home.jsp removed
<%@taglib uri="http://assda.anu.edu.au/ddiindex" prefix="ddiindex"%>
and login.jsp
<%@taglib uri="http://assda.anu.edu.au/ddiindex" prefix="ddiindex"%>
and search.jsp
<%@taglib uri="http://assda.anu.edu.au/ddiindex" prefix="ddiindex"%>

*** TODO Check the security implications of allowing write permissions here
I found I need to allow write access to the  xmldata folder for uploads of the xml metadata files.
#+name:write-permission
#+begin_src sh :session *shell* :tangle no  :eval no
adduser ddiindex
chown -R ddiindex /xmldata
chmod 0777 /xmldata
# bad for security, try 0750 instead
# chmod 0777 /opt/ddiindex/nceph
# or this works
chmod -R 777 /opt/ddiindex/nceph
chmod 0777 /opt/ddiindex/nceph_s
chmod -R 777 /opt/ddiindex/ewedb
chmod 0777 /opt/ddiindex/ewedb_s
#+end_src
**** COMMENT deprecated? permissions to try to fix lock out on nectar cloud
# trying stuff because after snapshot got read-only errors, cant start mysql daemon
chown -R mysql:mysql /var/lib/mysql
chmod 0777  /var/lib/mysql
chmod 0777  /var/log
chmod 0777  /usr/share/tomcat6/webapps/ddiindex/images
# but then I did a reboot and then could start it

*** Running the Indexer
Log on as an admin user (set in the MySQL userEJB table) and you can run the indexer.jsp
Before doing this the first time I did the following:
- went to nceph and nceph_s deleted the recent created files
- went to xmldata, deleted all
- restart TOMCAT
- go to indexer, upload xml
- hit the 'add to index' button.
*** personalise the ddiindex
ie home.jsp
   <div class="intro">The Graduate Information Literacy Program at ANU now offers resources for researchers and graduate students on data management planning: a comprehensive Manual and a Data Management Planning Template. <a href="http://ilp.anu.edu.au/dm/" target="_blank">http://ilp.anu.edu.au/dm</a></div>

** Set up a private git lab for data and code
Git needs to be installed first.
*** Firewall
*** SSH server
*** Encrypted files
** COMMENT ecryptfs-user-code
#+name:ecryptfs-user
#+begin_src sh :tangle no :eval no
apt-get install ecryptfs-utils
sudo adduser --encrypt-home username 
# Change "username" for the user name you want.
# it should probably be 'git'
exit
# log in as new user
ssh username@server
# ************************************************************************
# YOU SHOULD RECORD YOUR MOUNT PASSPHRASE AND STORE IT IN A SAFE LOCATION.
#   ecryptfs-unwrap-passphrase ~/.ecryptfs/wrapped-passphrase
# THIS WILL BE REQUIRED IF YOU NEED TO RECOVER YOUR DATA AT A LATER TIME.
# ************************************************************************

ecryptfs-unwrap-passphrase  ~/.ecryptfs/wrapped-passphrase
# record this in your encrypted passwords file
#+end_src
*** SSH keys
#+name:gitlab-sshkeys
#+begin_src sh :tangle no :eval no
#checked to see if I had a current public SSH key:
ls ~/.ssh/id_rsa.pub
  
#If you don’t have one, you can create a new public/private key pair by running ssh-keygen and following the prompts.
  
#Next I copied my public key to the root user’s home directory on the server, like this:
  
scp ~/.ssh/id_rsa.pub root@server:

#  When prompted, I entered git’s password (the one I set up earlier). Then I connected to the server over SSH, again using the git account:
  
ssh root@server
  
#  Again, I entered the git user’s password when prompted.
  
#  Once logged in  I copied my public SSH key into the list of authorised keys for that user.
# become root
sudo su
mkdir -p /etc/ssh/<username>
chown username /etc/ssh/username
# log in as username
#  Then I locked down the permissions on .ssh and its contents, since the SSH server is fussy about this.
cat id_rsa.pub >> /etc/ssh/username/authorized_keys  
chmod 755 /etc/ssh/username
chmod 644 /etc/ssh/username/authorized_keys

#Then edit your /etc/ssh/sshd_config and add:

#AuthorizedKeysFile    /etc/ssh/%u/authorized_keys
#Finally, restart ssh with:

service ssh restart
#The next time you connect with SSH you should not have to enter your password.

#  That’s it, passwordless-SSH is now set up. If you’re following along at home you can try it for yourself: log out of the server then log back in:
  
ssh username@[your server name]
  
#+end_src
*** COMMENT NOTE THAT THIS BROKE MY NX SERVER KEY
SO WHEN I WANT TO USE THE NO MACHINE DESKTOP I HAVE TO SSH IN 
ssh user@server
sudo su
vi /etc/ssh/sshd_config
# comment/uncomment AuthorizedKeysFile
service ssh restart

*** Set up Git repositories

#+name:gitstore
#+begin_src sh :tangle no :eval no
#  Logging in to the server once again as git, I created a directory called simon in git’s home directory.
  
ssh gituser@server
mkdir ResearchData
  
# created a directory to hold the repository. I followed Github’s convention and named the directory as [project name].git:

mkdir ResearchData/myproject.git
  
#  Finally,  initialised a bare Git repository in that directory:
  
cd ResearchData/myproject.git
git init --bare
  
#  Note the --bare option to git init: that means that rather than
#  creating all Git’s config in a .git subdirectory it’ll be created
#  directly in myproject.git, which is what we want.  Trying it out
  
#  All that was left was to push my repository to my new git server. 
# go to R studio and create a project from git, url is = gituser@server:ResearchData/myproject.git
# you might want to clone the project into a local truecrypt volume or use a local user with encryped home directory

# or run   
cd /path/to/git/repo
git remote add origin gituser@server:ResearchData/myproject.git
git push origin master

#+end_src

*** COMMENT working notes                             
**** TODO secure encrypted disk partition
The exhaustive approach:
http://www.rationallyparanoid.com/articles/ubuntu-12-lts-security.html
http://www.thefanclub.co.za/how-to/how-secure-ubuntu-1204-lts-server-part-1-basics
http://www.geoffke.be/nieuws/12/
sudo apt-get install ufw
sudo ufw enable
ufw allow from 192.168.1.0/24 to any app OpenSSH

Or a more minimal approach:
http://askubuntu.com/questions/132395/creating-a-new-user-after-install-with-encrypted-home
http://askubuntu.com/a/144866
might need this on remote Ubuntu
sudo apt-get install openssh-server

sudo adduser --encrypt-home username 
(Change "username" for the user name you want.)

Just worked for me in an up to date 12.04 Precise Pangolin ie.it created a new user with an encrypted home directory.

If you receive an error message :

adduser: unable to find a program named « ecryptfs-setup-private » in $PATH"...
Adding the package « ecryptfs-utils » solves this problem (thanks, apt-file...) :

apt-get install ecryptfs-utils

************************************************************************
YOU SHOULD RECORD YOUR MOUNT PASSPHRASE AND STORE IT IN A SAFE LOCATION.
  ecryptfs-unwrap-passphrase ~/.ecryptfs/wrapped-passphrase
THIS WILL BE REQUIRED IF YOU NEED TO RECOVER YOUR DATA AT A LATER TIME.
************************************************************************

ecryptfs-unwrap-passphrase


**** set up git user
***** background
#+name:Private Git-hub
#+begin_src sh :session *shell* :tangle no  :eval no
  http://blog.goosoftware.co.uk/2012/02/07/quick-git-server/
  
  Running a Simple Git Server Using SSH
  
  Feb 7th, 2012
  
  I had the need recently to set up a temporary git remote on a Mac mini on my local network. It turned out to be both easy and useful, so I thought I’d document how I did it.
  
  Throughout this article I’ll refer to the Mac mini where I set up the remote repository as the server, and my Macbook Pro (my primary work computer) as my laptop.
  What I wanted
  
  I use Github a lot, so I’m used to representing my remotes in the SSH style, like this:
  
  git@github.com:simonwhitaker/PyAPNs.git
  
  Then I’ll clone that remote by running:
  
  git clone git@github.com:simonwhitaker/PyAPNs.git
  
  and away I go. So, the question was: how do I create a remote on my own device and access it in the same way?
  Unpicking the SSH notation
  
  It’s helpful to first understand what the SSH notation actually means. It’s really simple. The basic format is:
  
  [username]@[hostname]:[path to repository]
  
  That’s it. So that Github connect string from earlier means that I’m connecting to the server at github.com as the git user, and cloning the repository that exists at simonwhitaker/PyAPNs.git within the git user’s home directory.
  
  Hang on though – I don’t have a password for the git user on github.com, so how come I can read and write stuff in their home directory? It’s because I’ve set up passwordless SSH by uploading my SSH public key to Github. When I connect via SSH, the SSH server at github.com looks to see if the private SSH key on my computer matches a public key on github.com. If it does, I’m allowed in.
  
  So, here’s what I had to do to set up a git server on my Mac mini:
  
      Create a new user called git
      Set up passwordless SSH so that I can connect to my git user’s account without a password
      Create a directory in git’s home directory for storing my repositories
      For each repository I want to host: log in to git’s account on my Mac mini and initialise a new Git repository.
#+end_src  
*****  Step 1: Create the git user
#+begin_src sh :session *shell* :tangle no  :eval no  
  The steps on how to do this will vary depending on your operating system. The git user doesn’t need (indeed, shouldn’t have) administrator or sudo privileges; a plain old user account will do fine.
  
#sudo adduser git
# use the encrypted home directory eg above

  For the sake of simplicity, on the server (running OS X, remember) I just opened System Preferences > Accounts and added a new user, setting their username to git and giving them a suitably strong password.
  
  (That’s not a perfect solution: it sets git up as a regular user so e.g. they appear in the login screen, but it works fine for me. If you’re using OS X and you want to create a user who doesn’t appear in the login screen, see this Super User answer for details.)

  Step 2: Set up passwordless SSH
  
  On my laptop I opened a terminal window and checked to see if I had a current public SSH key:
  
  $ ls ~/.ssh/id_rsa.pub
  
  I did. (If you don’t have one, you can create a new public/private key pair by running ssh-keygen and following the prompts.)
  
  Next I copied my public key to the git user’s home directory on the server, like this:
  
  $ scp ~/.ssh/id_rsa.pub git@Goo-mini.local:
  
  When prompted, I entered git’s password (the one I set up earlier). Then I connected to the server over SSH, again using the git account:
  
  $ ssh git@Goo-mini.local
  
  Again, I entered the git user’s password when prompted.
  
  Once logged in as git, I copied my public SSH key into the list of authorised keys for that user.
  
  $ mkdir -p .ssh
  $ cat id_rsa.pub >> .ssh/authorized_keys
# alternative if encrypted home
# https://help.ubuntu.com/community/SSH/OpenSSH/Keys
# log in as sudoer
sudo mkdir -p /etc/ssh/<username>
sudo chown username /etc/ssh/username
# log in as username

  
  Then I locked down the permissions on .ssh and its contents, since the SSH server is fussy about this.
#  cat id_rsa.pub >> .ssh/authorized_keys
#  $ chmod 700 .ssh
#  $ chmod 400 .ssh/authorized_keys
cat id_rsa.pub >> /etc/ssh/safe/authorized_keys  
chmod 755 /etc/ssh/safe
chmod 644 /etc/ssh/safe/authorized_keys

Then edit your /etc/ssh/sshd_config and add:

AuthorizedKeysFile    /etc/ssh/%u/authorized_keys
Finally, restart ssh with:

sudo service ssh restart
The next time you connect with SSH you should not have to enter your password.

  That’s it, passwordless-SSH is now set up. If you’re following along at home you can try it for yourself: log out of the server then log back in:
  
  $ ssh git@[your server name]
  
  You should be logged straight in without being prompted for a password. If you’re not, something’s gone wrong – check through the instructions so far and make sure you didn’t miss something.
  Step 3: Create a directory in git’s home directory for storing my repositories
  
  Logging in to the server once again as git, I created a directory called simon in git’s home directory.
  
  $ ssh git@Goo-mini.local
  $ mkdir simon
  
  Simple.
  Step 4: For each repository I want to host, log in to git’s account on my Mac mini and initialise a new Git repository.
  
  This one was pretty simple, too. First I created a directory to hold the repository. I followed Github’s convention and named the directory as [project name].git:
  
  $ ssh git@Goo-mini.local
  $ mkdir simon/myproject.git
  
  Finally, I initialised a bare Git repository in that directory:
  
  $ cd simon/myproject.git
  $ git init --bare
  
  Note the --bare option to git init: that means that rather than creating all Git’s config in a .git subdirectory it’ll be created directly in myproject.git, which is what we want.
  Trying it out
  
  All that was left was to push my repository to my new git server. On my laptop I ran the following:
  
  $ cd /path/to/git/repo
  $ git remote add origin git@Goo-mini.local:simon/myproject.git
  $ git push origin master
  Counting objects: 3, done.
  Writing objects: 100% (3/3), 232 bytes, done.
  Total 3 (delta 0), reused 0 (delta 0)
  To git@Goo-mini.local:simon/myproject.git
   * [new branch]      master -> master
  
  Cooooool!
  Next steps
  
  There are loads of ways you could improve this process. For example, any user who uploads their public SSH key would have access to all repositories in git’s home directory. You might want to change that, for example so that users can only write to their own repositories.
  
#+end_src

    
** True-Crypt encrypted volumes
The servers are secure but we will sometimes want local copies and so should use truecrypt or another option.  I am using Ubuntu as my desktop so here is the instructions.
#+name:True-Crypt Encrypted Volumes
#+begin_src R :session *R* :tangle no :eval no
################################################################
#http://www.liberiangeek.net/2012/07/install-truecrypt-via-ppa-in-ubuntu-12-04-precise-pangolin/
sudo add-apt-repository ppa:michael-astrapi/ppa
sudo apt-get update && sudo apt-get install truecrypt

#### FAILED TO INSTALL ON REDHAT ####
# or http://ubuntupop.blogspot.com.au/2012/10/how-to-install-truecrypt-on-centos-63.html
# 1. Download Truecrypt package. Or use wget instead
# wget http://www.truecrypt.org/download/truecrypt-7.1a-linux-x64.tar.gz
# 2.  Now extract the package
# tar xzvf truecrypt-7.1a-linux-x64.tar.gz
# 3. It will produce a file called truecrypt-7.1a-setup-x64
# chmod +x truecrypt-7.1a-setup-x64 
# ./truecrypt-7.1a-setup-x64
# installer ran but then 
# truecrypt -h 
# truecrypt: error while loading shared libraries: libfuse.so.2: cannot open shared object file: No such file or directory
# yum install libfuse.so.2 didn't help

# DIDN'T TRY  http://vinnn.blogspot.com.au/2009/01/tryecrypt-6-on-centos-5.html
#+end_src

** TODO ResearchData storage
#+name:ResearchData
#+begin_src R :session *R* :tangle no :eval no
################################################################
# name:ResearchData
cd /home
mkdir ResearchData
chmod 0777 /home/ResearchData
# TODO check the security of this
#+end_src


* Procedures for add users to the system.
** Description of the access procedure
*** Getting Access
The "Getting Access" procedure to help users apply for and gain access to mortality data is shown in Figure 1, and a more detailed description of the process is shown in the Appendix, Figure 4. The process is a set of formally defined steps that are designed to move the User through two general stages:
- Requesting data: guiding the researcher through the process of (a) gaining Ethics Approval from a Human Research Ethics Committee, and (b) Project Level Approval from the Registrar of Births, Deaths and Marriages.
- Providing data: provision of a confidentialised (often aggregated) dataset in an appropriately secured manner such as access to remote secure servers or encrypted archives accessed on local disk media, with security determined by (a) the nature of the data and (b) any project management related criteria.
**** COMMENT src
#+begin_src R :session *R* :tangle no :exports none :eval no
  ###########################################################################
  # Getting access
  
  nodes <- newnode(name='Browse Catalogue',
                   inputs = 'Search for Data',
                   outputs = 'Request Access',
                   newgraph = T
                   )
  ## NEEDS ETHICS COMMITTEE PROCESS HERE
  
  nodes <- newnode(name= 'Get Ethics Committee Approval',
                  inputs='Request Access',
                   outputs = 'Ethics Committee Approves Project')
  
  nodes <- newnode(name= 'Add Study Description in ANU-User-DB',
                  inputs= 'Ethics Committee Approves Project'
                   )
  
  nodes <- newnode(name = 'Get BDM Committee Approval',
                   inputs = 'Add Study Description in ANU-User-DB'
                   )
  ## INSERT BDM APPROVAL PROCESS HERE
  
  nodes <- newnode(name='Approve Access',
                   inputs = 'Get BDM Committee Approval'
  
                   )
  
  nodes <- newnode(name='Deny Access',
                   inputs = 'Get BDM Committee Approval'
  
  )
  
  
  
  ###########################################################################
  # Provide data
  # nodes <- newnode(name='Add to Study Description in ANU-User-DB',
  #                  inputs='Request Access',
  #                  outputs= 'Review Application',
  #
  #                  )
  
  # notify approval
  
  nodes <- newnode(name='Notify User of Approval',
                   inputs='Approve Access',
                   outputs='Add Access Record in ANU-User-DB',
                   )
  
  # or record why not
  
  nodes <- newnode(name='Notify User of Non-approval',
                   inputs='Deny Access',
                   outputs='Note Reason in Study Description in ANU-User-DB',
                   )
  
  
  
  nodes <- newnode(name='Give access to Restricted Server', newgraph = F,
                   inputs = 'Add Access Record in ANU-User-DB'
                   )
  
  
  nodes <- newnode(name='Extract to Restricted Server', newgraph = F,
                   inputs = 'Give access to Restricted Server'
                   )
  
  nodes <- newnode(name= 'Store data extract in appropriate location', newgraph = F,
                   inputs = c('Extract to Restricted Server'),
                   outputs = c('Low Risk Data')
                   )
  
  nodes <- newnode(name = 'CSV',
                   inputs = 'Low Risk Data')
  
  nodes <- newnode(name = 'High Risk Data', outputs =
                   c('Database schema', 'Rstudio user workspace'),
                   inputs = 'Store data extract in appropriate location'
                   )
  
  nodes <- newnode(name= 'Add File Record to ANU-User-DB', newgraph = F,
                   inputs = c('CSV', 'Database schema', 'Rstudio user workspace'),
  
  
                   outputs = c('Notify User of Access')
  )
  
  nodes <- newnode(name = 'Modify file and access records in ANU-User-DB',
                   inputs = 'Notify User of Access')
  
#+end_src  
**** COMMENT add colour
#+name:add-colour
#+begin_src R :session *R* :tangle no :exports none :eval no
  ###########################################################################
  # newnode: test-colour
  attrs <- list(node=list(shape="ellipse", fixedsize=FALSE))
  plot(nodes, attrs = attrs)
  nNodes <- length(nodes(nodes))
  nA <- list()
  nA$fillcolor <- rep('grey', nNodes)
  nA$shape <- rep("ellipse", nNodes)
  nA <- lapply(nA, function(x) { names(x) <- nodes(nodes); x})
  #nA
  #plot(nodes, nodeAttrs=nA, attrs = attrs)
  nodes(nodes)
  # USER
  nA$fillcolor[nodes(nodes)[1:4]] <- '#FB8072' #'#8DD3C7'
  # USER ADMIN
  nA$fillcolor[nodes(nodes)[c(6:7,10:13, 22:24)]] <- '#FFFFB3'
  # DATA ADMIN
  nA$fillcolor[nodes(nodes)[c(14:16, 18, 20, 21)]] <- '#BEBADA'
  # DECISIONS
  dec <- c(5,8:9, 17,19)
  nA$fillcolor[nodes(nodes)[dec]] <- 'white' 
  nA$shape[nodes(nodes)[dec]] <- 'box'
  
  plot(nodes, nodeAttrs=nA, attrs = attrs)
  legend('topleft', legend = c('User','User Admin', 'Data Admin','Decision'),
         pch = c(21,21,21,22), pt.cex = 1.5,
         pt.bg = c('#FB8072', '#FFFFB3', '#BEBADA', decisionCol)
         )
  
#+end_src

**** COMMENT plot nodes
#+begin_src R :session *R* :tangle no  :exports none :eval no   
    
  dev.copy2pdf(file='DataAccessFlowDiagram-GettingAccess.pdf')
  dev.off()
    
#+end_src


*** Managing Access
Procedures for managing access are shown in Figure 2 (and in the Appendix, Figure 5). These activities are intended to maintain information on the current state of projects using the mortality data, and to report any changes in situation to the State Registries. The User Administrator is responsible for conduct of the "Managing Access" procedures.

The process is initiated by running a query on the ANU-User-DB to make a list of all Projects and Users, and then each Project is sent a reminder to report any changes in Project Status (sent annually to coincide with a similar reminder sent by the ANU Human Research Ethics Committee). The purpose of these reminders is to ensure that Project management plans continue to consider data security as a primary concern, even during long multi-year projects where many project management and staffing issues inevitably arise.

Once a response is received, the User Administrator then enters the relevant information into the ANU-User-DB, and if the project has been concluded will then initiate the final "Ending Access" process.
*** Ending Access
The procedure for ending access aims to ensure that data are both securely and sustainably stored.  It is very important that files used for authorised projects are never re-used in un-authorised projects, but that future researchers may have the opportunity to create an authorised project and potentially replicate historical analyses.  This is an important part of reproducible research and the robust practice of scientific enquiry.

** The process user administrators go through to set up users
When adding users go to the oraphi and track their permissions
*** lodge request in user db
- new study into nceph ddiindexdb 
- new file with filelocation = brawns schema, working directory
- access requestor, accessors add accessor, date to end
- finalise approval
*** r-nceph
production goes thru david fisher
testbed done by me
** brains
*** linux user
#+name:add-linux-user
#+begin_src R :session *R* :tangle no  :eval no
adduser userxxx
passwd userxxx 
# (I use R to randomly generate passwords by mixing words, letters,
# numbers and symbols) 
#+end_src
*** mysql (check ddiindex)
#+name:add-linux-user
#+begin_src R :session *R* :tangle no  :eval no
mysql -u root -p security
select * from UserEJB;
insert into UserEJB (id, password, admin) values ('userxxx', 'passwd', 0);
#+end_src
******* oracle (oraphi)
- log in as admin user to ddiindexdb workspace and go to administration, 
- on the right there is a tasks pane with create user
******* github
- downstream users can just download zips from github
- collaborators need to sign up to github
- will create a walk through for those 
** brawn
*** TODO postgres
Need to find out if this 0.0.0.0/0 is a big no-no?
#+name:psql
#+begin_src R :session *R* :tangle no :eval no
  # first edit your pg_hba.conf file
  # under /home/pgsql/9.2/data/pg_hba.conf I added a super user from my ip
  # address and allowed all the www ip addresses access
  host    all             postgres        my.desk.ip.address/32       md5
  host    all             gislibrary      0.0.0.0/0                   md5
  # and save.  now
  su - postgres
  psql postgres postgres
  CREATE ROLE userxxx LOGIN PASSWORD 'xxxxx';
  GRANT gislibrary TO userxxx;
  CREATE SCHEMA userxxx;
  grant ALL on schema userxxx to userxxx;
  # might want to set a date for expiry?  now 
  select pg_reload_conf();
  # or if not connected just service postgresql-9.2 restart
#+end_src
*** geoserver
- general users don't need log in
- admin users can log on as admin?
*** alliance wiki
https://alliance.anu.edu.au/welcome/ 
uses their anu password
*** text message the set password
This solution is adequate.  Might want to invest in a sacrificial SIM card to avoid having too many phone calls?
** edits to oraphi via sql 
#+name:to-add-details
#+begin_src sh :session *shell* :tangle no  :eval no
  update accessors 
  set othermat = 'phone number'
  where accessornames = 'username';
#+end_src

*** revocation
#+name:revoke
#+begin_src R :session *R* :tangle no :exports reports :eval no
userdel -r ivan_hanigan
#+end_src

    
* Backups
** General
*** maintenance
*** nearline for potential restore
*** archive and remove

** General concerns
** Brains
*** Backup oraphi
Needs to be done from Brains as it needs the oracle client set up.
#+name:backupOraphi-brains
#+begin_src R :session *shell* :tangle no :exports reports :eval no
    ###########################################################################
    # newnode: backupOraphi
    # using an emacs ssh buffer log on to target server, start R, start ESS-remote
    # do this as root and put into /backups
    if(!require(RJDBC)) install.packages('RJDBC'); require(RJDBC)
    drv <- JDBC("oracle.jdbc.driver.OracleDriver",
                '/u01/app/oracle/product/11.2.0/xe/jdbc/lib/ojdbc6.jar')
    ch <- dbConnect(drv,"jdbc:oracle:thin:@xx.yy.zz.ww:1521","DDIINDEXDB","password")
    dir()
    dir.create(paste('ddiindexdb-backups/csvs/',Sys.Date(),sep=''),recursive=T)
  
    for( tbl in c('STDYDSCR','DATADSCR','FILEDSCR','KEYWORDS','ACCESSORS','ACCESSSTDY','OTHRSTDYMAT')){
    #tbl='STDYDSCR'
    dataout <- dbReadTable(ch, tbl)
    # sqlFetch(ch,tbl,as.is=T)
    write.table(dataout,paste('ddiindexdb-backups/csvs/',Sys.Date(),'/',tbl,'.csv',sep=''),na = "",row.names=F,sep=',',quote=T)
    }
  # download these to a backup location for storage.
  
#+end_src

** Brawn
*** Find out how big is it?
AKA brawn-dbsize
#+name:brawn-dbsize-header
#+begin_src markdown :tangle brawn-dbsize.md :exports none :eval no :padline no
  ---
  name: brawn-dbsize
  layout: default
  title: Find out how big is it?
  ---
  
    <li><a href="#sec-7-3-1">Previous: TODO Backup oraphi</a></li>
    <li><a href="/backup-brawn-filesystem.html">Next: Dump and download it to a secure computer</a></li>
  
  <p></p>
  
  This is a useful query to have on a postgres database cluster to keep track of the size of all the databases on the server.
  
  #### Code:brawn-dbsize   
      CREATE OR REPLACE VIEW dbsize AS 
      SELECT pg_database.datname, 
         pg_size_pretty(pg_database_size(pg_database.datname)) AS size
      FROM pg_database;
       
      ALTER TABLE dbsize OWNER TO postgres;
      GRANT ALL ON TABLE dbsize TO postgres;
      GRANT ALL ON TABLE dbsize TO public_group;
  
  
#+end_src


*** Dump and download it to a secure computer 
**** file system backup

**** backup-brawn-filesystem header
#+name:backup-brawn-filesystem-header
#+begin_src markdown :tangle backup-brawn-filesystem.md :exports none :eval no :padline no
---
name: backup-brawn-filesystem
layout: default
title: backup-brawn-filesystem
---

  <li><a href="/brawn-dbsize.html">Previous: Find out how big is it?</a></li>
  <li><a href="/brawn-dump-restore.html">Next: Restore databse dump into a new database on another machine.</a></li>

<p></p>

TODO look at [this link](http://www.cyberciti.biz/faq/howto-use-tar-command-through-network-over-ssh-session/)

#### Code:backup-brawn-filesystem
    mkdir /home/backup
    cd /home/backup     
    tar -cf ewedb-backup-20130411.tar /home/pgsql/9.2/data
    # or if not enough room to store it locally
    # on destination initdb
    # then on source server
    tar czvf - /var/lib/pgsql/9.2/data | ssh user@host "tar xvzf - -C /home/pgsql/9.2/data"
    # on destination
    service postgresql-9.2 start

#+end_src

**** pgdump
Backup the pg_dump to your workplace or another VM on the Research Cloud.


First do this to setup the backup directory
#+name:setup
#+begin_src R :session *shell* :tangle no :exports reports :eval no
mkdir /home/backup
cd /home/backup
/usr/pgsql-9.2/bin/pg_dump -U postgres ewedb | gzip > ewedb_$(date +"%F-%H%M").gz
# see http://www.postgresql.org/docs/9.2/static/app-pgdump.html
# NB tried -Fc compressed version but took too long to restore
# see http://www.postgresql.org/docs/9.2/static/backup-dump.html
# 24.1.3. Handling Large Databases
#+end_src

Optionally you could set up a scheduled cron job
#+name:cron
#+begin_src R :session *shell* :tangle no :exports reports :eval no
cron -e
# 15 past midnight everyday
15 0 * * * /home/ewedbbackup/backup_script.sh
#+end_src

Then add this script.
*** COMMENT TODO find out more about the -mtime +1 bit.  
if this is not every day is it just +whatever?
Do old files need to be removed using the rm -f?
#+name:backup_script.sh
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
#!/bin/bash
/usr/bin/find /home/backup/* -mtime +1 -type f -exec rm -rf {} \;
cd /home/backup
/usr/pgsql-9.2/bin/pg_dump -U postgres -Fc ewedb > ewedb_$(date +"%F-%H%M").dump
#+end_src
# /usr/pgsql-9.1/bin/pg_dump

Then SCP to your local copy
#+name:download
#+begin_src sh :session *shell* :tangle no :exports reports :eval no
# from your local machine
scp  username@the.remote.server.zz:/backup_yyyymmdd.dump /path/to/localbackup

#+end_src

*** Restore databse dump into a new database on another machine.
#+name:brawn-dump-restore-header
#+begin_src markdown :tangle brawn-dump-restore.md :exports none :eval no :padline no
  ---
  name: brawn-dump-restore
  layout: default
  title: Restore databse dump into a new database on another machine.
  --- 
  
    <li><a href="/backup-brawn-filesystem.html">Previous: Dump and download it to a secure computer</a></li>
    <li><a href="#sec-7-4-5">Next: TODO launch a new Nectar VM from the snapshot image</a></li>
    
    <p></p>
    
  TODO make sure you tune the performance on target machine
  [as per these instrctions](http://stackoverflow.com/a/2095283).
  
  shared_buffers (1024 or 2048) should be set correctly, maintenance_work_mem (uncomment) should be increased during the restore, full_page_writes uncommented and set to should be off during the restore, wal_buffers should be increased (from -1) to 16MB during the restore, checkpoint_segments should be increased (from 3) to something like 16 during the restore, you shouldn't have any unreasonable logging on (like logging every statement executed), auto_vacuum should be (uncommented and ) disabled during the restore.
  
  #### Code:brawn-dump-restore
      su - postgres 
      createdb ewedbbackup
      psql -d ewedbbackup -U postgres  
      CREATE EXTENSION postgis;  
      CREATE EXTENSION postgis_topology;  
      # make sure the users are all there     
      \q      
      # now dump and restore via a pipe. 
      # important to make sure database and users/permissions exist on target server first.
      # also that source server is in iptables and pg_hba.conf on destination
      
      /usr/pgsql-9.2/bin/pg_dump -U postgres mydb  | psql -h destination.ip.address mydb
      # password is postgres on database at destination 
  
#+end_src
*** COMMENT deprecated notes
# gunzip -c ewedb_2013-02-04-1943.gz | psql ewedbbackup
# if used -Fc format: pg_restore -d ewedbbackup ewedb_2013-02-02-1342.dump

*** COMMENT TODO experiment with tuning parameters to improve db and also backup/restore
I think I am at the stage I need to seek advice from a postgres expert on "tuning" database/server performance parameters. 
I've spent most time doing things to postgresql.conf.
For eg while trying to improve performance when creating tables/altering sequences (there over 71,100 of them),  "SELECT * INTO" queries and subsequent dump and restore operations I did the following tweaks (with no real significant improvements in performance):

shared_buffers = 1024MB (also tried 2048)
work mem 4MB
full_page_writes set to off
wal_buffers increased (from -1) to 16MB
checkpoint_segments  increased (from 3) to 16 
auto_vacuum disable (just during pg_restore, then enabled again - currently enabled)
max_locks_per_transaction set to 1000 up from 64, then down again to 100.
 
Also following other advice:
sysctl -w kernel.shmmax=17179869184
sysctl -w kernel.shmall=4194304

and then during dump/restore I used the -Fc format:
pg_restore -d ewedbbackup ewedb_2013-02-02-1342.dump
( I tried first with the "-j 2" option for parallelism but this looked like it failed to do anything except kick off one core at 100 % and no data was getting loaded for 5 mins - so I killed it).
 
The output of top during pg_restore shows we use most of the RAM but the CPU is just going from 1 - 10%
 and looks like taking 5-6 days for a 30GB database to restore (I killed it after 1.5)

Then as I am storing the data on the secondary disk I looked into the hard disk performance stats:
mke2fs -j /dev/vdb
# mke2fs -j  creates ext2 with a journal - which is ext3 effectively.
mount -t ext3 /dev/vdb /home

hdparm -tT /dev/vdb

/dev/vdb:
 Timing cached reads:   6206 MB in  2.00 seconds = 3104.55 MB/sec
 Timing buffered disk reads:   20 MB in  3.37 seconds =   5.94 MB/sec

whereas on a test box I have in my office I am getting
/dev/sda:
 Timing cached reads:   16190 MB in  2.00 seconds = 8102.04 MB/sec
 Timing buffered disk reads: 308 MB in  3.01 seconds = 102.16 MB/sec

which the IT guy here remarked: "Ouch, the disk buffered disk reads are pretty bad."
this guy  also said "your shared buffers size is a bit excessive try dropping that back to around 32MB"...
but google tells me "this parameter should be set at 25% of the system’s RAM" which would indicate 2GB should be fine...

Future ideas to try:
- see if aggregating the 71,000+ wkt_raster grids into a couple of master tables with multiple rasters in it will change things.
- stop extraneous services that are running.
- see if R connections are not being closed properly and causing issues with max connections (dbDisconnect(con))
- investigate file system differences, ext3?  
- seperate out the transaction log - pg_xlog (journaling not required)

References:
http://stackoverflow.com/a/2095283
http://michael.otacoo.com/postgresql-2/take-care-of-kernel-memory-limitation-for-postgresql-shared-buffers/
http://www.postgresql.org/docs/9.2/static/kernel-resources.html
http://www.linux.com/learn/tutorials/394523-configuring-postgresql-for-pretty-good-performance

*** TODO launch a new Nectar VM from the snapshot image
*** TODO mount the 2nd disc and load/restore the postgres db and data into it
#+name:scp
#+begin_src R :session *shell* :tangle no :exports none :eval no
###########################################################################
# newnode: scp
# from backup computer to new brawn server
scp homebackup_yyyymmdd.zip root@ww.xx.yy.zz:/
ssh root@ww.xx.yy.zz
cd /
unzip homebackup_yyyymmdd.zip
#+end_src

** Disaster Recovery Plan
*** test a snapshot
should test snapshots 3-monthly?

**** cleaning up links on github
in case of a completely failed VM, launching a fresh VM from snapshot will give a different IP-address and so Github and other links, and user's bookmarks will need to be revised.
*** TODO 60GB disk is not being saved in snapshots
Will also need to investigate the restore procedure (and security) for the secondary disc.
*** TODO Restore ORAPHI
This should have restored ok with data intact from the last snapshot.
If not first set up oracle and the tables.
Get the most recent backup of the tables.
Load the tables to the new ORAPHI.
In the event of an upgrade to a tested replacement you may find that the contents on the running VM that is getting retired is more current than the old stuff in the replacement tested server so will need to sync.
* COMMENT An Environmental Epidemiology Example
Using Suicide and Drought as an example.
* COMMENT Data
** Zones
** Outcome
** Exposure
** Population
* COMMENT Analysis
* COMMENT Document
** review
*** newnode
#+name:Merge-newnode
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:zones-newnode
  nodes <- newnode(name = 'document', inputs = 'results',
                   outputs = c('manuscript4review','metadata')
                   )
    
#+end_src

*** func
*** load
*** clean
*** do
** accepted
*** newnode
#+name:Merge-newnode
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:zones-newnode
  nodes <- newnode(name = 'publish', inputs = 'manuscript4review',
                   outputs = c('accepted','metadata')
                   )
  
#+end_src

*** func
*** load
*** clean
*** do

* COMMENT Bibliography
# \section{References}
# \bibliographystyle{unsrt}
# \bibliography{I:/references/library}

\begin{thebibliography}{1}
\bibitem{King1995}
G~King.
\newblock {Replication, replication}.
\newblock {\em PS: Political Science and Politics}, 28:443--499, 1995.

\bibitem{Gentleman2007}
 Robert Gentleman and Duncan {Temple Lang}.
 \newblock {Statistical Analyses and Reproducible Research}.
 \newblock {\em Journal of Computational and Graphical Statistics}, 16(1):1--23,
   March 2007.

 \bibitem{Schulte}
 E~Schulte, D~Davison, T~Dye, and C~Dominik.
 \newblock {A Multi-Language Computing Environment for Literate Programming and
   Reproducible Research}.
 \newblock {\em Journal of Statistical Software}, 46(3), 2012.


\end{thebibliography}
* COMMENT Archiving
*** newnode
#+name:Merge-newnode
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:zones-newnode
  nodes <- newnode(name = 'archive', inputs = 'accepted',
                   outputs = c('re-use')
                   )
    
#+end_src

*** func
*** load
*** clean
*** do

* COMMENT Metadata
** COMMENT review processing and select objects to produce metadata for
#+begin_src R :session *R* :tangle no :exports reports :eval no
dev.copy2pdf(file='nodes.pdf');
dev.off();
#+end_src
#+name:plot-nodes
#+begin_src R :session *R* :tangle no :exports reports :eval no
  ################################################################
  # name:plot-nodes
  source('src/nodes.r')
#+end_src

** graph of nodes in this analysis
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{nodes.pdf}
\caption{nodes.pdf}
\label{fig:nodes.pdf}
\end{figure}
\clearpage

** COMMENT add metadata using df2ddi
#+name:add_ddi
#+begin_src R :session *shell* :tangle no :exports reports :eval no
  ################################################################
  # name:add_ddi
  source('~/disentangle/src/df2ddi.r')
  source('~/disentangle/src/connect2postgres.r')
  ewedb <- connect2postgres()
  if(!require(rgdal)) install.packages('rgdal'); require(rgdal)
  if(!require(RJDBC)) install.packages('RJDBC'); require(RJDBC)
  drv <- JDBC("oracle.jdbc.driver.OracleDriver",
              '/u01/app/oracle/product/11.2.0/xe/jdbc/lib/ojdbc6.jar')
  p <- readline('enter password: ')
  h <- readline('enter target ipaddres: ')
  d <- readline('enter database name: ')
  ch <- dbConnect(drv,paste("jdbc:oracle:thin:@",h,":1521",sep=''),d,p)
  
  #dir.create('metadata')
  s <- dbGetQuery(ch, "select * from stdydscr where IDNO = 'BOUNDARIES_ELECTORATES'")
  # s <- add_stdydscr(ask=T)
  #write.table(s,'metadata/stdydscr.csv',sep=',',row.names=F)
  
  s$PRODDATESTDY=format(as.Date( substr(s$PRODDATESTDY,1,10),'%Y-%m-%d'),"%d/%b/%Y")
  s$PRODDATEDOC=format(as.Date( substr(s$PRODDATEDOC,1,10),'%Y-%m-%d'),"%d/%b/%Y")
  
  ## dbSendUpdate(ch,
  ## # cat(
  ## paste('
  ## insert into STDYDSCR (',paste(names(s), sep = '', collapse = ', '),')
  ## VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(s),'',s)),sep='',collapse="', '"),"'",sep=''),')',sep='')
  ## )
  
  f <- add_filedscr(fileid = 1, idno = 'BOUNDARIES_ELECTORATES', ask=T)
  f$FILELOCATION <- 'BOUNDARIES_ELECTORATES'
  
  dbSendUpdate(ch,
  # cat(
  paste('
  insert into FILEDSCR (',paste(names(f), sep = '', collapse = ', '),')
  VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(f),'',f)),sep='',collapse="', '"),"'",sep=''),')',sep='')
  )
  f <- dbGetQuery(ch, "select * from filedscr where IDNO = 'BOUNDARIES_ELECTORATES'")
  f
  
  fid <- dbGetQuery(ch,
  #                  cat(
                    paste("select FILEID
                    from filedscr
                    where filelocation = '",f$FILELOCATION,"'
                    and filename = '",f$FILENAME,"'",
                    sep=''))
  
  df <- dbGetQuery(ewedb,
                   'select elect_div, state from boundaries_electorates.electorates2009 limit 1'
                   )
  df[1,]
  df <- readOGR2(hostip = '115.146.94.209', user = 'steven_mceachern',
                   db = 'ewedb', layer =
                   'boundaries_electorates.electorates2009')
  df@data[1:10,]
  d <- add_datadscr(data_frame = df, fileid = fid[1,1], ask=T)
  d
  
  for(i in 1:nrow(d)){
  dbSendUpdate(ch,
  #i = 1
  # cat(
  paste('
  insert into DATADSCR (',paste(names(d), sep = '', collapse = ', '),')
  VALUES (',paste("'",paste(gsub("'","",ifelse(is.na(d[i,]),'',d[i,])),sep='',collapse="', '"),"'",sep=''),')',sep='')
  )
  }
  
  
  ###################################################
  # make xml
  studyID <- 'BOUNDARIES_ELECTORATES'
  s <- dbGetQuery(ch, paste("select * from stdydscr where idno = '",studyID,"'",sep=''))
  s
  f <- dbGetQuery(ch, paste("select * from filedscr where idno = '",studyID,"'",sep=''))
  f
  for(fi in f){
  d <- dbGetQuery(ch,
                  paste("select * from datadscr where FILEID = ",f$FILEID,
                        sep = '')
                  )
  d
  ddixml <- make_xml(s,f,d)
  }
  out <- dir(pattern='xml')
  file.remove(file.path('/xmldata', out))
  file.copy(out, '/xmldata')
  # go to indexer.jsp
  out
  
#+end_src
 
* COMMENT remote git
#+name:remote git
#+begin_src R :session *shell* :exports reports :eval no
  system('git fetch origin')
  system('git merge origin/master')
#+end_src
\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{nodes.pdf}
\caption{nodes.pdf}
\label{fig:nodes.pdf}
\end{figure}
\clearpage
